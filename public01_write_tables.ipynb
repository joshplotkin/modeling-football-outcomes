{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.508520Z",
     "start_time": "2019-01-20T22:39:57.228698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run initialize.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.703351Z",
     "start_time": "2019-01-20T22:40:03.513884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAABgCAYAAAAzQJvVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAOwwAADsMBx2+oZAAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAAq5JREFUeJzt3TFqlEEAhuFZYxKzVgFFsNHCQhCsAla5hYW9d7C0yD3svYCHEFIFBAsbG0FccBt3SbJhvcKKyM+Lz1PPwFe+TDOz5XK5HQAAEHBr6gEAALAr8QoAQIZ4BQAgQ7wCAJAhXgEAyBCvAABkiFcAADLEKwAAGeIVAIAM8QoAQMbtP73w5uLDv9jx3zieHYyXdx6Pp+fvx9Hq59Rz8tbz4/H55NU4/zLG6nLqNW3zwzFOnozx4OPl2F/5NfpvXc9n4/uLw/F1/m5c7S2mnpN2cHNvPFq9Hs/uvh1He9+mnpO3vnk4Pv06G/evLsb+dj31nLTr2dH4cfB8LBaLsdlspp6Tdnp6uvNZL68AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDIEK8AAGSIVwAAMsQrAAAZ4hUAgAzxCgBAhngFACBDvAIAkCFeAQDImC2Xy+3UIwAAYBdeXgEAyBCvAABkiFcAADLEKwAAGeIVAIAM8QoAQIZ4BQAgQ7wCAJAhXgEAyBCvAABkiFcAADJ+Ax1rM1/62bpNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 768x96 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## custom rcParams settings for matplotlib\n",
    "import sys\n",
    "sys.path.append('../modeling-football-outcomes/config')\n",
    "import mpl_style\n",
    "colors = rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.717744Z",
     "start_time": "2019-01-20T22:40:03.708001Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_dtypes(df):\n",
    "    '''spark and numpy types don't play\n",
    "    well together. cast as native python \n",
    "    types'''\n",
    "    np_to_python_dtypes = {\n",
    "        'int64': int,\n",
    "        'float64': float,\n",
    "        'object': str,\n",
    "        'datetime64[ns]': str\n",
    "    }\n",
    "\n",
    "    for field, np_dtype in df.dtypes.to_dict().iteritems():\n",
    "        df[field] = df[field].astype(np_to_python_dtypes[str(np_dtype)])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create __lines__ and __games__ DFs: load data and remove 2018 until it's complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.805164Z",
     "start_time": "2019-01-20T22:40:03.722724Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_df = pd.read_csv('all_lines.csv', index_col='Unnamed: 0')\n",
    "games_df = pd.read_csv('game_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.829752Z",
     "start_time": "2019-01-20T22:40:03.810829Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_df = lines_df.rename(columns={'Season':'season'})\n",
    "lines_df = lines_df[lines_df['season'] != 2018]\n",
    "games_df = games_df[games_df['team0_link'].apply(lambda x: x.split('/')[-1].split('.')[0]) != '2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.851948Z",
     "start_time": "2019-01-20T22:40:03.835589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007    267\n",
       "2008    267\n",
       "2009    267\n",
       "2010    267\n",
       "2011    267\n",
       "2012    267\n",
       "2013    267\n",
       "2014    267\n",
       "2015    267\n",
       "2016    267\n",
       "2017    267\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df['season'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.871862Z",
     "start_time": "2019-01-20T22:40:03.856293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007    267\n",
       "2008    267\n",
       "2009    267\n",
       "2010    267\n",
       "2011    267\n",
       "2012    267\n",
       "2013    267\n",
       "2014    267\n",
       "2015    267\n",
       "2016    267\n",
       "2017    267\n",
       "Name: team0_link, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_df['team0_link'].apply(lambda x: x.split('/')[-1].split('.')[0]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.889461Z",
     "start_time": "2019-01-20T22:40:03.876853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2937, 26)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2937, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df.shape\n",
    "games_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Fields/Values Across Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in raw data (__lines__ DF)\n",
    "* 3 games have home/visiting teams switched (note: these are 3 Superbowls so the home/away don't match the official home and visiting teams. the home/visiting team is always \"team0\" in football-ref. there is an assertion confirms this below).\n",
    "  * https://www.pro-football-reference.com/boxscores/201602070den.htm\n",
    "  * https://www.pro-football-reference.com/boxscores/201502010sea.htm\n",
    "  * https://www.pro-football-reference.com/boxscores/201302030sfo.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:03.900964Z",
     "start_time": "2019-01-20T22:40:03.892998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "switches = lines_df[lines_df['Date'].apply(lambda x: x in ['2016-02-07','2015-02-01','2013-02-03'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:04.219075Z",
     "start_time": "2019-01-20T22:40:03.905573Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshplotkin/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/joshplotkin/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py:3781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "col_val_switch = ['ML_Fav','Open_Fav','Close_Fav','2H_Fav']\n",
    "\n",
    "for c in col_val_switch:\n",
    "    switches.loc[:, c] = switches.loc[:, c].apply(lambda x: {'V':'H', 'H':'V', 'pickem': 'pickem'}[x])\n",
    "\n",
    "## H --> X\n",
    "## V --> H\n",
    "## X --> V\n",
    "h_to_x = dict([(c, c.replace('H_','X_')) for c in switches.columns if c.startswith('H_')])\n",
    "switches.rename(\n",
    "    columns=h_to_x, inplace=True\n",
    ")\n",
    "v_to_h = dict([(c, c.replace('V_','H_')) for c in switches.columns if c.startswith('V_')])\n",
    "switches.rename(\n",
    "    columns=v_to_h, inplace=True\n",
    ")\n",
    "x_to_v = dict([(c, c.replace('X_','V_')) for c in switches.columns if c.startswith('X_')])\n",
    "switches.rename(\n",
    "    columns=x_to_v, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:04.238869Z",
     "start_time": "2019-01-20T22:40:04.224063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshplotkin/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "idx = switches.index\n",
    "lines_df = lines_df.drop(idx)\n",
    "lines_df = lines_df.append(switches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:04.283650Z",
     "start_time": "2019-01-20T22:40:04.244066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1023</th>\n",
       "      <th>1290</th>\n",
       "      <th>1824</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2H_Fav</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H_OU</th>\n",
       "      <td>22.5</td>\n",
       "      <td>pk</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H_Spread</th>\n",
       "      <td>4.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close_Fav</th>\n",
       "      <td>H</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close_OU</th>\n",
       "      <td>43.5</td>\n",
       "      <td>47.5</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close_Spread</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>2016-02-07</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>2013-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Final</th>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_ML</th>\n",
       "      <td>-210</td>\n",
       "      <td>-110</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Q1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Q2</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Q3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Q4</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_Team</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>NewEngland</td>\n",
       "      <td>Baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_Fav</th>\n",
       "      <td>H</td>\n",
       "      <td>pickem</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open_Fav</th>\n",
       "      <td>H</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open_OU</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open_Spread</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Final</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_ML</th>\n",
       "      <td>180</td>\n",
       "      <td>-110</td>\n",
       "      <td>-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Q1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Q2</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Q3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Q4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Team</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>SanFrancisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>2015</td>\n",
       "      <td>2014</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1023        1290          1824\n",
       "2H_Fav                 H           H             V\n",
       "2H_OU               22.5          pk          24.5\n",
       "2H_Spread            4.5        24.5             7\n",
       "Close_Fav              H           V             V\n",
       "Close_OU            43.5        47.5            48\n",
       "Close_Spread           5           0           4.5\n",
       "Date          2016-02-07  2015-02-01    2013-02-03\n",
       "H_Final               10          28            34\n",
       "H_ML                -210        -110           170\n",
       "H_Q1                   0           0             7\n",
       "H_Q2                   7          14            14\n",
       "H_Q3                   0           0             7\n",
       "H_Q4                   3          14             6\n",
       "H_Team          Carolina  NewEngland     Baltimore\n",
       "ML_Fav                 H      pickem             V\n",
       "Open_Fav               H           V             V\n",
       "Open_OU               45          49          47.5\n",
       "Open_Spread            4           3           3.5\n",
       "V_Final               24          24            31\n",
       "V_ML                 180        -110          -200\n",
       "V_Q1                  10           0             3\n",
       "V_Q2                   3          14             3\n",
       "V_Q3                   3          10            17\n",
       "V_Q4                   8           0             8\n",
       "V_Team            Denver     Seattle  SanFrancisco\n",
       "season              2015        2014          2012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Carolina, New England, and Baltimore should be home teams\n",
    "lines_df[lines_df['Date'].apply(lambda x: x in ['2016-02-07','2015-02-01','2013-02-03'])].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates (__lines__ and __games__ DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:06.663631Z",
     "start_time": "2019-01-20T22:40:04.290117Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_df['Date'] = lines_df['Date'].apply(pd.to_datetime)\n",
    "\n",
    "games_df['day_of_week'] = games_df['date'].apply(pd.to_datetime).dt.dayofweek\n",
    "games_df['date'] = games_df['date'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team names (__lines__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:06.681046Z",
     "start_time": "2019-01-20T22:40:06.668351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dallas --> Cowboys\n",
      "NewYork --> Giants\n",
      "NYGiants --> Giants\n",
      "Minnesota --> Vikings\n",
      "Denver --> Broncos\n",
      "NewEngland --> Patriots\n"
     ]
    }
   ],
   "source": [
    "team_map = json.load(open('team_map_for_lines.json','r'))\n",
    "for i, (k,v) in enumerate(team_map.iteritems()):\n",
    "    print '{} --> {}'.format(k, v)\n",
    "    if i == 5:\n",
    "        break\n",
    "        \n",
    "for field in ['V_Team','H_Team']:\n",
    "    lines_df[field] = lines_df[field].map(team_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create __games_lines__: prepare and then join __games__ and __lines__\n",
    "* key is (date, team first alphabetically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:06.762484Z",
     "start_time": "2019-01-20T22:40:06.686183Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## cast dtypes to Spark compatible using global function cast_dtypes\n",
    "games_df = cast_dtypes(games_df)\n",
    "lines_df = cast_dtypes(lines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:06.974069Z",
     "start_time": "2019-01-20T22:40:06.766141Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_game_key(x):\n",
    "    '''generates a key for each game:\n",
    "    (first alphabetical team name, date)'''\n",
    "    team_a, team_b, dt = x\n",
    "    return (sorted([team_a, team_b])[0], dt)\n",
    "\n",
    "games_key_fields = ['team0_name','team1_name','date']\n",
    "lines_key_fields = ['H_Team','V_Team', 'Date']\n",
    "\n",
    "games_df['key'] = games_df[games_key_fields].apply(gen_game_key, axis=1)\n",
    "lines_df['key'] = lines_df[lines_key_fields].apply(gen_game_key, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: week 16 2018 lines not yet available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:06.996956Z",
     "start_time": "2019-01-20T22:40:06.978899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(games_df['key'].apply(lambda x: '_'.join(x)).values.tolist())\n",
    "b = set(lines_df['key'].apply(lambda x: '_'.join(x)).values.tolist())\n",
    "\n",
    "len(a - b)\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:07.013402Z",
     "start_time": "2019-01-20T22:40:07.000743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b-a)\n",
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:07.054748Z",
     "start_time": "2019-01-20T22:40:07.018457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2937, 2937, 2937)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_lines_df = games_df.merge(\n",
    "        lines_df, left_on='key', right_on='key'\n",
    "    ).drop(\n",
    "        lines_key_fields + ['key'], axis=1\n",
    "    )\n",
    "\n",
    "## ensure there are no join misses\n",
    "lines_df.shape[0], games_df.shape[0], games_lines_df.shape[0]\n",
    "assert lines_df.shape[0] == games_df.shape[0]\n",
    "assert games_lines_df.shape[0] == games_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create __game_info__: load PFR game_info files\n",
    "* contains metadata like turf and weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:22.584134Z",
     "start_time": "2019-01-20T22:40:07.059730Z"
    }
   },
   "outputs": [],
   "source": [
    "## get the path for each game_info csv\n",
    "game_info_csvs = map(\n",
    "    lambda x: 'game_info/{}'.format(x), \n",
    "    os.listdir('game_info')\n",
    ")\n",
    "## read each game_info csv into pandas\n",
    "game_info_dfs = map(\n",
    "    lambda x: pd.read_csv(x, index_col='Unnamed: 0'), \n",
    "    game_info_csvs\n",
    ")\n",
    "## union all the DFs\n",
    "game_info_df = reduce(\n",
    "    lambda x,y: x.append(y).reset_index(drop=True), \n",
    "    game_info_dfs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:22.643955Z",
     "start_time": "2019-01-20T22:40:22.588763Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in game_info_df.columns:\n",
    "    game_info_df[c] = game_info_df[c].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:22.666049Z",
     "start_time": "2019-01-20T22:40:22.648115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>info</th>\n",
       "      <th>stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201812090was</td>\n",
       "      <td>Won Toss</td>\n",
       "      <td>Giants (deferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201812090was</td>\n",
       "      <td>Roof</td>\n",
       "      <td>outdoors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201812090was</td>\n",
       "      <td>Surface</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201812090was</td>\n",
       "      <td>Weather</td>\n",
       "      <td>33 degrees, wind 6 mph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201812090was</td>\n",
       "      <td>Vegas Line</td>\n",
       "      <td>New York Giants -3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        game_id        info                    stat\n",
       "0  201812090was    Won Toss       Giants (deferred)\n",
       "1  201812090was        Roof                outdoors\n",
       "2  201812090was     Surface                   grass\n",
       "3  201812090was     Weather  33 degrees, wind 6 mph\n",
       "4  201812090was  Vegas Line    New York Giants -3.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.140321Z",
     "start_time": "2019-01-20T22:40:22.670691Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info_df['key_value'] = game_info_df[['info','stat']].apply(\n",
    "    lambda (k,v): {k:v}, axis=1\n",
    ")\n",
    "\n",
    "game_info_dict = {}\n",
    "for game in game_info_df['game_id'].unique():\n",
    "    curr_game_df = game_info_df[game_info_df['game_id'] == game]\n",
    "    game_info_dict[game] = {\n",
    "            d.keys()[0]:d.values()[0] \n",
    "            for d in curr_game_df['key_value'].values\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.406114Z",
     "start_time": "2019-01-20T22:40:29.145234Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_info_colnames = {\n",
    "    'Vegas Line': 'pfr_line',\n",
    "    'Over/Under': 'pfr_ou',\n",
    "    'Weather': 'prf_weather',\n",
    "    'Roof': 'roof',\n",
    "    'Surface': 'surface'\n",
    "}\n",
    "\n",
    "game_info = pd.DataFrame.from_dict(game_info_dict, orient='index')\\\n",
    "                .rename(columns=game_info_colnames)\\\n",
    "                .drop(['Won Toss','Won OT Toss','Super Bowl MVP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.437658Z",
     "start_time": "2019-01-20T22:40:29.411419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200709060clt</th>\n",
       "      <th>200709090buf</th>\n",
       "      <th>200709090cle</th>\n",
       "      <th>200709090dal</th>\n",
       "      <th>200709090gnb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>fieldturf</td>\n",
       "      <td>astroplay</td>\n",
       "      <td>grass</td>\n",
       "      <td>fieldturf</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pfr_line</th>\n",
       "      <td>Indianapolis Colts -5.5</td>\n",
       "      <td>Denver Broncos -3.0</td>\n",
       "      <td>Pittsburgh Steelers -4.5</td>\n",
       "      <td>Dallas Cowboys -6.5</td>\n",
       "      <td>Philadelphia Eagles -3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pfr_ou</th>\n",
       "      <td>53.5 (under)</td>\n",
       "      <td>37.0 (under)</td>\n",
       "      <td>36.5 (over)</td>\n",
       "      <td>44.5 (over)</td>\n",
       "      <td>42.5 (under)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roof</th>\n",
       "      <td>dome</td>\n",
       "      <td>outdoors</td>\n",
       "      <td>outdoors</td>\n",
       "      <td>outdoors</td>\n",
       "      <td>outdoors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prf_weather</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70 degrees relative humidity 93%, wind 5 mph, ...</td>\n",
       "      <td>71 degrees relative humidity 80%, wind 5 mph, ...</td>\n",
       "      <td>78 degrees relative humidity 85%, no wind, win...</td>\n",
       "      <td>62 degrees relative humidity 80%, wind 9 mph, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        200709060clt  \\\n",
       "surface                    fieldturf   \n",
       "pfr_line     Indianapolis Colts -5.5   \n",
       "pfr_ou                  53.5 (under)   \n",
       "roof                            dome   \n",
       "prf_weather                      NaN   \n",
       "\n",
       "                                                  200709090buf  \\\n",
       "surface                                              astroplay   \n",
       "pfr_line                                   Denver Broncos -3.0   \n",
       "pfr_ou                                            37.0 (under)   \n",
       "roof                                                  outdoors   \n",
       "prf_weather  70 degrees relative humidity 93%, wind 5 mph, ...   \n",
       "\n",
       "                                                  200709090cle  \\\n",
       "surface                                                  grass   \n",
       "pfr_line                              Pittsburgh Steelers -4.5   \n",
       "pfr_ou                                             36.5 (over)   \n",
       "roof                                                  outdoors   \n",
       "prf_weather  71 degrees relative humidity 80%, wind 5 mph, ...   \n",
       "\n",
       "                                                  200709090dal  \\\n",
       "surface                                              fieldturf   \n",
       "pfr_line                                   Dallas Cowboys -6.5   \n",
       "pfr_ou                                             44.5 (over)   \n",
       "roof                                                  outdoors   \n",
       "prf_weather  78 degrees relative humidity 85%, no wind, win...   \n",
       "\n",
       "                                                  200709090gnb  \n",
       "surface                                                  grass  \n",
       "pfr_line                              Philadelphia Eagles -3.0  \n",
       "pfr_ou                                            42.5 (under)  \n",
       "roof                                                  outdoors  \n",
       "prf_weather  62 degrees relative humidity 80%, wind 9 mph, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_info.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create __games_lines_info__: join __games_lines__ with __game_info__ assert there are no join misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.471034Z",
     "start_time": "2019-01-20T22:40:29.442831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2018'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2937, 3177, 2937)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2018 games\n",
    "diffs = set(game_info.index.tolist()) - set(games_lines_df['game_id'].tolist())\n",
    "set(map(lambda x: x[:4], diffs))\n",
    "\n",
    "games_lines_info = games_lines_df.merge(game_info, left_on='game_id', right_index=True)\n",
    "\n",
    "games_lines_df.shape[0], game_info.shape[0], games_lines_info.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.478457Z",
     "start_time": "2019-01-20T22:40:29.474743Z"
    }
   },
   "outputs": [],
   "source": [
    "## TEMPORARILY TURNING OFF\n",
    "# assert games_lines_df.shape[0] == game_info.shape[0]\n",
    "# assert games_lines_df.shape[0] == games_lines_info.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich __games_lines_info__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stadium lat/long and timezone\n",
    "* for each stadium\n",
    "* to be used for computing travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.508094Z",
     "start_time": "2019-01-20T22:40:29.483997Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "stadiums = pd.read_csv('stadiums_w_tz.csv')\n",
    "\n",
    "## assert that stadiums lat/long csv has all stadiums in the games_df\n",
    "assert not set(games_lines_info['stadium'].unique()) - set(stadiums['stadium'].unique())\n",
    "before = games_lines_info.shape[0]\n",
    "games_lines_info = games_lines_info.merge(\n",
    "        stadiums, left_on='stadium', right_on='stadium'\n",
    "    )\n",
    "\n",
    "assert games_lines_info.shape[0] == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.549556Z",
     "start_time": "2019-01-20T22:40:29.513388Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stadium</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCA Dome</td>\n",
       "      <td>39.76361</td>\n",
       "      <td>-86.16333</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ralph Wilson Stadium</td>\n",
       "      <td>42.77400</td>\n",
       "      <td>-78.78700</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Cleveland Browns Stadium</td>\n",
       "      <td>41.50611</td>\n",
       "      <td>-81.69944</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Texas Stadium</td>\n",
       "      <td>32.84000</td>\n",
       "      <td>-96.91100</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Lambeau Field</td>\n",
       "      <td>44.50139</td>\n",
       "      <td>-88.06222</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Reliant Stadium</td>\n",
       "      <td>29.68472</td>\n",
       "      <td>-95.41083</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Jacksonville Municipal Stadium</td>\n",
       "      <td>30.32389</td>\n",
       "      <td>-81.63750</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Hubert H. Humphrey Metrodome</td>\n",
       "      <td>44.97389</td>\n",
       "      <td>-93.25806</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Giants Stadium</td>\n",
       "      <td>40.81222</td>\n",
       "      <td>-74.07694</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>McAfee Coliseum</td>\n",
       "      <td>37.75167</td>\n",
       "      <td>-122.20056</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            stadium       lat       long  timezone\n",
       "0                          RCA Dome  39.76361  -86.16333      -5.0\n",
       "9              Ralph Wilson Stadium  42.77400  -78.78700      -5.0\n",
       "74         Cleveland Browns Stadium  41.50611  -81.69944      -5.0\n",
       "122                   Texas Stadium  32.84000  -96.91100      -6.0\n",
       "139                   Lambeau Field  44.50139  -88.06222      -6.0\n",
       "234                 Reliant Stadium  29.68472  -95.41083      -6.0\n",
       "292  Jacksonville Municipal Stadium  30.32389  -81.63750      -5.0\n",
       "316    Hubert H. Humphrey Metrodome  44.97389  -93.25806      -6.0\n",
       "342                  Giants Stadium  40.81222  -74.07694      -5.0\n",
       "391                 McAfee Coliseum  37.75167 -122.20056      -8.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_lines_info[stadiums.columns].drop_duplicates().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load __gamelogs__ and use it to add in the week number\n",
    "* #### Future TODO: incorporate gamelogs data as needed, e.g. game stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.605036Z",
     "start_time": "2019-01-20T22:40:29.555154Z"
    }
   },
   "outputs": [],
   "source": [
    "gamelogs = pd.read_csv('gamelogs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temp: remove 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.617945Z",
     "start_time": "2019-01-20T22:40:29.609103Z"
    }
   },
   "outputs": [],
   "source": [
    "gamelogs = gamelogs[gamelogs['season'] < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.634025Z",
     "start_time": "2019-01-20T22:40:29.623434Z"
    }
   },
   "outputs": [],
   "source": [
    "game_time_info = gamelogs[['game_id','season','week_id']].drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.657500Z",
     "start_time": "2019-01-20T22:40:29.639718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>season</th>\n",
       "      <th>week_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200909130clt</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200909200jax</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200909270htx</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200910040jax</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200910110sea</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        game_id  season  week_id\n",
       "0  200909130clt    2009        1\n",
       "1  200909200jax    2009        2\n",
       "2  200909270htx    2009        3\n",
       "3  200910040jax    2009        4\n",
       "4  200910110sea    2009        5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_time_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.688327Z",
     "start_time": "2019-01-20T22:40:29.663002Z"
    }
   },
   "outputs": [],
   "source": [
    "assert games_lines_info.shape[0] == games_lines_info.merge(\n",
    "        game_time_info, left_on='game_id', right_on='game_id'\n",
    "    ).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.720143Z",
     "start_time": "2019-01-20T22:40:29.691608Z"
    }
   },
   "outputs": [],
   "source": [
    "games_lines_tz = games_lines_info.drop(\n",
    "        'season', axis=1\n",
    "    ).merge(\n",
    "        game_time_info, left_on='game_id', right_on='game_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:40:29.767319Z",
     "start_time": "2019-01-20T22:40:29.725267Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert each season has the same number of games (267)\n",
    "tmp = games_lines_tz['season'].value_counts().sort_index()\n",
    "assert tmp.max() == tmp.min()\n",
    "\n",
    "tmp = games_lines_tz[['season','week_id','team0_link']].rename(\n",
    "        columns={'team0_link':'team'}\n",
    "    ).append(\n",
    "        games_lines_tz[['season','week_id','team1_link']].rename(\n",
    "            columns={'team1_link':'team'}\n",
    "        )\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "## assert each team has 16 games each season in gamelog\n",
    "assert (tmp[tmp['week_id'] <= 17].groupby(['team','season']).size() == 16).all()\n",
    "\n",
    "### assert that no one plays >1 game in a week\n",
    "assert (tmp[tmp['week_id'] <= 17].groupby(['team','season','week_id']).size() == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer stadium --> home team (or neutral)\n",
    "* TODO: clean this up\n",
    "* PFR has team0 == home team (including official \"home\" team in neutral stadiums)\n",
    "* treat jets vs. giants as both home (neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.366Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get a count of team games plabyed by stadium\n",
    "tmp = games_lines_tz.copy()\n",
    "tmp['teams'] = tmp[['team0_link','team1_link']].apply(\n",
    "    lambda x: map(lambda y: y.split('/')[2], x), \n",
    "    axis=1\n",
    ")\n",
    "stad = spark.createDataFrame(\n",
    "        tmp[['teams','stadium','season']]\n",
    "    ).select(\n",
    "        F.explode('teams').alias('team'), 'stadium', 'season'\n",
    "    ).groupby('stadium','season','team').count()\\\n",
    "     .groupby('stadium','season').pivot('team').agg(F.sum('count')).toPandas().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.371Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## compute the proportion of a stadium's games that each team played\n",
    "## **assumption used: of a stadium has had more thna 6 games played\n",
    "## and a team has >20% of games played there, that is the home team\n",
    "## TODO: make this more elegant, maybe leveraging the Wiki package\n",
    "key = ['stadium','season']\n",
    "teams = list(set(stad.columns) - set(key))\n",
    "\n",
    "stad['sum'] = stad[teams].sum(axis=1)\n",
    "\n",
    "for t in teams:\n",
    "    stad[t] /= stad['sum']\n",
    "    \n",
    "def find_home_team(props, teams):\n",
    "    props, ngames = (props[:-1], props[-1])\n",
    "    max_idx = [\n",
    "        i for (i, t) in enumerate(props) \n",
    "        if (t > 0.2) & (ngames > 6)\n",
    "    ]\n",
    "    return map(lambda t: teams[t], max_idx)\n",
    "\n",
    "stad['home_teams'] = stad[teams + ['sum']].apply(\n",
    "    lambda x: find_home_team(x, teams), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.379Z"
    }
   },
   "outputs": [],
   "source": [
    "# SPECIAL CASES (games relocated nearby)\n",
    "## set Rogers Centre to buf\n",
    "rog = stad[stad['stadium'] == 'Rogers Centre'].index\n",
    "stad.loc[rog, 'home_teams'] = stad.loc[rog, 'season'].apply(lambda x: ['buf'])\n",
    "stad.loc[rog, :]\n",
    "\n",
    "## set TCF Bank Stadium to min\n",
    "tcf = stad[stad['stadium'] == 'TCF Bank Stadium'].index\n",
    "stad.loc[tcf, 'home_teams'] = stad.loc[tcf, 'season'].apply(lambda x: ['min'])\n",
    "stad.loc[tcf, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stad[['stadium','season','home_teams']].sort_values(by=['stadium','season']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.388Z"
    }
   },
   "outputs": [],
   "source": [
    "## take the previous DF, and tack on the \n",
    "## list of home teams\n",
    "games_lines_info_homeaway = games_lines_tz.merge(\n",
    "    stad[['stadium','season','home_teams']],\n",
    "    left_on=['stadium','season'],\n",
    "    right_on=['stadium','season']\n",
    ")\n",
    "assert games_lines_info_homeaway.shape[0] == games_lines_tz.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.392Z"
    }
   },
   "outputs": [],
   "source": [
    "games_lines_info_homeaway['team0'] = games_lines_info_homeaway['team0_link'].apply(lambda x: x.split('/')[2])\n",
    "games_lines_info_homeaway['team1'] = games_lines_info_homeaway['team1_link'].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "games_lines_info_homeaway['team0_home_unof'] = games_lines_info_homeaway[['team0','home_teams']].apply(\n",
    "    lambda x: x[0] in x[1],\n",
    "    axis=1\n",
    ")\n",
    "games_lines_info_homeaway['team1_home_unof'] = games_lines_info_homeaway[['team1','home_teams']].apply(\n",
    "    lambda x: x[0] in x[1],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.396Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_home_team(x):\n",
    "    team0, team1 = x\n",
    "    if team0 == team1:\n",
    "        return 'neutral'\n",
    "    elif team0 is True:\n",
    "        return 'team0'\n",
    "    elif team1 is True:\n",
    "        return 'team1'\n",
    "    \n",
    "games_lines_info_homeaway['home_team'] = games_lines_info_homeaway.loc[\n",
    "        :, ['team0_home_unof','team1_home_unof']\n",
    "    ].apply(\n",
    "        extract_home_team, axis=1\n",
    "    )\n",
    "games_lines_info_homeaway.drop(\n",
    "    ['team0_home_unof','team1_home_unof'], axis=1, inplace=True\n",
    ")\n",
    "games_lines_info_homeaway.rename(\n",
    "    columns={'home_teams':'stadium_home_team'}, inplace=True\n",
    ")\n",
    "\n",
    "games_lines_info_homeaway['home_team'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.402Z"
    }
   },
   "outputs": [],
   "source": [
    "## replace team0 with the team0 name\n",
    "team0_idx = games_lines_info_homeaway[\n",
    "                games_lines_info_homeaway['home_team'] == 'team0'\n",
    "            ].index.tolist()\n",
    "\n",
    "games_lines_info_homeaway.loc[team0_idx, 'home_team'] = games_lines_info_homeaway\\\n",
    "                                                            .loc[team0_idx, 'team0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.406Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## still 39 neutral\n",
    "games_lines_info_homeaway['home_team'].value_counts().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.411Z"
    }
   },
   "outputs": [],
   "source": [
    "assert games_lines_info_homeaway[\n",
    "    (games_lines_info_homeaway['home_team'] == 'neutral')\n",
    "    & (games_lines_info_homeaway[['H_Q1','H_Q2','H_Q3','H_Q4']].sum(axis=1) \n",
    "          != games_lines_info_homeaway['team0_score'])\n",
    "    & (games_lines_info_homeaway[['V_Q1','V_Q2','V_Q3','V_Q4']].sum(axis=1) \n",
    "          != games_lines_info_homeaway['team1_score'])\n",
    "].shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.416Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "games_lines_info_homeaway['H_score_calc'] = games_lines_info_homeaway\\\n",
    "                                        [['H_Q1','H_Q2','H_Q3','H_Q4']].sum(axis=1)\n",
    "games_lines_info_homeaway['V_score_calc'] = games_lines_info_homeaway\\\n",
    "                                        [['V_Q1','V_Q2','V_Q3','V_Q4']].sum(axis=1)\n",
    "\n",
    "games_lines_info_homeaway[\n",
    "        games_lines_info_homeaway['home_team'] == 'neutral'\n",
    "    ].loc[:, ['game_id','team0','team1','stadium','H_score_calc',\n",
    "              'team0_score','V_score_calc','team1_score',\n",
    "              'stadium_home_team']\n",
    "    ].sort_values(by='stadium').head(10)\n",
    "                                 \n",
    "games_lines_info_homeaway.drop(['H_score_calc','V_score_calc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T22:08:24.085048Z",
     "start_time": "2019-01-04T22:08:24.078649Z"
    }
   },
   "source": [
    "#### another method for home teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.422Z"
    }
   },
   "outputs": [],
   "source": [
    "home_teams = gamelogs[['game_id','game_location','game_loc_team_ref']]\n",
    "loc_neutral = home_teams[home_teams['game_location'] == 'N']\\\n",
    "                    .loc[:, ['game_location','game_id']]\\\n",
    "                    .drop_duplicates()\\\n",
    "                    .set_index('game_id')\n",
    "loc_neutral['game_loc_team_ref'] = None\n",
    "\n",
    "loc_home = home_teams[home_teams['game_location'].isnull()]\\\n",
    "                    .set_index('game_id')\n",
    "loc_all = loc_home.append(loc_neutral).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.426Z"
    }
   },
   "outputs": [],
   "source": [
    "loc_all['home_team_src'] = loc_all[['game_location','game_loc_team_ref']].apply(\n",
    "    lambda x: x[1] if x[0] != 'N' else None,\n",
    "    axis=1\n",
    ")\n",
    "loc_all = loc_all[['game_id','home_team_src']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.431Z"
    }
   },
   "outputs": [],
   "source": [
    "assert games_lines_info_homeaway.shape[0] == games_lines_info_homeaway.merge(\n",
    "        loc_all, left_on='game_id', right_on='game_id'\n",
    "    ).shape[0]\n",
    "\n",
    "games_lines_info_homeaway = games_lines_info_homeaway.merge(\n",
    "        loc_all, left_on='game_id', right_on='game_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### home team sanity check\n",
    "* home teams mismatch only in games originally tagged as neutral\n",
    "* this includes games in Wembley, \n",
    "* result\n",
    "  * satisfied with the original tagging\n",
    "  * the official tagging in neutral games is unnecessary\n",
    "  * leaving UK/Mexico games, Jets vs. Giants games, and moved games as neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.441Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mismatches = games_lines_info_homeaway[\n",
    "    games_lines_info_homeaway[['home_team','home_team_src']].apply(\n",
    "        lambda x: x[0] != x[1], \n",
    "        axis=1\n",
    "    )][['team0_fullname','team1_fullname','stadium','home_team',\n",
    "     'home_team_src','game_id','stadium_home_team']]\n",
    "\n",
    "print 'SUPER BOWLS'\n",
    "mismatches[\n",
    "    (mismatches['home_team'] == 'neutral')\n",
    "    & (mismatches['home_team_src'].apply(lambda x: x is None))\n",
    "]\n",
    "\n",
    "print 'GAMES TAGGED AS NEUTRAL'\n",
    "mismatches[\n",
    "    (mismatches['home_team_src'].apply(lambda x: x is not None))\n",
    "].sort_values(by='stadium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.445Z"
    }
   },
   "outputs": [],
   "source": [
    "games_lines_info_homeaway.drop('home_team_src', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.449Z"
    }
   },
   "outputs": [],
   "source": [
    "divisions_df = pd.read_csv('team_divisions.csv', index_col='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.454Z"
    }
   },
   "outputs": [],
   "source": [
    "before = games_lines_info_homeaway.shape[0]\n",
    "for nteam in [0,1]:\n",
    "    games_lines_info_homeaway = games_lines_info_homeaway.merge(\n",
    "        divisions_df.rename(\n",
    "            columns={'division': 'team{}_division'.format(nteam)}\n",
    "        ),\n",
    "        left_on=['season','team{}_fullname'.format(nteam)],\n",
    "        right_on=['season','team']\n",
    "    ).drop('team', axis=1)\n",
    "    assert games_lines_info_homeaway.shape[0] == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.461Z"
    }
   },
   "outputs": [],
   "source": [
    "def reorder_cols(df, front_cols):\n",
    "    '''takes a DF and list of columns \n",
    "    to move to the front (in order).\n",
    "    returns DF'''\n",
    "    return df[\n",
    "        front_cols \n",
    "        + filter(\n",
    "            lambda x: x not in front_cols, \n",
    "            df.columns\n",
    "        )]\n",
    "    \n",
    "front_cols = ['game_id','season','week_id']\n",
    "base_df = reorder_cols(games_lines_info_homeaway, front_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Hive Table: __games_denorm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up column names\n",
    "* make all columns lowercase\n",
    "* change all team0 --> h, team1 --> v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.466Z"
    }
   },
   "outputs": [],
   "source": [
    "## make columns all lowercase\n",
    "base_df.columns = map(lambda x: x.lower(), base_df.columns)\n",
    "\n",
    "## rename team0 and team1 to h and v, respectively\n",
    "team_fields = filter(\n",
    "    lambda x: x.startswith('team0_') or x.startswith('team1_'), \n",
    "    base_df.columns\n",
    ")\n",
    "\n",
    "team_field_rename = dict(\n",
    "    map(\n",
    "        lambda x: (x, x.replace('team0_','h_').replace('team1_','v_')), \n",
    "        team_fields\n",
    "    )\n",
    ")\n",
    "\n",
    "## modify team IDs to match the same convention\n",
    "team_field_rename['team0'] = 'h_team_id'\n",
    "team_field_rename['team1'] = 'v_team_id'\n",
    "\n",
    "## execute name changes\n",
    "base_df.rename(columns=team_field_rename, inplace=True)\n",
    "\n",
    "## don't need coach_raw (PFR link)\n",
    "base_df = base_df.drop(['h_coach_raw','v_coach_raw'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.471Z"
    }
   },
   "outputs": [],
   "source": [
    "print 'RENAMING'\n",
    "for k in sorted(team_field_rename.keys()):\n",
    "    print '{}\\t->\\t{}'.format(k, team_field_rename[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.475Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "        cast_dtypes(\n",
    "            base_df\n",
    "        )\n",
    "    ).write.mode('overwrite').saveAsTable('games_denorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Denormalized Schema and Write Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.482Z"
    }
   },
   "outputs": [],
   "source": [
    "## union h and v fields and get distinct values\n",
    "def combine_h_v(full_df, all_fields, non_team_fields):\n",
    "    '''given a DataFrame with 2 teams:\n",
    "    * split into 2 DFs by h and v\n",
    "    * rename and align schemas\n",
    "    * union\n",
    "    * drop duplicates\n",
    "    * return a DataFrame to be converted into Hive table'''\n",
    "    for team_i in ['h','v']:\n",
    "        prefix = '{}_'.format(team_i)\n",
    "        fields = [\n",
    "            a for a in all_fields\n",
    "            if a.startswith(prefix)\n",
    "        ]\n",
    "        curr_table = full_df[non_team_fields + fields]\n",
    "        ## team out team0 and team1 to align schemas\n",
    "        field_map = dict(map(\n",
    "                lambda x: ( x, '_'.join(x.split('_')[1:]) ), \n",
    "                fields\n",
    "            ))\n",
    "        curr_table = curr_table.rename(columns=field_map)\n",
    "\n",
    "        if team_i == 'h':\n",
    "            curr_df = curr_table\n",
    "        else:\n",
    "            curr_df = curr_df.append(\n",
    "                curr_table[curr_df.columns]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "    return curr_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __team_season__\n",
    "* #### primary key: (season, team_id)\n",
    "* #### foreign key: team_id --> game(season, h_team_id)\n",
    "* #### foreign key: team_id --> game(season, v_team_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.487Z"
    }
   },
   "outputs": [],
   "source": [
    "key = ['season','team_id']\n",
    "team_season_fields = [\n",
    "    'season',\n",
    "    'h_team_id','h_link','h_name','h_fullname','h_city','h_division',\n",
    "    'v_team_id','v_link','v_name','v_fullname','v_city','v_division'\n",
    "]\n",
    "\n",
    "team_season_df = combine_h_v(base_df.copy(), team_season_fields, ['season'])\n",
    "\n",
    "## assert that every season has the same amount of teams\n",
    "assert team_season_df['season'].value_counts().min() \\\n",
    "        == team_season_df['season'].value_counts().max()\n",
    "\n",
    "## assert key (season, link) is unique\n",
    "assert team_season_df.groupby(key).size().max() == 1\n",
    "for h_v in ['h','v']:\n",
    "    fkey = ['season','{}_team_id'.format(h_v)]\n",
    "    assert base_df.shape[0] == base_df[fkey].merge(\n",
    "                                        team_season_df, left_on=fkey, \n",
    "                                        right_on=key\n",
    "                                    ).shape[0]\n",
    "\n",
    "\n",
    "team_season_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.492Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "team_season_df['name'].value_counts().shape[0]\n",
    "team_season_df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.497Z"
    }
   },
   "outputs": [],
   "source": [
    "team_season_df.shape[0]\n",
    "team_season_df = team_season_df.drop_duplicates()\n",
    "team_season_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.501Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(team_season_df)\n",
    ").write.mode('overwrite').saveAsTable('team_season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.506Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df.columns) \\\n",
    "                    - set(team_season_fields) \\\n",
    "                    | set(['season','h_team_id','v_team_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __coach__\n",
    "* #### primary key: coach_id\n",
    "* #### foreign key: coach_id --> game(h_coach_id)\n",
    "* #### foreign key: coach_id --> game(v_coach_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.511Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "key = ['coach_id']\n",
    "coach_fields = [\n",
    "    'h_coach_name','h_coach_id',\n",
    "    'v_coach_name','v_coach_id'\n",
    "]\n",
    "\n",
    "coach_df = combine_h_v(base_df.copy(), coach_fields, [])\n",
    "## assert coach ID is unique\n",
    "assert coach_df.groupby(key).size().max() == 1\n",
    "## assert join is 1:many\n",
    "for h_v in ['h','v']:\n",
    "    fkey = ['{}_coach_id'.format(h_v)]\n",
    "    assert base_df.shape[0] == base_df[fkey].merge(\n",
    "                                        coach_df, left_on=fkey, \n",
    "                                        right_on=key\n",
    "                                    ).shape[0]\n",
    "\n",
    "\n",
    "coach_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.516Z"
    }
   },
   "outputs": [],
   "source": [
    "coach_df.shape[0]\n",
    "coach_df = coach_df.drop_duplicates()\n",
    "coach_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.520Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(coach_df)\n",
    ").write.mode('overwrite').saveAsTable('coach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.524Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(coach_fields) \\\n",
    "                    | set(['h_coach_id','v_coach_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T03:58:10.166924Z",
     "start_time": "2018-11-19T03:58:10.157897Z"
    }
   },
   "source": [
    "### __stadium__\n",
    "* #### primary key: stadium\n",
    "* #### foreign key: stadium --> game(stadium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.529Z"
    }
   },
   "outputs": [],
   "source": [
    "## roof and surface are game metadata since it can change\n",
    "## within a season\n",
    "key = ['stadium']\n",
    "stadium_fields = [\n",
    "    'stadium_link','lat','long',\n",
    "    'timezone','stadium_home_team'\n",
    "]\n",
    "\n",
    "stadium_df = base_df[key + stadium_fields].drop_duplicates()\n",
    "## assert stadium name is unique\n",
    "assert stadium_df.groupby(key).size().max() == 1\n",
    "\n",
    "## assert join is 1:many\n",
    "fkey = ['stadium']\n",
    "assert base_df.shape[0] == base_df[fkey].merge(\n",
    "                                    stadium_df, left_on=fkey, \n",
    "                                    right_on=key\n",
    "                                ).shape[0]\n",
    "\n",
    "\n",
    "stadium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.534Z"
    }
   },
   "outputs": [],
   "source": [
    "stadium_df.shape[0]\n",
    "stadium_df = stadium_df.drop_duplicates()\n",
    "stadium_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.538Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(stadium_df)\n",
    ").write.mode('overwrite').saveAsTable('stadium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.543Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(stadium_fields) \\\n",
    "                    | set(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T07:24:01.447853Z",
     "start_time": "2018-12-29T07:24:01.443026Z"
    }
   },
   "source": [
    "### __game_line__\n",
    "* #### primary key: game_id\n",
    "* #### foreign key: game_id --> game(game_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.548Z"
    }
   },
   "outputs": [],
   "source": [
    "key = ['game_id']\n",
    "line_fields = key + [\n",
    "    'ml_fav','h_ml','v_ml',\n",
    "    'open_fav','open_spread',\n",
    "    'close_fav','close_spread',\n",
    "    '2h_fav','2h_spread',\n",
    "    'open_ou','close_ou','2h_ou',\n",
    "    'pfr_line','pfr_ou'\n",
    "]\n",
    "\n",
    "lines = base_df[line_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract line info from PFR text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.553Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "game_teams = base_df[['game_id','h_fullname','v_fullname','h_team_id','v_team_id']]\n",
    "before = lines.shape[0]\n",
    "lines = lines.merge(game_teams, left_on='game_id', right_on='game_id')\n",
    "assert lines.shape[0] == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.558Z"
    }
   },
   "outputs": [],
   "source": [
    "## Open_Fav and Close_Fav have a H/V value for pick'ems\n",
    "## replace those with 'PK'\n",
    "for oc in ['open','close']:\n",
    "    lines[oc + '_fav'] = lines[[oc + '_fav', oc + '_spread']].apply(\n",
    "        lambda x: 'PK' if x[1] == 0 else x[0], \n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.563Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_pfr_fav(x):\n",
    "    '''pandas UDF to take pfr line in form of\n",
    "    CITY NAME -SPREAD and returns the favored team\n",
    "    or \"PK\" if it\\'s a pick\\'em'''\n",
    "    spread, line_txt = x\n",
    "    if spread == 0:\n",
    "        return 'PK'\n",
    "    else: \n",
    "        return ' '.join(line_txt.split(' ')[:-1])\n",
    "\n",
    "def extract_pfr_fav_id(x):\n",
    "    pfr_fav, h_fullname, v_fullname, h_team, v_team = x\n",
    "    if pfr_fav == 'PK':\n",
    "        return ['PK', None]\n",
    "    elif pfr_fav == h_fullname:\n",
    "        return ['H', h_team]\n",
    "    elif pfr_fav == v_fullname:\n",
    "        return ['V', v_team]\n",
    "    else:\n",
    "        return 'FAILED'\n",
    "\n",
    "lines['pfr_spread'] = lines['pfr_line'].apply(\n",
    "    lambda x: abs(\n",
    "        float(x.split(' ')[-1]\n",
    "               .replace('Pick','0')))\n",
    ")\n",
    "\n",
    "## extract the PFR favorite full team name \n",
    "lines['pfr_fav'] = lines[['pfr_spread','pfr_line']].apply(\n",
    "    extract_pfr_fav, axis=1\n",
    ")\n",
    "\n",
    "## extract whether favorite is H/V and get the team ID\n",
    "lines['pfr_fav'] = lines[\n",
    "        ['pfr_fav','h_fullname','v_fullname','h_team_id','v_team_id']\n",
    "    ].apply(\n",
    "        extract_pfr_fav_id, axis=1\n",
    "    )\n",
    "lines['pfr_fav_id'] = lines['pfr_fav'].apply(lambda x: x[1])\n",
    "lines['pfr_fav'] = lines['pfr_fav'].apply(lambda x: x[0])\n",
    "\n",
    "assert 'FAILED' not in lines['pfr_fav'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.567Z"
    }
   },
   "outputs": [],
   "source": [
    "lines.drop(['h_fullname','v_fullname','h_team_id','v_team_id','pfr_line'], \n",
    "           axis=1, inplace=True)\n",
    "\n",
    "## Drop processed column\n",
    "drops = ['pfr_line']\n",
    "base_df.drop(drops, axis=1, inplace=True)\n",
    "base_df_remaining = filter(lambda x: x not in drops, base_df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.572Z"
    }
   },
   "outputs": [],
   "source": [
    "lines['pfr_ou'] = lines['pfr_ou'].apply(lambda x: float(x.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.576Z"
    }
   },
   "outputs": [],
   "source": [
    "lines.shape[0]\n",
    "lines = lines.drop_duplicates()\n",
    "lines.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.581Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert game_id is unique in line\n",
    "assert lines.groupby(key).size().max() == 1\n",
    "## assert join is 1:1\n",
    "assert base_df.shape[0] == base_df[key].merge(lines, left_on=key, \n",
    "                                    right_on=key).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.585Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(lines)\n",
    ").write.mode('overwrite').saveAsTable('game_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.590Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(line_fields) \\\n",
    "                    | set(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T07:28:43.269181Z",
     "start_time": "2018-12-29T07:28:43.263882Z"
    }
   },
   "source": [
    "### __game_outcome__\n",
    "* #### primary key: game_id\n",
    "* #### foreign key: game_id --> game(game_id)\n",
    "* v_final is v_q1 + v_q2 + ...\n",
    "* v_score comes from PFR (renamed from team1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that PFR final score ({h,v}\\_final) and Lines Final ({h,v}\\_final) are the same\n",
    "* in one game they are not: 201610020pit\n",
    "* https://www.pro-football-reference.com/boxscores/201610020pit.htm\n",
    "* Chiefs scored 14 in Q4, but v_q4 shows 7. this is incorrect.\n",
    "* After the fix, {h,v}\\_final == {h,v}\\_score always, so drop {h,v}\\_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.596Z"
    }
   },
   "outputs": [],
   "source": [
    "## find and display any final score mismatches\n",
    "mismatches = base_df[\n",
    " (base_df['v_final'] != base_df['v_score'])\n",
    " | (base_df['h_final'] != base_df['h_score'])\n",
    "]\n",
    "mismatches[['game_id','v_final','v_score','v_q1','v_q2','v_q3','v_q4']]\n",
    "\n",
    "## Fix 201610020pit by changing Chiefs Q4 score (and total) to 14\n",
    "idx = mismatches[mismatches['game_id'] == '201610020pit'].index\n",
    "base_df.loc[idx, 'v_q4'] = 14\n",
    "base_df.loc[idx, 'v_final'] = 14\n",
    "\n",
    "## Check that there are no more mismatches\n",
    "assert base_df[\n",
    " (base_df['v_final'] != base_df['v_score'])\n",
    " | (base_df['h_final'] != base_df['h_score'])\n",
    "].shape[0] == 0\n",
    "\n",
    "## Drop the superfluous columns\n",
    "drops = ['h_score','v_score']\n",
    "base_df.drop(drops, axis=1, inplace=True)\n",
    "base_df_remaining = filter(lambda x: x not in drops, base_df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.601Z"
    }
   },
   "outputs": [],
   "source": [
    "key = ['game_id']\n",
    "outcome_fields = key + [\n",
    "    'h_final','h_q1','h_q2','h_q3','h_q4',\n",
    "    'v_final','v_q1','v_q2','v_q3','v_q4']\n",
    "\n",
    "outcomes = base_df[outcome_fields]\n",
    "\n",
    "## assert game_id is unique in line\n",
    "assert outcomes.groupby(key).size().max() == 1\n",
    "## assert join is 1:1\n",
    "assert base_df.shape[0] == base_df[key].merge(outcomes, left_on=key, \n",
    "                                               right_on=key).shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines data does not include OT (only Q1-Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp1 = outcomes[\n",
    "        outcomes.loc[:, ['h_q1','h_q2','h_q3','h_q4']].sum(axis=1) \n",
    "        != outcomes.loc[:, 'h_final']\n",
    "    ]\n",
    "tmp1.loc[:, 'diff'] = tmp1[['h_q1','h_q2','h_q3','h_q4']].sum(axis=1) \\\n",
    "                        - tmp1['h_final']\n",
    "tmp1 = tmp1.loc[:, ['game_id','h_q1','h_q2','h_q3','h_q4','h_final','diff']]\n",
    "\n",
    "tmp2 = outcomes[\n",
    "        outcomes.loc[:, ['v_q1','v_q2','v_q3','v_q4']].sum(axis=1) \n",
    "        != outcomes['v_final']\n",
    "    ]\n",
    "tmp2.loc[:, 'diff'] = tmp2.loc[:, ['v_q1','v_q2','v_q3','v_q4']].sum(axis=1) \\\n",
    "                        - tmp2['v_final']\n",
    "tmp2 = tmp2[['game_id','v_q1','v_q2','v_q3','v_q4','h_final','diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.612Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp1.sort_values(by=['diff']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.617Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp2.sort_values(by='diff').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.621Z"
    }
   },
   "outputs": [],
   "source": [
    "outcomes.loc[:, 'h_ot'] = outcomes['h_final'] - outcomes[['h_q1','h_q2','h_q3','h_q4']].sum(axis=1)\n",
    "outcomes.loc[:, 'v_ot'] = outcomes['v_final'] - outcomes[['v_q1','v_q2','v_q3','v_q4']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.625Z"
    }
   },
   "outputs": [],
   "source": [
    "outcomes.shape[0]\n",
    "outcomes = outcomes.drop_duplicates()\n",
    "outcomes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.631Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(outcomes)\n",
    ").write.mode('overwrite').saveAsTable('game_outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.637Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(outcomes) \\\n",
    "                    | set(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __game_metadata__\n",
    "* #### primary key: game_id\n",
    "* #### foreign key: game_id --> game(game_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.642Z"
    }
   },
   "outputs": [],
   "source": [
    "key = ['game_id']\n",
    "metadata_fields = key + ['day_of_week','duration','prf_weather','roof',\n",
    "                         'surface','time','attendance']\n",
    "\n",
    "metadata_df = base_df[metadata_fields]\n",
    "\n",
    "## assert game_id is unique in metadata\n",
    "assert metadata_df.groupby(key).size().max() == 1\n",
    "## assert join is 1:1\n",
    "assert base_df.shape[0] == base_df[key].merge(metadata_df, left_on=key, \n",
    "                                          right_on=key).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.647Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_df.shape[0]\n",
    "metadata_df = metadata_df.drop_duplicates()\n",
    "metadata_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.652Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(metadata_df)\n",
    ").write.mode('overwrite').mode('overwrite').saveAsTable('game_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.660Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(metadata_fields) \\\n",
    "                    - set(['home_team']) \\\n",
    "                    | set(key)\n",
    "base_df[sorted(list(base_df_remaining))].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __game__\n",
    "* #### primary key: game_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.665Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df['is_neutral'] = (base_df['home_team'] == 'neutral').astype(int)\n",
    "\n",
    "## Drop superfluous column\n",
    "drops = ['home_team']\n",
    "base_df.drop(drops, axis=1, inplace=True)\n",
    "base_df_remaining = filter(lambda x: x not in drops, base_df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.670Z"
    }
   },
   "outputs": [],
   "source": [
    "base_cols = sorted(list(base_df_remaining))\n",
    "key = ['game_id']\n",
    "ordered_cols = ['season','week_id','date','is_neutral']\n",
    "\n",
    "game = base_df[\n",
    "    key \n",
    "    + ordered_cols \n",
    "    + list( set(base_cols) - set(key) - set(ordered_cols) )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.675Z"
    }
   },
   "outputs": [],
   "source": [
    "game.shape[0]\n",
    "game = game.drop_duplicates()\n",
    "game.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.682Z"
    }
   },
   "outputs": [],
   "source": [
    "game.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.686Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(game)\n",
    ").write.mode('overwrite').mode('overwrite').saveAsTable('game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.691Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_remaining = set(base_df_remaining) \\\n",
    "                    - set(game)\n",
    "\n",
    "assert base_df[sorted(list(base_df_remaining))].head().T.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __dvoa__\n",
    "* #### primary key: (team_id, season, week_id)\n",
    "* #### foreign key: (team_id, season, week_id) --> game((h_team_id, season, week_id))\n",
    "* #### foreign key: (team_id, season, week_id) --> game((v_team_id, season, week_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.696Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = ['team_id','season','week_id']\n",
    "dvoa_df = pd.read_csv('dvoa/dvoa_alltime.csv', index_col='Unnamed: 0')\n",
    "## no cases in hive table\n",
    "dvoa_df.columns = map(lambda x: x.replace(' ','_'), dvoa_df)\n",
    "dvoa_df.rename(columns={'week':'week_id'}, inplace=True)\n",
    "## headers are in data from appending dataframes. filter them out\n",
    "dvoa_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temporarily remove 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.701Z"
    }
   },
   "outputs": [],
   "source": [
    "dvoa_df = dvoa_df[dvoa_df['season'] != 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.706Z"
    }
   },
   "outputs": [],
   "source": [
    "dvoa_df['season'].value_counts().sort_index()\n",
    "dvoa_df['week_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.710Z"
    }
   },
   "outputs": [],
   "source": [
    "team_season = spark.table('team_season').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.715Z"
    }
   },
   "outputs": [],
   "source": [
    "team_season_years = set(\n",
    "    team_season\n",
    "        .groupby('season')\n",
    "        .count()\n",
    "        .toPandas()['season'].tolist()\n",
    ")\n",
    "dvoa_years = set(dvoa_df['season'].value_counts().index)\n",
    "\n",
    "## assert years perfectly overlap\n",
    "## 2018 missing from team_season_df\n",
    "assert team_season_years == dvoa_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.721Z"
    }
   },
   "outputs": [],
   "source": [
    "team_season_teams = set(\n",
    "    team_season\n",
    "        .groupby('name')\n",
    "        .count()\n",
    "        .toPandas()['name'].tolist()\n",
    ")\n",
    "dvoa_teams = set(dvoa_df['name'].value_counts().index)\n",
    "\n",
    "## assert team names perfectly overlap\n",
    "assert team_season_teams == dvoa_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.725Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#### assert that dvoa_df joins cleanly with team_season table\n",
    "dvoa_size = dvoa_df.shape[0]\n",
    "team_season_size = team_season.count()\n",
    "\n",
    "join_key = ['name','season']\n",
    "join_size = dvoa_df.merge(\n",
    "    team_season.toPandas(), left_on=join_key, right_on=join_key\n",
    ").shape[0]\n",
    "\n",
    "## assert that dvoa_df hits all joins with team_season_df\n",
    "## missing 2018 in team_season_df\n",
    "assert dvoa_size == join_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.730Z"
    }
   },
   "outputs": [],
   "source": [
    "### convert % columns to floats\n",
    "import numpy as np\n",
    "\n",
    "pct_cols = [\n",
    "    'TOTAL_DAVE','WEIGHTEDDVOA','TOTALDVOA','OFFENSEDVOA','DEFENSEDVOA',\n",
    "    'S.T.DVOA','PAST_SCHED','FUTURE_SCHED','DAVE_OR_WTDDVOA','VAR'\n",
    "]\n",
    "for p in pct_cols:\n",
    "    dvoa_df[p] = dvoa_df[p].str.replace('%','').astype(float).fillna(np.nan)\n",
    "    \n",
    "dvoa_df.columns = map(lambda x: x.lower()\n",
    "                                 .replace('.','_')\n",
    "                                 .replace('-','_'), \n",
    "                      dvoa_df.columns\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.735Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert 32 teams\n",
    "tmp = dvoa_df['name'].value_counts()\n",
    "assert tmp.shape[0] == 32\n",
    "## assert every team plays the same amount of games\n",
    "assert tmp.max() == tmp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.740Z"
    }
   },
   "outputs": [],
   "source": [
    "join_key = ['name','season']\n",
    "\n",
    "team_names = spark.table('team').select(*(['team_id'] + join_key)).toPandas()\n",
    "before = dvoa_df.shape[0]\n",
    "dvoa_df = dvoa_df.merge(\n",
    "        team_names, left_on=join_key, right_on=join_key\n",
    "    ).drop(\n",
    "        'name', axis=1\n",
    "    )\n",
    "assert dvoa_df.shape[0] == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.743Z"
    }
   },
   "outputs": [],
   "source": [
    "dvoa_df.shape[0]\n",
    "dvoa_df = dvoa_df.drop_duplicates()\n",
    "dvoa_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.748Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_rest = list(set(dvoa_df.columns) - set(key))\n",
    "dvoa_df = dvoa_df[key + cols_rest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since playoffs DVOA is not available, set week 18 to all weeks beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.754Z"
    }
   },
   "outputs": [],
   "source": [
    "repeats = dvoa_df[dvoa_df['week_id'] == 18]\n",
    "repeats.head()\n",
    "\n",
    "for wk in np.arange(19, 22):\n",
    "    repeats['week_id'] = wk\n",
    "    dvoa_df = dvoa_df.append(repeats).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T05:01:51.229047Z",
     "start_time": "2019-01-05T05:01:51.215668Z"
    }
   },
   "source": [
    "#### Week 1 DVOA isn't available. add NULL rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.760Z"
    }
   },
   "outputs": [],
   "source": [
    "skip = ['team_id','season','week_id']\n",
    "null_rows = dvoa_df[\n",
    "    (dvoa_df['season'] == 2010) & (dvoa_df['week_id'] == 10)\n",
    "]\n",
    "null_rows['week_id'] = 1\n",
    "for col in filter(lambda x: x not in skip, null_rows.columns):\n",
    "    null_rows[col] = np.nan\n",
    "\n",
    "for season in dvoa_df['season'].unique():\n",
    "    null_rows['season'] = season\n",
    "    dvoa_df = dvoa_df.append(null_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.765Z"
    }
   },
   "outputs": [],
   "source": [
    "dvoa_df['week_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.770Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert game_id is unique in metadata\n",
    "assert dvoa_df.groupby(key).size().max() == 1\n",
    "## assert join is 1:many\n",
    "for h_v in ['h','v']:\n",
    "    fkey = ['{}_team_id'.format(h_v),'season','week_id']\n",
    "    assert base_df.shape[0] == base_df[fkey].merge(\n",
    "                                        dvoa_df, left_on=fkey, \n",
    "                                        right_on=key\n",
    "                                    ).shape[0]\n",
    "\n",
    "## assert keys are unique\n",
    "assert base_df.groupby(fkey).size().max() == 1\n",
    "assert dvoa_df.groupby(key).size().max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T22:39:57.774Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    cast_dtypes(dvoa_df)\n",
    ").write.mode('overwrite').mode('overwrite').saveAsTable('dvoa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
