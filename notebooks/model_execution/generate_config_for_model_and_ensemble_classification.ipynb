{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:30.343082Z",
     "start_time": "2019-08-30T14:33:30.316904Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set root dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:30.367588Z",
     "start_time": "2019-08-30T14:33:30.345717Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT='../..'\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:32.494386Z",
     "start_time": "2019-08-30T14:33:30.371584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run config/initialize_nospark.ipynb\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.461244Z",
     "start_time": "2019-08-30T14:33:32.497821Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('model_pipeline')\n",
    "from Ensemble import Ensemble\n",
    "from ExecuteModelPipeline import ExecuteModelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.524738Z",
     "start_time": "2019-08-30T14:33:34.462863Z"
    }
   },
   "outputs": [],
   "source": [
    "# toggle whether to train the models \n",
    "# or just create the JSON files\n",
    "DO_EXECUTE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location of base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.579032Z",
     "start_time": "2019-08-30T14:33:34.526890Z"
    }
   },
   "outputs": [],
   "source": [
    "MODELS_DIR='models'\n",
    "MODEL_ID = '{}_regression'.format(str(date.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary version of model.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.635171Z",
     "start_time": "2019-08-30T14:33:34.581019Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'models_dir': MODELS_DIR\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data for model\n",
    "* features\n",
    "* labels\n",
    "* Note: can modify/create tables here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.704285Z",
     "start_time": "2019-08-30T14:33:34.639381Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['features_tbl'] = 'features.combined_0601'\n",
    "model_dict['labels_tbl'] = 'labels.combined_0601'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.914375Z",
     "start_time": "2019-08-30T14:33:34.708362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = list(pd.read_csv('data/features/combined_0601.csv').columns[3:])\n",
    "len(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns from tables\n",
    "* index: unique identifier in features/labels table (must be in both)\n",
    "* label column, and indicator of what is a positive label\n",
    "  * currently not supported: multi-class\n",
    "  * code will binarize\n",
    "* list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:34.997100Z",
     "start_time": "2019-08-30T14:33:34.917161Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['index'] = ['game_id']\n",
    "model_dict['label_col'] = 'did_win'\n",
    "model_dict['pos_labels'] = [1]\n",
    "model_dict['neg_labels'] = [-1]\n",
    "model_dict['features_list'] = all_features\n",
    "\n",
    "model_dict['features_list'].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Sets\n",
    "* random seeds for reproducibility\n",
    "* number of folds for cross-validation (value of <= 1 doesn't do k-fold\n",
    "* global_dataset_proportions\n",
    " * proportion of the data for each of training, scoring only, holdout, and throwaway\n",
    " * generated using stratified sampling\n",
    "* dimensional_dataset_proportions\n",
    " * post-processing after global_dataset_proportions\n",
    " * idea is to move specific field values, e.g. move certain seasons to the holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: use cross-validation data from another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.076546Z",
     "start_time": "2019-08-30T14:33:35.001124Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model_cv_to_use'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV parameters, when not using another model CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample usage for Dimensional Dataset Proportions\n",
    "```python\n",
    "model_dict['dimensional_dataset_proportions'] = {\n",
    "        'throw_away': [\n",
    "            {\n",
    "                'vals': [\n",
    "                    0\n",
    "                ], \n",
    "                'dim': 'is_home',\n",
    "                'prop_to_move': 1.0, \n",
    "                'from_groups': [\n",
    "                    'training',\n",
    "                    'holdout',\n",
    "                    'scoring_only'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.148394Z",
     "start_time": "2019-08-30T14:33:35.081446Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['fold_seed'] = 99\n",
    "model_dict['dataset_seed'] = 9\n",
    "model_dict['kfolds'] = 5\n",
    "model_dict['strata_cols'] = ['season','week_id']\n",
    "\n",
    "model_dict['global_dataset_proportions'] = {\n",
    "        'training': 1.,\n",
    "        'holdout': 0,\n",
    "        'throw_away': 0,\n",
    "        'scoring_only': 0\n",
    "    }\n",
    "\n",
    "# DEFAULT: model_dict['dimensional_dataset_proportions'] = {}\n",
    "model_dict['dimensional_dataset_proportions'] = {\n",
    "        'holdout': [\n",
    "            {\n",
    "                'vals': [\n",
    "                    17, \n",
    "                    18, \n",
    "                    19, \n",
    "                    20, \n",
    "                    21, \n",
    "                    22\n",
    "                ], \n",
    "                'dim': 'week_id', \n",
    "                'prop_to_move': 1.0, \n",
    "                'from_groups': [\n",
    "                    'training', \n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        'holdout': [{\n",
    "            'vals': [2016,2017],\n",
    "            'dim': 'season',\n",
    "            'prop_to_move': 1.0,\n",
    "            'from_groups': ['training', 'scoring_only']}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice\n",
    "* package/class name as a string\n",
    "* parameters as a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.210787Z",
     "start_time": "2019-08-30T14:33:35.150727Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_dict['model'] = 'sklearn.ensemble.GradientBoostingClassifier'\n",
    "# model_dict['model_params'] = {\n",
    "#     'learning_rate': 0.1, \n",
    "#     'n_estimators': 200, \n",
    "#     'max_features': 'auto', \n",
    "#     'subsample': 0.9, \n",
    "#     'random_state': 9, \n",
    "#     'max_depth': 12, \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.269330Z",
     "start_time": "2019-08-30T14:33:35.212661Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model'] = 'xgboost.XGBClassifier'\n",
    "model_dict['model_params'] = {\n",
    "        'n_jobs': 1,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100, \n",
    "        'max_features': 'auto', \n",
    "        'booster': 'gbtree', \n",
    "        'silent': True, \n",
    "        'nthread': None, \n",
    "        'subsample': 0.5, \n",
    "        'random_state': 9, \n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6, \n",
    "        'gamma': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions to perform and data to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.325714Z",
     "start_time": "2019-08-30T14:33:35.271157Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['save'] = {\n",
    "    'cv_data': True,\n",
    "    'serialized_models': False,\n",
    "    'cv_scores': True,\n",
    "    'holdout_scores': False\n",
    "}\n",
    "\n",
    "model_dict['actions'] = {\n",
    "        'do_train_and_score_cv': True,\n",
    "        'do_score_holdout': False,\n",
    "        'do_evaluate': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out model.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.381294Z",
     "start_time": "2019-08-30T14:33:35.327997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/model__classification_example.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json_path = 'model_configs/model__classification_example.json'\n",
    "model_json_path\n",
    "\n",
    "with open(model_json_path,'w') as w:\n",
    "    json.dump(model_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dictionary version of evaluate.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.431836Z",
     "start_time": "2019-08-30T14:33:35.383612Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'models_dir': MODELS_DIR,\n",
    "    'regression_evaluation': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Labels\n",
    "* labels --> names (note: keys should be strings)\n",
    "* name for success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.495050Z",
     "start_time": "2019-08-30T14:33:35.434024Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict['label_map'] = {\n",
    "    '1': 'Won',\n",
    "    '0': 'Lost'\n",
    "}\n",
    "evaluate_dict['success_name'] = 'Win Rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins to plot\n",
    "* plot_bins: \n",
    "   * Number of bins to plot (i.e. number of bars on the bar chart)\n",
    "* bin_types:\n",
    "   * \"Bin\" puts scores into uniform bins, e.g. [0, 0.10], (0.10, 0.20], ..., (0.9, 1.0]\n",
    "   * \"Percentile\" bins scores into ntiles determined by plot_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.566141Z",
     "start_time": "2019-08-30T14:33:35.500535Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict['bin_types'] = ['Bin', 'Percentile']\n",
    "evaluate_dict['plot_bins'] = [10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Metrics to Plot\n",
    "* metrics evaluated at each of 100 score threshold points\n",
    "* currently only supports Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.628711Z",
     "start_time": "2019-08-30T14:33:35.568603Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict['threshold_metrics'] = ['Accuracy','F1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy at Top 'N' plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.687163Z",
     "start_time": "2019-08-30T14:33:35.631671Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict['accuracy_at_topn'] = {\n",
    "        'week_id__season': [1, 16],\n",
    "        'season': [1, 200, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.747703Z",
     "start_time": "2019-08-30T14:33:35.689625Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_dict['to_plot'] = {\n",
    "    'ridge': True,\n",
    "    'thresholds': True,\n",
    "    'bins': True,\n",
    "    'roc': True,\n",
    "    'accuracy_by_top_n': True,\n",
    "    'shap__feature_importance': True,\n",
    "    'shap__dependence_plots': False,\n",
    "    'feature_importance': True\n",
    "}\n",
    "\n",
    "evaluate_dict['save'] = {\n",
    "    'plots': False,\n",
    "    'data': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out evaluate.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.826340Z",
     "start_time": "2019-08-30T14:33:35.752895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/evaluate__classification_example.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_json_path = 'model_configs/evaluate__classification_example.json'\n",
    "eval_json_path\n",
    "\n",
    "with open(eval_json_path,'w') as w:\n",
    "    json.dump(evaluate_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.890650Z",
     "start_time": "2019-08-30T14:33:35.829857Z"
    }
   },
   "outputs": [],
   "source": [
    "if DO_EXECUTE:\n",
    "    model_pipeline = ExecuteModelPipeline(model_json_path, eval_json_path, 'Y')\n",
    "    model_pipeline.execute_model_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an ensemble, using the above model as a template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the sub-model template: modify model.json (model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstrap: select 50% of the data for each model, with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:35.958326Z",
     "start_time": "2019-08-30T14:33:35.893801Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['global_dataset_proportions']['training'] = 0.5\n",
    "model_dict['global_dataset_proportions']['throw_away'] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save options for ensemble's sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.035741Z",
     "start_time": "2019-08-30T14:33:35.961392Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['save'] = {\n",
    "    'cv_data': True,\n",
    "    'serialized_models': False,\n",
    "    'cv_scores': True,\n",
    "    'holdout_scores': False\n",
    "}\n",
    "model_dict['actions'] = {\n",
    "    'do_train_and_score_cv': True,\n",
    "    'do_score_holdout': False,\n",
    "    'do_evaluate': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store ensemble submodel JSON for use in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.115621Z",
     "start_time": "2019-08-30T14:33:36.039022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/ensemble_submodel_model__classification_example.json'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_submodel_path = 'model_configs/ensemble_submodel_model__classification_example.json'\n",
    "ensemble_submodel_path\n",
    "json.dump(model_dict, open(ensemble_submodel_path,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify evaluate.json (evaluate_dict) if needed\n",
    "* toggle which plots to include\n",
    "* toggle whether to save evaluation data\n",
    "* toggle whether to save plots as png files or display (in notebook only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.213047Z",
     "start_time": "2019-08-30T14:33:36.121065Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_dict['to_plot'] = {\n",
    "#     'ridge': True,\n",
    "#     'thresholds': True,\n",
    "#     'bins': True,\n",
    "#     'roc': True,\n",
    "#     'accuracy_by_top_n': True,\n",
    "#     'shap__feature_importance': True,\n",
    "#     'shap__dependence_plots': False,\n",
    "#     'feature_importance': True\n",
    "# }\n",
    "\n",
    "# evaluate_dict['save'] = {\n",
    "#     'plots': False,\n",
    "#     'data': False\n",
    "# }\n",
    "\n",
    "# evaluate_dict['models_dir'] = f'/Users/{user}/Dropbox/data_science/modeling-football-outcomes/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store ensemble submodel evaluation JSON for use in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.289161Z",
     "start_time": "2019-08-30T14:33:36.216260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/ensemble_submodel_evaluate__classification_example.json'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_submodel_eval_path = 'model_configs/ensemble_submodel_evaluate__classification_example.json'\n",
    "ensemble_submodel_eval_path\n",
    "json.dump(evaluate_dict, open(ensemble_submodel_eval_path,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble_evaluate.json (ensemble_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.401739Z",
     "start_time": "2019-08-30T14:33:36.296308Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "ensemble_eval = deepcopy(evaluate_dict)\n",
    "\n",
    "ensemble_eval['save'] = {\n",
    "    'plots': False,\n",
    "    'data': True\n",
    "}    \n",
    "\n",
    "ensemble_eval['to_plot'] = {\n",
    "    'ridge': True,\n",
    "    'thresholds': True,\n",
    "    'bins': True,\n",
    "    'roc': True,\n",
    "    'accuracy_by_top_n': True,\n",
    "    'shap__feature_importance': True,\n",
    "    'shap__dependence_plots': True,\n",
    "    'feature_importance': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store ensemble evaluation JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.460163Z",
     "start_time": "2019-08-30T14:33:36.404519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/ensemble_evaluate__classification_example.json'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_evaluate_json_path = 'model_configs/ensemble_evaluate__classification_example.json'\n",
    "ensemble_evaluate_json_path\n",
    "with open(ensemble_evaluate_json_path, 'w') as w:\n",
    "    json.dump(ensemble_eval, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ensemble by generating new CV data\n",
    "### Create ensemble.json (ensemble_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.520268Z",
     "start_time": "2019-08-30T14:33:36.462596Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dict = {}\n",
    "ensemble_dict['models_dir'] = f'models'\n",
    "ensemble_dict['ensemble_model_id'] = 'classification_ensemble_with_new_cv_data'\n",
    "ensemble_dict['number_of_models'] = 5\n",
    "ensemble_dict['aggregation_method'] = ['mean', 'median'] # mean, median, max, min, mean excluding top/bottom n (robust mean?)\n",
    "ensemble_dict['source'] = ensemble_submodel_path\n",
    "ensemble_dict['save'] = {'scores': True}\n",
    "\n",
    "ensemble_dict['evaluation_config'] = ensemble_submodel_eval_path\n",
    "ensemble_dict['submodel_plots'] = True\n",
    "\n",
    "assert os.path.exists(ensemble_dict['models_dir'])\n",
    "assert not set(ensemble_dict['aggregation_method']) - set(['mean','median','min','max'])\n",
    "if 'load_cv_data' not in ensemble_dict.keys():\n",
    "    assert (type(ensemble_dict['source']) is str) | (len(ensemble_dict['source']) == ensemble_dict['number_of_models'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of modifying ensemble.json: each model in the ensemble gets a random 5 features (ensemble_dict['input_changes_by_iteration']['features_list'] contains a list of N lists of 5 random features each.\n",
    "* this can be used for the output of feature selection, e.g. take the top 10 feature sets and ensemble those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.577685Z",
     "start_time": "2019-08-30T14:33:36.524241Z"
    }
   },
   "outputs": [],
   "source": [
    "# features_list = pd.read_csv('data/{}/{}.csv'.format(\n",
    "#     *model_dict['features_tbl'].split('.'))\n",
    "# ).columns.tolist()[3:]\n",
    "\n",
    "# features_lists = [\n",
    "#     list(set(np.random.choice(features_list, size=5).tolist()))\n",
    "#     for _ in range(ensemble_dict['number_of_models'])\n",
    "# ]\n",
    "\n",
    "# ensemble_dict['input_changes_by_iteration'] = {\n",
    "#     'features_list': features_lists\n",
    "# }\n",
    "\n",
    "# # test\n",
    "# if 'input_changes_by_iteration' in ensemble_dict:\n",
    "#     assert type(ensemble_dict['input_changes_by_iteration']) is dict\n",
    "#     for param, values in ensemble_dict['input_changes_by_iteration'].items():\n",
    "#         assert len(values) == ensemble_dict['number_of_models']\n",
    "#         for value in values:\n",
    "#             assert type(value) == type(model_dict[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store ensemble JSON for generating new cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.644072Z",
     "start_time": "2019-08-30T14:33:36.580158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/ensemble_model_new_cv__classification_example.json'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model_json_path = 'model_configs/ensemble_model_new_cv__classification_example.json'\n",
    "ensemble_model_json_path\n",
    "with open(ensemble_model_json_path, 'w') as w:\n",
    "    json.dump(ensemble_dict, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.694578Z",
     "start_time": "2019-08-30T14:33:36.646581Z"
    }
   },
   "outputs": [],
   "source": [
    "if DO_EXECUTE:\n",
    "    ensemble = Ensemble(ensemble_model_json_path, ensemble_evaluate_json_path)\n",
    "    ensemble.execute_ensemble()\n",
    "    ensemble.evaluate_ensemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy ensemble CV data for a new ensemble\n",
    "#### Use cases\n",
    "* hyperparameter optimization (change only the hyperparameters in each sub-model's model.json file)\n",
    "* __feature selection if base models (and cv_data) include all features__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble.json (ensemble_dict) for a new ensemble that loads CV data from another ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.753202Z",
     "start_time": "2019-08-30T14:33:36.696260Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dict_load_cv = {}\n",
    "ensemble_dict_load_cv['models_dir'] = f'models'\n",
    "ensemble_dict_load_cv['ensemble_model_id'] = 'ensemble_load_cv'\n",
    "ensemble_dict_load_cv['load_cv_data_from'] = ensemble_dict['ensemble_model_id']\n",
    "ensemble_dict_load_cv['number_of_models'] = 5\n",
    "ensemble_dict_load_cv['save'] = {'scores': True}\n",
    "ensemble_dict_load_cv['evaluation_config'] = ensemble_evaluate_json_path\n",
    "ensemble_dict_load_cv['submodel_plots'] = False\n",
    "ensemble_dict_load_cv['aggregation_method'] = ['mean', 'median'] # mean, median, max, min, mean excluding top/bottom n (robust mean?)\n",
    "\n",
    "assert os.path.exists(ensemble_dict_load_cv['models_dir'])\n",
    "assert not set(ensemble_dict_load_cv['aggregation_method']) - set(['mean','median','min','max'])\n",
    "if 'load_cv_data_from' in ensemble_dict_load_cv.keys():\n",
    "    assert os.path.exists(\n",
    "        os.path.join(ensemble_dict_load_cv['models_dir'], \n",
    "                     ensemble_dict_load_cv['load_cv_data_from'])\n",
    "        )\n",
    "    \n",
    "    source_path = os.path.join(ensemble_dict_load_cv['models_dir'], \n",
    "                               ensemble_dict_load_cv['load_cv_data_from'])\n",
    "    n_models_expected = 0\n",
    "    for d in os.listdir(source_path):\n",
    "        try:\n",
    "            _ = int(d)\n",
    "            n_models_expected += 1\n",
    "        except:\n",
    "            pass\n",
    "    assert ensemble_dict_load_cv['number_of_models'] == n_models_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.804615Z",
     "start_time": "2019-08-30T14:33:36.755389Z"
    }
   },
   "outputs": [],
   "source": [
    "params_lists = []\n",
    "for n in range(ensemble_dict['number_of_models']):\n",
    "    model_dict['model_params']['max_depth'] = 12\n",
    "    params_lists.append(model_dict['model_params'])\n",
    "\n",
    "ensemble_dict_load_cv['input_changes_by_iteration'] = {\n",
    "    'model_params': params_lists\n",
    "}\n",
    "\n",
    "# # test\n",
    "if 'input_changes_by_iteration' in ensemble_dict:\n",
    "    assert type(ensemble_dict['input_changes_by_iteration']) is dict\n",
    "    for param, values in ensemble_dict['input_changes_by_iteration'].items():\n",
    "        assert len(values) == ensemble_dict['number_of_models']\n",
    "        for value in values:\n",
    "            assert type(value) == type(model_dict[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T02:53:31.969115Z",
     "start_time": "2019-08-05T02:52:15.758Z"
    }
   },
   "source": [
    "#### Store ensemble model JSON for existing cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.856936Z",
     "start_time": "2019-08-30T14:33:36.806560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_configs/ensemble_model_load_cv__classification_example.json'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model_json_path = 'model_configs/ensemble_model_load_cv__classification_example.json'\n",
    "ensemble_model_json_path\n",
    "with open(ensemble_model_json_path, 'w') as w:\n",
    "    json.dump(ensemble_dict_load_cv, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:33:36.906048Z",
     "start_time": "2019-08-30T14:33:36.859183Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if DO_EXECUTE:\n",
    "    ensemble = Ensemble(ensemble_model_json_path, ensemble_evaluate_json_path)\n",
    "    ensemble.execute_ensemble()\n",
    "    ensemble.evaluate_ensemble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
