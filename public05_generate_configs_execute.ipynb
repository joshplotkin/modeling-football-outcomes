{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.718700Z",
     "start_time": "2019-01-15T22:37:25.790183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run initialize.ipynb\n",
    "\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* how to generalize to regression problems?\n",
    "  * easier to create a separate process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set root dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.730707Z",
     "start_time": "2019-01-15T22:37:31.724173Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT='/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/' \\\n",
    "     'models'\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "* set model ID\n",
    "* remove this model ID's directory if it exists\n",
    "* create directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.746126Z",
     "start_time": "2019-01-15T22:37:31.736281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory models/0115_test_ou **EXISTS**\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = '0115_test_ou'\n",
    "\n",
    "if os.path.exists(MODEL_ID):\n",
    "    print 'Directory models/{} **EXISTS**'.format(MODEL_ID)    \n",
    "else:\n",
    "    print 'Directory models/{} **DOES NOT EXIST**'.format(MODEL_ID)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.773027Z",
     "start_time": "2019-01-15T22:37:31.752565Z"
    }
   },
   "outputs": [],
   "source": [
    "## wipe out existing directory\n",
    "\n",
    "if os.path.exists(MODEL_ID):\n",
    "    shutil.rmtree(MODEL_ID)\n",
    "os.mkdir(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dictionary version of model.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.803771Z",
     "start_time": "2019-01-15T22:37:31.779192Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {'model_id': MODEL_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hive Tables\n",
    "* features\n",
    "* tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.836815Z",
     "start_time": "2019-01-15T22:37:31.824703Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['features_tbl'] = 'features.0111_ou_test'\n",
    "model_dict['labels_tbl'] = 'labels.0111_ou_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns from Hive tables\n",
    "* index: unique identifier in features/labels table (must be in both)\n",
    "* label column, and indicator of what is a positive label\n",
    "  * currently not supported: multi-class\n",
    "  * code will binarize\n",
    "* list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:31.853229Z",
     "start_time": "2019-01-15T22:37:31.843120Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['index'] = ['game_id']\n",
    "model_dict['label_col'] = 'is_sbr_ou_over'\n",
    "model_dict['pos_labels'] = [1]\n",
    "model_dict['neg_labels'] = [-1]\n",
    "model_dict['features_list'] = ['season','sbr_ou']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:34.985100Z",
     "start_time": "2019-01-15T22:37:31.860892Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert these fields are of the correct type\n",
    "assert type(model_dict['index']) is list\n",
    "assert type(model_dict['label_col']) is str\n",
    "assert type(model_dict['features_tbl']) is str\n",
    "assert type(model_dict['features_tbl']) is str\n",
    "assert type(model_dict['features_list']) is list\n",
    "assert type(model_dict['pos_labels']) is list\n",
    "\n",
    "## assert format is schema.table and that\n",
    "## table exists in hive\n",
    "for tbl_str in ['features_tbl','features_tbl']:\n",
    "    schema_and_tbl = model_dict[tbl_str].split('.')\n",
    "    assert len(schema_and_tbl) == 2\n",
    "    schema, tbl = schema_and_tbl\n",
    "    assert spark.sql(\n",
    "            'show tables in {}'.format(schema)\n",
    "        ).filter(\n",
    "            col('tableName') == tbl\n",
    "        ).count() == 1\n",
    "\n",
    "feat_cols_set = set(spark.table(model_dict['features_tbl']).columns)\n",
    "label_cols_set = set(spark.table(model_dict['labels_tbl']).columns)\n",
    "idx_set = set(model_dict['index'])\n",
    "feat_set = set(model_dict['features_list'])\n",
    "label_set = set([model_dict['label_col']])\n",
    "\n",
    "## assert the chosen columns exist in the\n",
    "## chosen tables\n",
    "assert not idx_set - feat_cols_set\n",
    "assert not idx_set - label_cols_set\n",
    "assert not feat_set - feat_cols_set\n",
    "assert not label_set - label_cols_set\n",
    "\n",
    "## check that positive and negative label values \n",
    "## are valid\n",
    "for label_val in ['pos_labels','neg_labels']:\n",
    "    assert spark.table(\n",
    "            model_dict['labels_tbl']\n",
    "        ).filter(\n",
    "            col(model_dict['label_col']).isin(model_dict[label_val])\n",
    "        ).count() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Sets\n",
    "* random seeds for reproducibility\n",
    "* number of folds for cross-validation (value of <= 1 doesn't do k-fold\n",
    "* __TODO__: ability to call another's CV set\n",
    "* global_dataset_proportions\n",
    " * proportion of the data for each of training, scoring only, holdout, and throwaway\n",
    " * generated using stratified sampling\n",
    "* dimensional_dataset_proportions\n",
    " * post-processing after global_dataset_proportions\n",
    " * idea is to move specific field values, e.g. move certain seasons to the holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: use cross-validation data from another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:34.997529Z",
     "start_time": "2019-01-15T22:37:34.991248Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model_cv_to_use'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV parameters, when not using another model CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:35.017879Z",
     "start_time": "2019-01-15T22:37:35.003451Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['kfold_seed'] = 99\n",
    "model_dict['dataset_seed'] = 9\n",
    "model_dict['kfolds'] = 5\n",
    "model_dict['strata_cols'] = []\n",
    "model_dict['holdout_set'] = {\n",
    "    'store_to_disk': False,\n",
    "    'score_using_full_model': False \n",
    "}\n",
    "\n",
    "model_dict['global_dataset_proportions'] = {\n",
    "        'in_training': 0.5,\n",
    "        'holdout': 0.5,\n",
    "        'throw_away': 0,\n",
    "        'scoring_only': 0\n",
    "    }\n",
    "\n",
    "## SAMPLE USAGE:\n",
    "_ = '''\n",
    "model_dict['dimensional_dataset_proportions'] = {\n",
    "        'throw_away': [\n",
    "            {\n",
    "                'vals': [\n",
    "                    0\n",
    "                ], \n",
    "                'dim': 'is_home',\n",
    "                'prop_to_move': 1.0, \n",
    "                'from_groups': [\n",
    "                    'in_training',\n",
    "                    'holdout',\n",
    "                    'scoring_only'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "'''\n",
    "model_dict['dimensional_dataset_proportions'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:35.089669Z",
     "start_time": "2019-01-15T22:37:35.023730Z"
    }
   },
   "outputs": [],
   "source": [
    "assert (model_dict['model_cv_to_use'] is None) \\\n",
    "        | (type(model_dict['model_cv_to_use']) is str)\n",
    "\n",
    "## TODO: when loading the CV sets, ensure that set(load cvs) - set(current df) is empty\n",
    "\n",
    "if model_dict['model_cv_to_use']:\n",
    "    assert os.path.exists(model_dict['model_cv_to_use'])\n",
    "else:\n",
    "    ## assert the data structures/types are correct\n",
    "    assert type(model_dict['kfold_seed']) is int\n",
    "    assert type(model_dict['dataset_seed']) is int\n",
    "    assert type(model_dict['kfolds']) is int\n",
    "    assert type(model_dict['strata_cols']) is list\n",
    "    assert type(model_dict['global_dataset_proportions']) is dict\n",
    "    assert type(model_dict['dimensional_dataset_proportions']) is dict\n",
    "    assert type(model_dict['holdout_set']) is dict\n",
    "\n",
    "    ## assert strata cols are present in the labels table\n",
    "    assert not set(model_dict['strata_cols']) - label_cols_set\n",
    "\n",
    "    dataset_types = set(['in_training','holdout','throw_away','scoring_only'])\n",
    "    global_datasets = model_dict['global_dataset_proportions']\n",
    "    dim_datasets = model_dict['dimensional_dataset_proportions']\n",
    "\n",
    "    ## assert global_dataset_proportions has all possible dataset types\n",
    "    assert set(global_datasets.keys()) == dataset_types\n",
    "    ## values are proportions that must sum to 1\n",
    "    assert sum(global_datasets.values()) == 1\n",
    "    ## assert that the keys are valid dataset types\n",
    "    assert not set(dim_datasets.keys()) - dataset_types\n",
    "    ## assert the following (in order of assertion block):\n",
    "    ## (1) each value is a list\n",
    "    ## (2) each element of the list is a dict\n",
    "    ## (3) each dict has the 5 required keys\n",
    "    ## (4) the \"dim\" field is in the strata columns \n",
    "    ## (5) \"prop_to_move\" field is [0, 1]\n",
    "    ## (6) \"from_groups\" are in the possible dataset types\n",
    "    for k, dim_list in dim_datasets.iteritems():\n",
    "        assert (type(dim_list)) is list\n",
    "        for entry in dim_list:\n",
    "            assert type(entry) is dict\n",
    "            assert set(entry.keys()) \\\n",
    "                    == set(['vals','dim','prop_to_move','from_groups'])\n",
    "            assert entry['dim'] in model_dict['strata_cols']\n",
    "            assert 0 <= entry['prop_to_move'] <= 1\n",
    "            assert not set(entry['from_groups']) - dataset_types\n",
    "\n",
    "    ## assert holdout set has 2 keys (store_to_disk, score_using_full_model)\n",
    "    ## and the corresponding values are boolean\n",
    "    assert set(model_dict['holdout_set'].keys()) \\\n",
    "            == set(['store_to_disk','score_using_full_model'])\n",
    "    assert len(filter(\n",
    "        lambda x: type(x) is not bool, \n",
    "        model_dict['holdout_set'].values()\n",
    "    )) == 0\n",
    "    ## if holdout data isn't stored, it can't be scored\n",
    "    assert not (model_dict['holdout_set']['store_to_disk'] is False) \\\n",
    "                & (model_dict['holdout_set']['score_using_full_model'] is True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice\n",
    "* package/class name as a string\n",
    "* parameters as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:35.105080Z",
     "start_time": "2019-01-15T22:37:35.094950Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model'] = 'xgboost.XGBClassifier'\n",
    "model_dict['model_params'] = {\n",
    "    'n_jobs': 1, \n",
    "    'learning_rate': 0.1, \n",
    "    'n_estimators': 200, \n",
    "    'max_features': 'auto', \n",
    "    'booster': 'gbtree', \n",
    "    'silent': True, \n",
    "    'nthread': None, \n",
    "    'subsample': 0.9, \n",
    "    'random_state': 9, \n",
    "    'objective': 'binary:logistic', \n",
    "    'max_depth': 3, \n",
    "    'gamma': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.540036Z",
     "start_time": "2019-01-15T22:37:35.107863Z"
    }
   },
   "outputs": [],
   "source": [
    "## test that model object can be created\n",
    "## from model inputs\n",
    "try:\n",
    "    import importlib\n",
    "\n",
    "    model_class_str = model_dict['model']\n",
    "    model_obj_path = '.'.join(model_class_str.split('.')[:-1])\n",
    "    model_name = model_class_str.split('.')[-1]\n",
    "    model_package = importlib.import_module(model_obj_path)\n",
    "    model_class = getattr(model_package, model_name)\n",
    "    _ = model_class(**model_dict['model_params'])\n",
    "except Exception as e:\n",
    "    e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out model.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.552488Z",
     "start_time": "2019-01-15T22:37:36.544654Z"
    }
   },
   "outputs": [],
   "source": [
    "model_json_path = '{}/model.json'.format(model_dict['model_id'])\n",
    "assert not os.path.exists(model_json_path)\n",
    "\n",
    "with open(model_json_path,'w') as w:\n",
    "    json.dump(model_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dictionary version of plots.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.563333Z",
     "start_time": "2019-01-15T22:37:36.557242Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict = {'model_id': MODEL_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Labels\n",
    "* labels --> names (note: keys should be strings)\n",
    "* name for success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.574363Z",
     "start_time": "2019-01-15T22:37:36.568076Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['label_map'] = {\n",
    "    '1': 'Won',\n",
    "    '0': 'Lost'\n",
    "}\n",
    "plots_dict['success_name'] = 'Win Rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.584681Z",
     "start_time": "2019-01-15T22:37:36.578888Z"
    }
   },
   "outputs": [],
   "source": [
    "assert type(plots_dict['label_map']) is dict\n",
    "assert type(plots_dict['success_name']) is str\n",
    "assert set(plots_dict['label_map'].keys()) == set(['0','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins to plot\n",
    "* plot_bins: \n",
    "   * Number of bins to plot (i.e. number of bars on the bar chart)\n",
    "* bin_types:\n",
    "   * \"Bin\" puts scores into uniform bins, e.g. [0, 0.10], (0.10, 0.20], ..., (0.9, 1.0]\n",
    "   * \"Percentile\" bins scores into ntiles determined by plot_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.594570Z",
     "start_time": "2019-01-15T22:37:36.589212Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['bin_types'] = ['Bin', 'Percentile']\n",
    "plots_dict['plot_bins'] = [10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.605606Z",
     "start_time": "2019-01-15T22:37:36.599175Z"
    }
   },
   "outputs": [],
   "source": [
    "## currently only supports \"Bin\" and \"Percentile\"\n",
    "assert not set(plots_dict['bin_types']) - set(['Bin','Percentile'])\n",
    "## all plot bins values should be ints\n",
    "assert plots_dict['plot_bins'] == map(int, plots_dict['plot_bins'])\n",
    "## ensure all bins values are in [2, 1000]\n",
    "assert filter(\n",
    "        lambda x: 2 <= x <= 1000, plots_dict['plot_bins']\n",
    "    )   == plots_dict['plot_bins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:11:29.935287Z",
     "start_time": "2019-01-14T05:11:29.928839Z"
    }
   },
   "source": [
    "### Threshold Metrics to Plot\n",
    "* metrics evaluated at each of 100 score threshold points\n",
    "* currently only supports Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.615409Z",
     "start_time": "2019-01-15T22:37:36.610225Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['threshold_metrics'] = ['Accuracy','F1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.624823Z",
     "start_time": "2019-01-15T22:37:36.619773Z"
    }
   },
   "outputs": [],
   "source": [
    "assert type(plots_dict['threshold_metrics']) is list\n",
    "## currently only supports Accuracy and F1\n",
    "assert not set(plots_dict['threshold_metrics']) - set(['Accuracy','F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T17:15:57.428617Z",
     "start_time": "2019-01-14T17:15:57.423675Z"
    }
   },
   "source": [
    "### Write out plots.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.636258Z",
     "start_time": "2019-01-15T22:37:36.629410Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_json_path = '{}/plots.json'.format(model_dict['model_id'])\n",
    "assert not os.path.exists(plots_json_path)\n",
    "\n",
    "with open(plots_json_path,'w') as w:\n",
    "    json.dump(plots_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:37:36.645942Z",
     "start_time": "2019-01-15T22:37:36.641027Z"
    }
   },
   "outputs": [],
   "source": [
    "# foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:38:21.177090Z",
     "start_time": "2019-01-15T22:37:36.650620Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0115_test_ou'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check JSON files\n",
      "\n",
      "real\t0m13.812s\n",
      "user\t0m27.367s\n",
      "sys\t0m2.324s\n",
      "\n",
      "Cross-validation data\n",
      "\n",
      "real\t0m16.554s\n",
      "user\t0m42.666s\n",
      "sys\t0m2.568s\n",
      "\n",
      "Train and score\n",
      "\n",
      "real\t0m3.419s\n",
      "user\t0m2.137s\n",
      "sys\t0m1.037s\n",
      "\n",
      "Evaluate and plot\n",
      "\n",
      "real\t0m7.778s\n",
      "user\t0m6.705s\n",
      "sys\t0m1.280s\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID\n",
    "!source ~/.bashrc && \\\n",
    "    unset PYSPARK_PYTHON && \\\n",
    "    unset PYSPARK_DRIVER_PYTHON && \\\n",
    "    unset PYSPARK_DRIVER_PYTHON_OPTS && \\\n",
    "    cd /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/ && \\\n",
    "    src/model_pipeline.sh {MODEL_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T22:38:21.318781Z",
     "start_time": "2019-01-15T22:38:21.182961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON configuration files passed checks.\r\n",
      "cv sets wrote successfully.\r\n",
      "True\r\n",
      "successfully completed evaluation and plotting.\r\n"
     ]
    }
   ],
   "source": [
    "!cat /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/{MODEL_ID}/logs/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
