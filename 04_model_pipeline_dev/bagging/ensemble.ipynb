{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:46.885067Z",
     "start_time": "2019-08-04T02:40:46.863790Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:49.990656Z",
     "start_time": "2019-08-04T02:40:47.268575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../../config/initialize_nospark.ipynb\n",
    "\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:50.029621Z",
     "start_time": "2019-08-04T02:40:49.992871Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../config')\n",
    "from mpl_style import *\n",
    "colors = rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "rcParams['figure.dpi'] = 96\n",
    "rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:50.079943Z",
     "start_time": "2019-08-04T02:40:50.032629Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "params_backup = rcParams.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:50.141554Z",
     "start_time": "2019-08-04T02:40:50.086882Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:40:54.185352Z",
     "start_time": "2019-08-04T02:40:50.151970Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('model_pipeline')\n",
    "from Ensemble import Ensemble\n",
    "from EvaluateModel import EvaluationData, EvaluateAndPlot\n",
    "from ExecuteModelPipeline import ExecuteModelPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "* check JSON\n",
    "  * assert all labels tables are the same\n",
    "* some way of loading existing CV... maybe a dict of cv directories with files to skip CV part (make false) and copy files in... instead of changing seed, this would leave it constant\n",
    "  * assert do cv is True OR there are N CVs to load\n",
    "* clean up the ensemble execution part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T02:53:39.024488Z",
     "start_time": "2019-07-17T02:53:38.973296Z"
    }
   },
   "source": [
    "#### multi-thread batch?\n",
    "#### try DL on it? optimization, bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:35:17.621157Z",
     "start_time": "2019-08-04T03:35:17.572964Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "base_model = json.load(open('models/0625/model.json'))\n",
    "base_model['global_dataset_proportions']['training'] = 0.5\n",
    "base_model['global_dataset_proportions']['throw_away'] = 0.5\n",
    "\n",
    "base_model['save'] = {\n",
    "    'cv_data': False,\n",
    "    'serialized_models': False,\n",
    "    'cv_scores': True,\n",
    "    'holdout_scores': False\n",
    "}\n",
    "base_model['actions'] = {\n",
    "    'do_train_and_score_cv': True,\n",
    "    'do_score_holdout': False,\n",
    "    'do_evaluate': True\n",
    "}\n",
    "\n",
    "json.dump(base_model, open('model_configs/base_model.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:35:17.808520Z",
     "start_time": "2019-08-04T03:35:17.735053Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model['label_col'] = 'did_win'\n",
    "base_model['model'] = 'xgboost.XGBClassifier'\n",
    "base_model['model_params'] = {'n_jobs': 1,\n",
    "  'learning_rate': 0.1,\n",
    "  'n_estimators': 100,\n",
    "  'max_features': 'auto',\n",
    "  'booster': 'gbtree',\n",
    "  'silent': True,\n",
    "  'nthread': None,\n",
    "  'subsample': 0.5,\n",
    "  'random_state': 9,\n",
    "  'objective': 'binary:logistic',\n",
    "  'max_depth': 6,\n",
    "  'gamma': 0}\n",
    "base_model['actions']['do_evaluate'] = False\n",
    "base_model['save']['cv_data'] = True\n",
    "\n",
    "json.dump(base_model, open('model_configs/base_model_classification.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:35:21.025458Z",
     "start_time": "2019-08-04T03:35:20.975663Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "base_eval = json.load(open('models/0625/evaluate.json'))\n",
    "base_eval['to_plot'] = {\n",
    "    'ridge': True,\n",
    "    'thresholds': True,\n",
    "    'bins': True,\n",
    "    'roc': True,\n",
    "    'accuracy_by_top_n': True,\n",
    "    'regression__distributions': True,\n",
    "    'regression__scatter': True,\n",
    "    'regression__residuals_by_season_week': True,\n",
    "    'regression__confusion_matrix': True,\n",
    "    'shap__feature_importance': True,\n",
    "    'shap__dependence_plots': False,\n",
    "    'feature_importance': True\n",
    "}\n",
    "base_eval['save']['plots'] = False\n",
    "base_eval['save']['data'] = False\n",
    "json.dump(base_eval, open('model_configs/base_eval.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble - load CV data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:57:33.009056Z",
     "start_time": "2019-08-04T03:57:32.954844Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dict = {}\n",
    "ensemble_dict['models_dir'] = '/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models'\n",
    "ensemble_dict['ensemble_model_id'] = 'ensemble_0803'\n",
    "ensemble_dict['load_cv_data_from'] = 'ensemble_0630'\n",
    "ensemble_dict['number_of_models'] = 5\n",
    "ensemble_dict['save'] = {'scores': True, 'plots': True}\n",
    "ensemble_dict['evaluation_config'] = 'model_configs/base_eval.json'\n",
    "ensemble_dict['submodel_plots'] = True\n",
    "ensemble_dict['aggregation_method'] = ['mean', 'median'] # mean, median, max, min, mean excluding top/bottom n (robust mean?)\n",
    "\n",
    "assert os.path.exists(ensemble_dict['models_dir'])\n",
    "assert not set(ensemble_dict['aggregation_method']) - set(['mean','median','min','max'])\n",
    "if 'load_cv_data_from' in ensemble_dict.keys():\n",
    "    assert os.path.exists(\n",
    "        os.path.join(ensemble_dict['models_dir'], \n",
    "                     ensemble_dict['load_cv_data_from'])\n",
    "        )\n",
    "    \n",
    "    source_path = os.path.join(ensemble.config['models_dir'], \n",
    "                               ensemble.config['load_cv_data_from'])\n",
    "    n_models_expected = 0\n",
    "    for d in os.listdir(source_path):\n",
    "        try:\n",
    "            _ = int(d)\n",
    "            n_models_expected += 1\n",
    "        except:\n",
    "            pass\n",
    "    assert ensemble_dict['number_of_models'] == n_models_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Ensemble for method loading CV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:25.797885Z",
     "start_time": "2019-08-04T03:48:25.748932Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = Ensemble(ensemble_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:26.043363Z",
     "start_time": "2019-08-04T03:48:25.986559Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.trial_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:26.248121Z",
     "start_time": "2019-08-04T03:48:26.197047Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_path(model_name):\n",
    "    return os.path.join(ensemble.config['models_dir'], ensemble.config[model_name])\n",
    "\n",
    "def get_json_file_paths(source_path, d):\n",
    "    files = os.listdir(os.path.join(source_path, d))\n",
    "    return [f for f in files if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:26.471573Z",
     "start_time": "2019-08-04T03:48:26.412365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803...\n"
     ]
    }
   ],
   "source": [
    "ensemble.setup_trial_dir(ensemble.get_trial_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:26.712213Z",
     "start_time": "2019-08-04T03:48:26.662313Z"
    }
   },
   "outputs": [],
   "source": [
    "source_path = get_path('load_cv_data_from')\n",
    "dest_path = get_path('ensemble_model_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:57:07.284169Z",
     "start_time": "2019-08-04T03:57:07.229518Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in os.listdir(source_path):\n",
    "    try:\n",
    "        _ = int(d)\n",
    "        _ = shutil.copytree(f'{source_path}/{d}/cv_data', \n",
    "                        f'{dest_path}/{d}/cv_data')\n",
    "        for json_file in get_json_file_paths(source_path, d):\n",
    "                _ = shutil.copyfile(f'{source_path}/{d}/{json_file}', \n",
    "                        f'{dest_path}/{d}/{json_file}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaving off:\n",
    "* need to have sub-models read cv_data\n",
    "  * flaw in current process: don't need to copy data, just point the individual JSON configs to read CV data from source model\n",
    "  * need to have that file tell it to save sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:38.307643Z",
     "start_time": "2019-08-04T03:48:27.595471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00000\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00001\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00002\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00003\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00004\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n"
     ]
    }
   ],
   "source": [
    "ensemble.train_and_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble - generate new CV data\n",
    "* TODO: how to make it configurable how to handle plotting submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:36:15.886883Z",
     "start_time": "2019-08-04T03:36:15.828821Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dict = {}\n",
    "ensemble_dict['models_dir'] = '/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models'\n",
    "ensemble_dict['ensemble_model_id'] = 'ensemble_0630'\n",
    "ensemble_dict['number_of_models'] = 5\n",
    "ensemble_dict['aggregation_method'] = ['mean', 'median'] # mean, median, max, min, mean excluding top/bottom n (robust mean?)\n",
    "ensemble_dict['source'] = 'model_configs/base_model_classification.json'\n",
    "ensemble_dict['save'] = {'scores': True, 'plots': True}\n",
    "\n",
    "ensemble_dict['evaluation_config'] = 'model_configs/base_eval.json'\n",
    "ensemble_dict['submodel_evaluation_config'] = 'model_configs/base_eval.json'\n",
    "ensemble_dict['submodel_plots'] = True\n",
    "\n",
    "assert os.path.exists(ensemble_dict['models_dir'])\n",
    "assert not set(ensemble_dict['aggregation_method']) - set(['mean','median','min','max'])\n",
    "if 'load_cv_data' not in ensemble_dict.keys():\n",
    "    assert (type(ensemble_dict['source']) is str) | (len(ensemble_dict['source']) == ensemble_dict['number_of_models'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: each model in the ensemble gets a random 5 features\n",
    "* in the JSON, provide a list of n sets of features for the features_list key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:36:16.343538Z",
     "start_time": "2019-08-04T03:36:16.185226Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list = pd.read_csv('data/{}/{}.csv'.format(\n",
    "    *base_model['features_tbl'].split('.'))\n",
    ").columns.tolist()[3:]\n",
    "\n",
    "features_lists = [\n",
    "    list(set(np.random.choice(features_list, size=5).tolist()))\n",
    "    for _ in range(ensemble_dict['number_of_models'])\n",
    "]\n",
    "\n",
    "ensemble_dict['input_changes_by_iteration'] = {\n",
    "    'features_list': features_lists\n",
    "}\n",
    "\n",
    "# test\n",
    "if 'input_changes_by_iteration' in ensemble_dict:\n",
    "    assert type(ensemble_dict['input_changes_by_iteration']) is dict\n",
    "    for param, values in ensemble_dict['input_changes_by_iteration'].items():\n",
    "        assert len(values) == ensemble_dict['number_of_models']\n",
    "        for value in values:\n",
    "            assert type(value) == type(base_model[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Ensemble for method generating new CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:36:32.801351Z",
     "start_time": "2019-08-04T03:36:19.857378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00000\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00001\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00002\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00003\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n",
      "Model Path:\n",
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630/00004\n",
      "SAVE CV DATA\n",
      "per model.json, skipping model evaluation...\n"
     ]
    }
   ],
   "source": [
    "ensemble = Ensemble(ensemble_dict)\n",
    "ensemble.setup_trial_dir(ensemble.get_trial_path())\n",
    "ensemble.create_ensemble_dir_structure()\n",
    "ensemble.train_and_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Ensemble\n",
    "* #### TODO: move this into Ensemble class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:48:51.841380Z",
     "start_time": "2019-08-04T03:48:51.786620Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_eval = base_eval.copy()\n",
    "\n",
    "ensemble_eval['save']['plots'] = True\n",
    "ensemble_eval['save']['data'] = True\n",
    "ensemble_eval['model_id'] = ensemble_dict['ensemble_model_id']\n",
    "ensemble_eval['to_plot'] = {\n",
    "    'ridge': True,\n",
    "    'thresholds': True,\n",
    "    'bins': True,\n",
    "    'roc': True,\n",
    "    'accuracy_by_top_n': True,\n",
    "    'regression__distributions': False,\n",
    "    'regression__scatter': False,\n",
    "    'regression__residuals_by_season_week': False,\n",
    "    'regression__confusion_matrix': False,\n",
    "    'shap__feature_importance': True,\n",
    "    'shap__dependence_plots': False,\n",
    "    'feature_importance': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T03:56:14.676560Z",
     "start_time": "2019-08-04T03:56:14.550998Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803/00000/scores/cv_scores.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803/00000/scores/cv_scores.csv' does not exist: b'/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803/00000/scores/cv_scores.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-cca25e81ee28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magg_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mensemble_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aggregation_method'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ensemble_scores_{agg_method}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/data_science/modeling-football-outcomes/model_pipeline/Ensemble.py\u001b[0m in \u001b[0;36mcombine_scores\u001b[0;34m(self, aggregation_method)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_nbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_prepare_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_nbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/data_science/modeling-football-outcomes/model_pipeline/Ensemble.py\u001b[0m in \u001b[0;36mload_and_prepare_scores\u001b[0;34m(self, model_path, model_dict, model_nbr)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scores/cv_scores.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scores/cv_scores.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_only_index_label_or_scores_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_tbl_columns_renamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_nbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803/00000/scores/cv_scores.csv' does not exist: b'/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0803/00000/scores/cv_scores.csv'"
     ]
    }
   ],
   "source": [
    "is_classification = True\n",
    "\n",
    "for agg_method in ensemble_dict['aggregation_method']:\n",
    "    scores = ensemble.combine_scores(agg_method)\n",
    "    scores.to_csv(f'ensemble_scores_{agg_method}.csv')\n",
    "    \n",
    "    ensemble_eval['model_id'] = '{}/evaluation_{}'.format(\n",
    "        ensemble_dict['ensemble_model_id'],\n",
    "        agg_method\n",
    "    )\n",
    "    \n",
    "    eval_path = os.path.join(ensemble_eval['models_dir'], \n",
    "                             ensemble_eval['model_id'])\n",
    "    if not os.path.exists(eval_path):\n",
    "        os.mkdir(eval_path)\n",
    "    \n",
    "    # somehow change directories\n",
    "    plot = EvaluateAndPlot(\n",
    "        ensemble_eval, scores, is_classification\n",
    "    )\n",
    "\n",
    "    plot.plot_all(ensemble_eval.get('to_plot', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T02:43:44.786842Z",
     "start_time": "2019-08-04T02:43:44.494340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m00000\u001b[m\u001b[m             \u001b[1m\u001b[34m00002\u001b[m\u001b[m             \u001b[1m\u001b[34m00004\u001b[m\u001b[m             \u001b[1m\u001b[34mevaluation_median\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34m00001\u001b[m\u001b[m             \u001b[1m\u001b[34m00003\u001b[m\u001b[m             \u001b[1m\u001b[34mevaluation_mean\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/ensemble_0630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.798994Z",
     "start_time": "2019-07-17T05:10:23.681772Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'foo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f1d2d2f924e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'foo' is not defined"
     ]
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.803183Z",
     "start_time": "2019-07-17T05:09:21.560Z"
    }
   },
   "outputs": [],
   "source": [
    "base_eval = json.load(open('models/0625/evaluate.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T02:10:34.702845Z",
     "start_time": "2019-07-16T02:10:34.657289Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T02:10:34.747426Z",
     "start_time": "2019-07-16T02:10:34.705843Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load model.json\n",
    "* create parent directory\n",
    "* move json to parent directory\n",
    "* create child directories with individual model.json (optional evaluate.json)\n",
    "* run pipeline in each child directory\n",
    "* aggregate\n",
    "* evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: support a list of model.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.805453Z",
     "start_time": "2019-07-17T05:09:21.568Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_trial_dir(overwrite, trial_path):\n",
    "    overwrite = True if overwrite.upper()[0] == 'Y' else False\n",
    "\n",
    "    print('Model Path:\\n{}'.format(trial_path))\n",
    "    if (overwrite is False) & (os.path.exists(trial_path)):\n",
    "        print('model path already exists and user input disallows overwriting. exiting...')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if (overwrite) & (os.path.exists(trial_path)):\n",
    "        import shutil\n",
    "        shutil.rmtree(trial_path)\n",
    "    os.mkdir(trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.807484Z",
     "start_time": "2019-07-17T05:09:21.571Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_dict = ensemble_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.809308Z",
     "start_time": "2019-07-17T05:09:21.574Z"
    }
   },
   "outputs": [],
   "source": [
    "overwrite = 'Y'\n",
    "trial_path = os.path.join(bag_dict['models_dir'], bag_dict['ensemble_model_id'])\n",
    "\n",
    "setup_trial_dir(overwrite, trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.811735Z",
     "start_time": "2019-07-17T05:09:21.577Z"
    }
   },
   "outputs": [],
   "source": [
    "json.dump(bag_dict, open(os.path.join(trial_path, 'bag.json'), 'w'), indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.814233Z",
     "start_time": "2019-07-17T05:09:21.580Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def create_ensemble_dir_structure(bag_dict, trial_path):\n",
    "    model_dict = json.load(open(bag_dict['source']))\n",
    "    seed = np.random.randint(1, 1000000)\n",
    "    for model_nbr in np.arange(bag_dict['number_of_models']):\n",
    "        model_dict['dataset_seed'] = int(seed + model_nbr)\n",
    "        model_dict['fold_seed'] = int(seed + model_nbr)\n",
    "\n",
    "        model_id = '{:05d}'.format(model_nbr)\n",
    "        model_path = os.path.join(trial_path, model_id)\n",
    "        setup_trial_dir('Y', model_path)\n",
    "\n",
    "        model_dict['model_id'] = '{}/{}'.format(bag_dict['ensemble_model_id'], model_id)\n",
    "        model_dict['models_dir'] = bag_dict['models_dir']\n",
    "        json.dump(\n",
    "            model_dict, \n",
    "            open(os.path.join(model_path, 'model.json'), 'w'),\n",
    "            indent=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.816791Z",
     "start_time": "2019-07-17T05:09:21.585Z"
    }
   },
   "outputs": [],
   "source": [
    "if type(bag_dict['source']) is str:\n",
    "    create_ensemble_dir_structure(bag_dict, trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.820020Z",
     "start_time": "2019-07-17T05:09:21.588Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_nbr in np.arange(bag_dict['number_of_models']):\n",
    "    model_id = '{:05d}'.format(model_nbr)\n",
    "    model_path = os.path.join(trial_path, model_id)\n",
    "    model_json_path = os.path.join(model_path, 'model.json')\n",
    "    ExecuteModelPipeline(model_json_path, None, 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.822809Z",
     "start_time": "2019-07-17T05:09:21.592Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores = pd.read_csv(\n",
    "    'data/{}/{}.csv'.format(\n",
    "        *base_model['labels_tbl'].split('.')\n",
    "    )\n",
    ")\n",
    "\n",
    "for model_nbr in np.arange(bag_dict['number_of_models']):\n",
    "    model_id = '{:05d}'.format(model_nbr)\n",
    "    model_path = os.path.join(trial_path, model_id)\n",
    "    model_dict = json.load(open(os.path.join(model_path, 'model.json')))\n",
    "    scores_path = os.path.join(model_path, 'scores/cv_scores.csv')\n",
    "    \n",
    "    scores = pd.read_csv(scores_path)\n",
    "    cols = [c for c in scores.columns if c in model_dict['index'] or c.endswith('_label') or c.endswith('_score')]\n",
    "    scores = scores[cols]\n",
    "    \n",
    "    score_rename = {c: '{}_{}'.format(model_nbr, c) for c in cols if c.endswith('_score')}\n",
    "    scores = scores.rename(columns=score_rename)\n",
    "    \n",
    "    labels_rename = {c: '{}_{}'.format(model_nbr, c) for c in cols if c.endswith('_label')}\n",
    "    scores = scores.rename(columns=labels_rename)\n",
    "    \n",
    "    all_scores = all_scores.merge(scores, on=model_dict['index'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.826040Z",
     "start_time": "2019-07-17T05:09:21.595Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_cols = [c for c in all_scores.columns if c.endswith('_label')]\n",
    "scores_cols = [c for c in all_scores.columns if c.endswith('_score')]\n",
    "\n",
    "label_col_base = '_'.join(labels_cols[0].split('_')[1:])\n",
    "score_col_base = '_'.join(scores_cols[0].split('_')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.829094Z",
     "start_time": "2019-07-17T05:09:21.598Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "all_scores[label_col_base] = all_scores[labels_cols]\\\n",
    "                                    .apply(np.nanmean, axis=1)\\\n",
    "                                    .astype(float)\n",
    "all_scores['label'] = (all_scores[label_col_base] > 0).astype(int)\n",
    "all_scores_nonnull = all_scores[~all_scores[label_col_base].isnull()]\n",
    "\n",
    "agg_method = eval('np.nan{}'.format(bag_dict['aggregation_method']))\n",
    "all_scores_nonnull[score_col_base] = all_scores_nonnull[scores_cols].apply(agg_method, axis=1)\n",
    "\n",
    "all_scores_prepped = all_scores_nonnull.drop(labels_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T05:10:23.831951Z",
     "start_time": "2019-07-17T05:09:21.601Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot = EvaluateAndPlot(\n",
    "    base_eval, all_scores_prepped, False\n",
    ")\n",
    "\n",
    "plot.plot_ridge()\n",
    "mpl.rcParams.update(params_backup)\n",
    "\n",
    "plot.plot_thresholds()\n",
    "plot.plot_bins()\n",
    "plot.plot_roc()\n",
    "plot.plot_accuracy_by_topn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
