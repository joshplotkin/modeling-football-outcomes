{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:20:29.877668Z",
     "start_time": "2019-05-21T03:20:24.338895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../config/initialize.ipynb\n",
    "\n",
    "import cPickle as pickle\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:20:35.952175Z",
     "start_time": "2019-05-21T03:20:35.946527Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.dpi'] = 96\n",
    "rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:20:36.193083Z",
     "start_time": "2019-05-21T03:20:36.188394Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../modeling-football-outcomes/model_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quick and dirty features/labels sets for testing purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:43:15.044762Z",
     "start_time": "2019-05-21T03:41:14.904248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 00_plan_and_ideas.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 01_time_date.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 02_teams.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 03_matchup.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 04_travel.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 05_homeaway.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 06_weather.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 07_line.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 08_rankings.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run combine_features.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:25:43.010827Z",
     "start_time": "2019-05-21T03:25:43.001340Z"
    }
   },
   "source": [
    "### Modeling ideas\n",
    "* different labels (regression?)\n",
    "* hyperparam tuning\n",
    "* model selection\n",
    "* feature/team combinations, e.g. H-A DVOA\n",
    "* narrow down to certain weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:44:17.483111Z",
     "start_time": "2019-05-21T03:44:15.656554Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPREAD\n",
    "ranks = spark.table('dvoa').select(\n",
    "        'team_id','season','week_id','dave_or_wtddvoa',\n",
    "        'offensedvoa','defensedvoa','s_t_dvoa'\n",
    "    ).cache()\n",
    "game_feats = spark.table('labels.team_game_line_labels').select(\n",
    "        'game_id','season','week_id','team_id','is_home'\n",
    "    ).cache()\n",
    "\n",
    "features = game_feats.join(\n",
    "        ranks, on=['team_id','season','week_id']\n",
    "    ).fillna(-99999)\n",
    "labels = spark.table('labels.team_game_line_labels').select(\n",
    "    ## index\n",
    "    'game_id','team_id',\n",
    "    ## strata\n",
    "    'is_home','is_fav_sbr',\n",
    "    ## labels\n",
    "    'did_win','final_margin','did_cover_pfr',\n",
    "    'did_cover_sbr','did_cover_sbr_open'\n",
    ")\n",
    "\n",
    "assert features.count() == labels.count()\n",
    "\n",
    "features.write.mode('overwrite').saveAsTable('features.190320_test')\n",
    "labels.write.mode('overwrite').saveAsTable('labels.190320_test')\n",
    "\n",
    "## OVER/UNDER\n",
    "features = spark.table('labels.over_under_labels').select(\n",
    "        'game_id','season','week_id', 'sbr_ou'\n",
    "    ).fillna(-99999).cache()\n",
    "\n",
    "labels = spark.table('labels.over_under_labels').select(\n",
    "    ## index\n",
    "    'game_id',\n",
    "    ## strata\n",
    "    ## label\n",
    "    'is_sbr_ou_over'\n",
    ")\n",
    "\n",
    "assert features.count() == labels.count()\n",
    "\n",
    "features.write.mode('overwrite').saveAsTable('features.190320_ou_test')\n",
    "labels.write.mode('overwrite').saveAsTable('labels.190320_ou_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Won/Lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'model_id': '0111_did_win_initial',\n",
    "    'features_tbl': 'features.190320_test',\n",
    "    'labels_tbl': 'labels.190320_test',\n",
    "    'features_list': [\n",
    "        'season',\n",
    "        'week_id',\n",
    "        'is_home',\n",
    "        'dave_or_wtddvoa',\n",
    "        'offensedvoa',\n",
    "        'defensedvoa',\n",
    "        's_t_dvoa'\n",
    "    ],\n",
    "    'label_col': 'did_win',\n",
    "    'pos_labels': [1],\n",
    "    'neg_labels': [-1],\n",
    "    'index': ['game_id','team_id'],\n",
    "    'kfolds': 5,\n",
    "    'kfold_seed': 99,\n",
    "    'dataset_seed': 9,\n",
    "    'strata_cols': ['did_win','is_home'],\n",
    "    'global_dataset_proportions': {\n",
    "        'holdout': 0.5,\n",
    "        'throw_away': 0,\n",
    "        'in_training': 0.5,\n",
    "        'scoring_only': 0\n",
    "    },\n",
    "    'dimensional_dataset_proportions': {\n",
    "    'throw_away': [{\n",
    "        'dim': 'is_home',\n",
    "        'from_groups': ['in_training','holdout','scoring_only'],\n",
    "        'vals': [0],\n",
    "        'prop_to_move': 1.\n",
    "    }]\n",
    "    },\n",
    "    'model': 'xgboost.XGBClassifier',\n",
    "    'model_params': {\n",
    "        'booster':'gbtree',\n",
    "        'gamma': 0,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 3,\n",
    "        'max_features': 'auto',\n",
    "        'n_estimators': 100,\n",
    "        'n_jobs': 1,\n",
    "        'nthread': None,\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 9,\n",
    "        'silent': True,\n",
    "        'subsample': 0.9\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over/Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:28:31.271749Z",
     "start_time": "2019-05-21T03:28:31.262933Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_dict = {\n",
    "#     'model_id': '0111_ou_initial',\n",
    "#     'features_tbl': 'features.0111_ou_test',\n",
    "#     'labels_tbl': 'labels.0111_ou_test',\n",
    "#     'features_list': ['season','week_id', 'sbr_ou'],\n",
    "#     'label_col': 'is_sbr_ou_over',\n",
    "#     'pos_labels': [1],\n",
    "#     'neg_labels': [-1],\n",
    "#     'index': ['game_id'],\n",
    "#     'kfolds': 5,\n",
    "#     'kfold_seed': 99,\n",
    "#     'dataset_seed': 9,\n",
    "#     'strata_cols': [],\n",
    "#     'global_dataset_proportions': {\n",
    "#         'holdout': 0.5,\n",
    "#         'throw_away': 0,\n",
    "#         'in_training': 0.5,\n",
    "#         'scoring_only': 0\n",
    "#     },\n",
    "#     'dimensional_dataset_proportions': {},\n",
    "#     'model': 'xgboost.XGBClassifier',\n",
    "#     'model_params': {\n",
    "#         'booster':'gbtree',\n",
    "#         'gamma': 0,\n",
    "#         'learning_rate': 0.1,\n",
    "#         'max_depth': 3,\n",
    "#         'max_features': 'auto',\n",
    "#         'n_estimators': 200,\n",
    "#         'n_jobs': 1,\n",
    "#         'nthread': None,\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'random_state': 9,\n",
    "#         'silent': True,\n",
    "#         'subsample': 0.9\n",
    "#     }    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(model_dict['global_dataset_proportions'].values()) == 1\n",
    "assert type(model_dict['global_dataset_proportions']) is dict\n",
    "for d in model_dict['dimensional_dataset_proportions'].values():\n",
    "    assert type(d) is list\n",
    "    assert sum([type(x) is not dict for x in d]) == 0\n",
    "    \n",
    "for k in ['features_list', 'pos_labels', \n",
    "          'neg_labels', 'index', 'strata_cols']:\n",
    "    assert type(model_dict[k]) is list\n",
    "    \n",
    "for k in ['global_dataset_proportions',\n",
    "          'dimensional_dataset_proportions',\n",
    "          'model_params']:\n",
    "    assert type(model_dict[k]) is dict\n",
    "\n",
    "assert set(model_dict['global_dataset_proportions'].keys()) \\\n",
    "        == set(['holdout','throw_away','in_training','scoring_only'])\n",
    "\n",
    "assert sum(model_dict['global_dataset_proportions'].values()) == 1\n",
    "\n",
    "label_cols = set(spark.table(model_dict['labels_tbl']).columns)\n",
    "assert not set(model_dict['index']) - label_cols \n",
    "assert not set(model_dict['strata_cols']) - label_cols\n",
    "assert not set([model_dict['label_col']]) - label_cols\n",
    "      \n",
    "feats = set(spark.table(model_dict['features_tbl']).columns)\n",
    "assert not set(model_dict['features_list']) - feats\n",
    "assert not set(model_dict['index']) - feats\n",
    "\n",
    "\n",
    "## other assertions\n",
    "# tables exists\n",
    "# columns are in tables\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = get_cv_data(model_dict)\n",
    "global_rolling = prop_dict_rolling(\n",
    "    model_dict['global_dataset_proportions']\n",
    ")\n",
    "datasets = assign_group(\n",
    "    model_dict, cv_data, global_rolling, \n",
    "    model_dict['strata_cols'], 'dataset'\n",
    ")\n",
    "datasets = modify_group_for_dim(\n",
    "    model_dict, datasets, model_dict['dimensional_dataset_proportions'], 'dataset' \n",
    ")\n",
    "\n",
    "## assert (1) training set is not empty \n",
    "## (2) either k-fold or scoring set is not empty\n",
    "assert datasets.filter(col('dataset') == 'in_training').count() > 0\n",
    "if model_dict['kfolds'] <= 1:\n",
    "    assert datasets.filter(col('dataset') == 'scoring_only').count() > 0\n",
    "\n",
    "scoring_rows = datasets.filter(col('dataset') == 'scoring_only')\n",
    "training_rows = datasets.filter(col('dataset') == 'in_training')\n",
    "\n",
    "training_rows = assign_k_folds(model_dict, training_rows)\n",
    "training, scoring_only = get_training_scoring_sets(model_dict, training_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as opposed to spark:\n",
    "model_obj, train_in_memory = get_model_obj(model_dict)\n",
    "## if train_in_memory is False --> spark\n",
    "if train_in_memory is True:\n",
    "    training_scoring_dict = cv_train(model_dict, training, \n",
    "                                                scoring_only, model_obj)\n",
    "    \n",
    "    scores_df = cv_score(model_dict, training_scoring_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plots.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dict = {\n",
    "   'plot_bins': [10, 100],\n",
    "   'label_map': {\n",
    "        '0': 'Lost',\n",
    "        '1': 'Won'\n",
    "    },\n",
    "    'bin_types': ['Bin','Percentile'],\n",
    "    'eval_dict': {\n",
    "    'success_name': 'Win Rate',\n",
    "    'success_col': 'Won',\n",
    "    'failure_col': 'Lost'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot metrics by score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_threshold(scores_df, 'Accuracy')\n",
    "plot_by_threshold(scores_df, 'F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_plot(plots_dict, scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = {}\n",
    "for bin_type in plots_dict['bin_types']:\n",
    "    binned_data[bin_type] = {}\n",
    "    for nbins in plots_dict['plot_bins']:\n",
    "        ## plot bins\n",
    "        curr_bins = compute_bins(plots_dict, scores_df, nbins, bin_type)\n",
    "        compute_plot_bins(plots_dict, curr_bins, nbins, bin_type, colors)\n",
    "        ## plot trend\n",
    "        if bin_type == 'Percentile':\n",
    "            plot_trend(plots_dict, curr_bins, bin_type, nbins)\n",
    "        ## store data\n",
    "        binned_data[bin_type][nbins] = curr_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ROC\n",
    "roc_sets = {\n",
    "    set_nbr: set_data['score'][['label','score']]\n",
    "    for set_nbr, set_data in training_scoring_dict.iteritems()\n",
    "    if (set_data['score'].shape[0] > 0) \n",
    "       & (set_nbr in training_scoring_dict.keys())\n",
    "}\n",
    "\n",
    "roc_plot_kfold_errband(roc_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T03:31:01.044513Z",
     "start_time": "2019-05-21T03:31:01.039801Z"
    }
   },
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../modeling-football-outcomes/models/0115_test_ou/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = get_feat_importance_df()\n",
    "importance.to_csv('stats/reported/importance_agg.csv')\n",
    "plot_feature_importance(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
