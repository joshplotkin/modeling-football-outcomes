{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:42:42.793083Z",
     "start_time": "2019-01-05T04:42:42.724408Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:42:42.981891Z",
     "start_time": "2019-01-05T04:42:42.884074Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pandas import *\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import wikipedia\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing data\n",
    "* identify data sources\n",
    "* scrape data\n",
    "    * [x] Injuries: URL\n",
    "    * [x] Weekly rank: URL\n",
    "    * [x] Historical lines/scores: URL\n",
    "    * [x] Play-by-play?: (needs to be parsed) but backup link: URL\n",
    "    * [x] Historical weather: used .. but backup link: URL\n",
    "    * [x] Snap counts (could be helpful with injuries): URL\n",
    "    * [x] Head coach info (# wins, winning %, age)\n",
    "    * [x] Weekly stats -- player/team: URL\n",
    "    * [x] division/conference\n",
    "    * [x] travel data between teams, or timezone of location (timezone change and direction of change)\n",
    "    * [x] timezones\n",
    "    * NFL API: URL\n",
    "    * __sports info solutions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: convert to .py files, that (1) do original data load, and (2) find only net new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T20:27:02.413496Z",
     "start_time": "2019-01-04T20:27:02.408742Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../../football_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team ID Mapping for site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:55:47.888559Z",
     "start_time": "2019-01-02T18:55:44.351384Z"
    }
   },
   "outputs": [],
   "source": [
    "## code after having BeautifulSoup object\n",
    "pfr_teams = {}\n",
    "for tbl in soup.findAll('table', {'id': 'teams_active'}):\n",
    "    tbl = tbl.findAll('tbody')[0]\n",
    "    for row in tbl.findAll('tr'):\n",
    "        for team in row.findAll('th'):\n",
    "            for a in team.findAll('a'):\n",
    "                pfr_teams[a.attrs['href'].split('/')[2]] = a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:46:45.025629Z",
     "start_time": "2018-11-08T21:46:45.010446Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atl': u'Atlanta Falcons',\n",
       " 'buf': u'Buffalo Bills',\n",
       " 'car': u'Carolina Panthers',\n",
       " 'chi': u'Chicago Bears',\n",
       " 'cin': u'Cincinnati Bengals',\n",
       " 'cle': u'Cleveland Browns',\n",
       " 'clt': u'Indianapolis Colts',\n",
       " 'crd': u'Arizona Cardinals',\n",
       " 'dal': u'Dallas Cowboys',\n",
       " 'den': u'Denver Broncos',\n",
       " 'det': u'Detroit Lions',\n",
       " 'gnb': u'Green Bay Packers',\n",
       " 'htx': u'Houston Texans',\n",
       " 'jax': u'Jacksonville Jaguars',\n",
       " 'kan': u'Kansas City Chiefs',\n",
       " 'mia': u'Miami Dolphins',\n",
       " 'min': u'Minnesota Vikings',\n",
       " 'nor': u'New Orleans Saints',\n",
       " 'nwe': u'New England Patriots',\n",
       " 'nyg': u'New York Giants',\n",
       " 'nyj': u'New York Jets',\n",
       " 'oti': u'Tennessee Titans',\n",
       " 'phi': u'Philadelphia Eagles',\n",
       " 'pit': u'Pittsburgh Steelers',\n",
       " 'rai': u'Oakland Raiders',\n",
       " 'ram': u'Los Angeles Rams',\n",
       " 'rav': u'Baltimore Ravens',\n",
       " 'sdg': u'Los Angeles Chargers',\n",
       " 'sea': u'Seattle Seahawks',\n",
       " 'sfo': u'San Francisco 49ers',\n",
       " 'tam': u'Tampa Bay Buccaneers',\n",
       " 'was': u'Washington Redskins'}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfr_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:46:46.509173Z",
     "start_time": "2018-11-08T21:46:46.447528Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_injury_team_year(team, year, driver):\n",
    "    '''scrape for injury info:\n",
    "    '''\n",
    "    i = 0\n",
    "    ## code after having BeautifulSoup object\n",
    "    tbl = soup.findAll('div', {'id':'div_team_injuries'})[0]\n",
    "    tbl = tbl.findAll('tbody')[0]\n",
    "\n",
    "    def get_status(col):\n",
    "        status = col.attrs['class'][2:]\n",
    "        if len(status) == 0:\n",
    "            return ['A', False]\n",
    "        elif len(status) == 1:\n",
    "            return [status[0], False]\n",
    "        else:\n",
    "            return [status[0], True]\n",
    "\n",
    "    def get_injury(col):\n",
    "        if 'data-tip' in col.attrs.keys():\n",
    "            return col.attrs['data-tip'].split(': ')[-1]\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    players = {}\n",
    "    for row in tbl.findAll('tr'):\n",
    "        for th in row.findAll('th'):\n",
    "            if 'data-append-csv' not in th.attrs.keys():\n",
    "                return None\n",
    "            pid = th.attrs['data-append-csv']\n",
    "            name = th.text\n",
    "        for col in row.findAll('td'):\n",
    "            status, dnp = get_status(col)\n",
    "\n",
    "            week = int(col.attrs['data-stat'].split('_')[-1])\n",
    "            inj = get_injury(col)\n",
    "            players[i] = {\n",
    "                'pid': pid,\n",
    "                'status': status, \n",
    "                'dnp': dnp, \n",
    "                'inj': inj,\n",
    "                'name': name,\n",
    "                'team': team,\n",
    "                'week': week, \n",
    "                'year': year\n",
    "            }\n",
    "            i += 1\n",
    "\n",
    "    return DataFrame.from_dict(players, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:00:40.866021Z",
     "start_time": "2018-11-08T21:59:48.723635Z"
    }
   },
   "outputs": [],
   "source": [
    "first = False\n",
    "for team in pfr_teams.keys():\n",
    "    for year in np.arange(2018,2019):\n",
    "        df = get_injury_team_year(team, year, driver)\n",
    "        if df is not None:\n",
    "            if first is True:\n",
    "                injuries = df\n",
    "                first = False\n",
    "            else:\n",
    "                injuries = injuries.append(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:01:34.633987Z",
     "start_time": "2018-11-08T22:01:34.605097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167208, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>week</th>\n",
       "      <th>dnp</th>\n",
       "      <th>inj</th>\n",
       "      <th>team</th>\n",
       "      <th>year</th>\n",
       "      <th>pid</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Los Angeles Rams</td>\n",
       "      <td>2011</td>\n",
       "      <td>AhYoC.00</td>\n",
       "      <td>C.J. Ah You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>out</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>wrist</td>\n",
       "      <td>Los Angeles Rams</td>\n",
       "      <td>2011</td>\n",
       "      <td>AhYoC.00</td>\n",
       "      <td>C.J. Ah You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>questionable</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>wrist</td>\n",
       "      <td>Los Angeles Rams</td>\n",
       "      <td>2011</td>\n",
       "      <td>AhYoC.00</td>\n",
       "      <td>C.J. Ah You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Los Angeles Rams</td>\n",
       "      <td>2011</td>\n",
       "      <td>AhYoC.00</td>\n",
       "      <td>C.J. Ah You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Los Angeles Rams</td>\n",
       "      <td>2011</td>\n",
       "      <td>AhYoC.00</td>\n",
       "      <td>C.J. Ah You</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         status  week    dnp    inj              team  year       pid  \\\n",
       "0             A     1  False         Los Angeles Rams  2011  AhYoC.00   \n",
       "1           out     2   True  wrist  Los Angeles Rams  2011  AhYoC.00   \n",
       "2  questionable     3   True  wrist  Los Angeles Rams  2011  AhYoC.00   \n",
       "3             A     4  False         Los Angeles Rams  2011  AhYoC.00   \n",
       "4             A     6  False         Los Angeles Rams  2011  AhYoC.00   \n",
       "\n",
       "          name  \n",
       "0  C.J. Ah You  \n",
       "1  C.J. Ah You  \n",
       "2  C.J. Ah You  \n",
       "3  C.J. Ah You  \n",
       "4  C.J. Ah You  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print injuries.shape\n",
    "injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:02:08.388689Z",
     "start_time": "2018-11-08T22:02:08.311650Z"
    }
   },
   "outputs": [],
   "source": [
    "injuries['team'] = injuries['team'].map(pfr_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:02:16.899593Z",
     "start_time": "2018-11-08T22:02:16.252676Z"
    }
   },
   "outputs": [],
   "source": [
    "injuries.to_csv('all_injuries.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Lines:\n",
    "* Site has a horrible data format. lines and O/U are inconsistent\n",
    " * ML will indicate who is the favorite (open or close?)\n",
    " * lower of 2H is the spread; higher is the O/U\n",
    " * open/close have 4 combos. take max open and max close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:56:14.429465Z",
     "start_time": "2019-01-02T18:56:13.262173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rot</th>\n",
       "      <th>VH</th>\n",
       "      <th>Team</th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>4th</th>\n",
       "      <th>Final</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>ML</th>\n",
       "      <th>2H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>906</td>\n",
       "      <td>451</td>\n",
       "      <td>V</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>46.5</td>\n",
       "      <td>pk</td>\n",
       "      <td>-119</td>\n",
       "      <td>pk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906</td>\n",
       "      <td>452</td>\n",
       "      <td>H</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>-101</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>909</td>\n",
       "      <td>453</td>\n",
       "      <td>V</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-185</td>\n",
       "      <td>pk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>909</td>\n",
       "      <td>454</td>\n",
       "      <td>H</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>47</td>\n",
       "      <td>41</td>\n",
       "      <td>155</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>909</td>\n",
       "      <td>455</td>\n",
       "      <td>V</td>\n",
       "      <td>SanFrancisco</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>230</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date  Rot VH          Team  1st  2nd  3rd  4th  Final  Open Close   ML  \\\n",
       "0   906  451  V       Atlanta    3    3    0    6     12  46.5    pk -119   \n",
       "1   906  452  H  Philadelphia    0    3    7    8     18     4    44 -101   \n",
       "2   909  453  V    Pittsburgh    0    7   14    0     21   6.5   3.5 -185   \n",
       "3   909  454  H     Cleveland    0    0    7   14     21    47    41  155   \n",
       "4   909  455  V  SanFrancisco    0    3   10    3     16    47    46  230   \n",
       "\n",
       "     2H  \n",
       "0    pk  \n",
       "1    21  \n",
       "2    pk  \n",
       "3    20  \n",
       "4  21.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_excel('URL')\\\n",
    "        .reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that it alternates Visitor-Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:56:35.354463Z",
     "start_time": "2019-01-02T18:56:35.331531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rot</th>\n",
       "      <th>VH</th>\n",
       "      <th>Team</th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>4th</th>\n",
       "      <th>Final</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>ML</th>\n",
       "      <th>2H</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, Date, Rot, VH, Team, 1st, 2nd, 3rd, 4th, Final, Open, Close, ML, 2H, exp]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=False)\n",
    "df['exp'] = df['index'].apply(lambda x: 'V' if x % 2 == 0 else 'H')\n",
    "df[df['exp'] != df['VH']] # neutral location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:58:21.602138Z",
     "start_time": "2019-01-02T18:58:21.507815Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_lines(df, season):\n",
    "    '''takes in a season of lines. pre-processes the funky schema/pattern\n",
    "    and returns a df ready for analysis'''\n",
    "    df['game_id'] = map(lambda x: np.floor(x/2.), np.arange(df.shape[0]))\n",
    "\n",
    "    def clean_pk(x):\n",
    "        '''takes in a value that is either a numeric or string.\n",
    "        if string, it is a pickem (value==pk). return 0. otherwise,\n",
    "        return original value'''\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return 0.\n",
    "\n",
    "    games_dict = {}\n",
    "    for idx in df.index:\n",
    "        ## visitor\n",
    "        if idx % 2 == 0:\n",
    "            game_id = df.loc[idx, 'game_id']\n",
    "            games_dict[game_id] = {\n",
    "                'Date': df.loc[idx, 'Date'],\n",
    "                'V_Team': df.loc[idx, 'Team'],\n",
    "                'V_Q1': df.loc[idx, '1st'],\n",
    "                'V_Q2': df.loc[idx, '2nd'],\n",
    "                'V_Q3': df.loc[idx, '3rd'],\n",
    "                'V_Q4': df.loc[idx, '4th'],\n",
    "                'V_Final': df.loc[idx, 'Final'],\n",
    "                'v_open': clean_pk(df.loc[idx, 'Open']),\n",
    "                'v_close': clean_pk(df.loc[idx, 'Close']),\n",
    "                'V_ML': df.loc[idx, 'ML'],\n",
    "                'v_2h': df.loc[idx, '2H'],\n",
    "                'Neutral': df.loc[idx, 'VH'] == 'N'\n",
    "            }\n",
    "\n",
    "        ## home\n",
    "        else:\n",
    "            games_dict[game_id]['H_Team'] = df.loc[idx, 'Team']\n",
    "            games_dict[game_id]['H_Q1'] = df.loc[idx, '1st']\n",
    "            games_dict[game_id]['H_Q2'] = df.loc[idx, '2nd']\n",
    "            games_dict[game_id]['H_Q3'] = df.loc[idx, '3rd']\n",
    "            games_dict[game_id]['H_Q4'] = df.loc[idx, '4th']\n",
    "            games_dict[game_id]['H_Final'] = df.loc[idx, 'Final']\n",
    "            games_dict[game_id]['h_open'] = clean_pk(df.loc[idx, 'Open'])\n",
    "            games_dict[game_id]['h_close'] = clean_pk(df.loc[idx, 'Close'])\n",
    "            games_dict[game_id]['H_ML'] = df.loc[idx, 'ML']\n",
    "            games_dict[game_id]['h_2h'] = df.loc[idx, '2H']\n",
    "\n",
    "    def ml_fav(x):\n",
    "        '''whoever has smaller ML\n",
    "        is the ML fav'''\n",
    "        h_ml, v_ml = x\n",
    "        if h_ml == v_ml:\n",
    "            return 'pickem'\n",
    "        elif h_ml < v_ml:\n",
    "            return 'H'\n",
    "        else:\n",
    "            return 'V'\n",
    "\n",
    "    def spread_fav(home, vis):\n",
    "        '''the smaller number between home/visitor for both\n",
    "        open and close is the favorite. \n",
    "        this returns the spread, favorite, or O/U'''\n",
    "        fav = 'H' if home < vis else 'V'\n",
    "        if home < vis:\n",
    "            fav = 'H'\n",
    "            ou = vis\n",
    "            spread = home\n",
    "        else:\n",
    "            fav = 'V'\n",
    "            ou = home\n",
    "            spread = vis\n",
    "\n",
    "        return [fav, ou, spread]\n",
    "\n",
    "    games = DataFrame.from_dict(games_dict, orient='index')\n",
    "    # ML will indicate who is the favorite (open or close?)\n",
    "    games['ML_Fav'] = games[['H_ML','V_ML']].apply(ml_fav, axis=1)\n",
    "    # favorite is the one with the lower (spread)\n",
    "    for bet_type, bet_name in [('open','Open'),('close','Close'),('2h','2H')]:\n",
    "        info_col = '{}_info'.format(bet_type)\n",
    "        games[info_col] = games[\n",
    "                    ['h_{}'.format(bet_type),\n",
    "                     'v_{}'.format(bet_type)]\n",
    "                ].apply(\n",
    "                    lambda (home,vis): spread_fav(home, vis), axis=1\n",
    "                )\n",
    "\n",
    "        for i, (field, field_name) in enumerate([\n",
    "                                        ('fav','Fav'),\n",
    "                                        ('ou','OU'),\n",
    "                                        ('spread','Spread')]):\n",
    "            data_field = '{}_{}'.format(bet_name, field_name)\n",
    "            games[data_field] = games[info_col].apply(lambda x: x[i])\n",
    "\n",
    "    games['Season'] = season\n",
    "    games = games[[\n",
    "        'Season','Date','H_Team','V_Team','H_Final','V_Final','H_ML','V_ML','ML_Fav',\n",
    "        'Open_Fav','Open_OU','Open_Spread','Close_Fav','Close_OU','Close_Spread',\n",
    "        '2H_Fav','2H_OU','2H_Spread','H_Q1','H_Q2','H_Q3','H_Q4',\n",
    "        'V_Q1','V_Q2','V_Q3','V_Q4'\n",
    "    ]]\n",
    "\n",
    "    assert games[games['Open_Spread'] >= games['Open_OU']].shape[0] == 0\n",
    "    assert games[games['Close_Spread'] >= games['Close_OU']].shape[0] == 0\n",
    "    assert games[games['2H_Spread'] >= games['2H_OU']].shape[0] == 0\n",
    "\n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:58:40.100337Z",
     "start_time": "2019-01-02T18:58:22.517774Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "2018\n",
      "2017\n",
      "2016\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2012\n",
      "2011\n",
      "2010\n",
      "2009\n",
      "2008\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for season in np.arange(2019, 2007, -1):\n",
    "    print season\n",
    "    df = read_excel(\n",
    "        'URL' + \n",
    "        '{}-{}.xlsx'.format(season-1, str(season)[-2:]))\\\n",
    "            .reset_index(drop=True)\n",
    "    dfs.append(process_lines(df, season-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:58:50.227533Z",
     "start_time": "2019-01-02T18:58:50.157866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3160, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = reduce(lambda x,y: x.append(y).reset_index(drop=True), dfs)\n",
    "all_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:58:59.864230Z",
     "start_time": "2019-01-02T18:58:59.854228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007    267\n",
       "2008    267\n",
       "2009    267\n",
       "2010    267\n",
       "2011    267\n",
       "2012    267\n",
       "2013    267\n",
       "2014    267\n",
       "2015    267\n",
       "2016    267\n",
       "2017    267\n",
       "2018    223\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines['Season'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix erroneous game dates for historical lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:59:46.722321Z",
     "start_time": "2019-01-02T18:59:46.687993Z"
    }
   },
   "outputs": [],
   "source": [
    "## These games say 10-17 but were actually on 10-18\n",
    "## caught this when exploring data\n",
    "idx = all_lines[\n",
    "    (all_lines['Season'] == 2015)\n",
    "    & (all_lines['Date'] == 1017)\n",
    "].index\n",
    "all_lines.loc[idx, 'Date'] = 1018\n",
    "\n",
    "## 1224 --> 1225, and the rest are all similar\n",
    "idx = all_lines[\n",
    "    (all_lines['Season'] == 2010)\n",
    "    & (all_lines['Date'] == 1224)\n",
    "    & (all_lines['H_Team'] == 'Arizona')\n",
    "].index\n",
    "all_lines.loc[idx, 'Date'] = 1225\n",
    "\n",
    "idx = all_lines[\n",
    "    (all_lines['Season'] == 2014)\n",
    "    & (all_lines['Date'] == 930)\n",
    "    & (all_lines['H_Team'] == 'KansasCity')\n",
    "].index\n",
    "all_lines.loc[idx, 'Date'] = 929\n",
    "\n",
    "idx = all_lines[\n",
    "    (all_lines['Season'] == 2015)\n",
    "    & (all_lines['Date'] == 123)\n",
    "    & (all_lines['H_Team'] == 'Denver')\n",
    "].index\n",
    "all_lines.loc[idx, 'Date'] = 124\n",
    "\n",
    "idx = all_lines[\n",
    "    (all_lines['Season'] == 2015)\n",
    "    & (all_lines['Date'] == 123)\n",
    "    & (all_lines['H_Team'] == 'Carolina')\n",
    "].index\n",
    "all_lines.loc[idx, 'Date'] = 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T18:59:50.346018Z",
     "start_time": "2019-01-02T18:59:50.218166Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "def map_dates(x):\n",
    "    dt, yy = x\n",
    "    dt = str(dt)\n",
    "    dd = int(dt[-2:])\n",
    "    mm = int(dt[:-2])\n",
    "    yy = int(yy)\n",
    "    \n",
    "    if mm < 6:\n",
    "        return date(yy+1, mm, dd)\n",
    "    else:\n",
    "        return date(yy, mm, dd)\n",
    "    \n",
    "all_lines['Date'] = all_lines[['Date','Season']].apply(map_dates, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T19:01:50.502154Z",
     "start_time": "2019-01-02T19:01:50.444957Z"
    }
   },
   "outputs": [],
   "source": [
    "all_lines.to_csv('{}all_lines.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative For Lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T05:13:02.448865Z",
     "start_time": "2018-12-26T05:12:31.939212Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "started = False\n",
    "dfs = []\n",
    "for years in [(2018, 2014), (2014, 2010), (2010, 2006), (2006, 2000)]:\n",
    "    curr = read_csv('URL')\n",
    "    dfs.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T05:13:02.468643Z",
     "start_time": "2018-12-26T05:13:02.452550Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = reduce(\n",
    "    lambda x,y: x.append(y[x.columns]).reset_index(drop=True),\n",
    "    dfs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T05:13:02.508470Z",
     "start_time": "2018-12-26T05:13:02.474297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Vis Team</th>\n",
       "      <th>Home Team</th>\n",
       "      <th>Vis Spread</th>\n",
       "      <th>Home Spread</th>\n",
       "      <th>Over Under</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-09-04</td>\n",
       "      <td>Packers</td>\n",
       "      <td>Seahawks</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-09-07</td>\n",
       "      <td>Saints</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-09-07</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>Rams</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-09-07</td>\n",
       "      <td>Browns</td>\n",
       "      <td>Steelers</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-09-07</td>\n",
       "      <td>Jaguars</td>\n",
       "      <td>Eagles</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>48.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Vis Team  Home Team   Vis Spread   Home Spread   Over Under\n",
       "0  2014-09-04   Packers   Seahawks          5.0          -5.0         47.0\n",
       "1  2014-09-07    Saints    Falcons         -3.0           3.0         50.5\n",
       "2  2014-09-07   Vikings       Rams          3.0          -3.0         43.5\n",
       "3  2014-09-07    Browns   Steelers          6.0          -6.0         41.5\n",
       "4  2014-09-07   Jaguars     Eagles         10.0         -10.0         48.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T05:13:15.132623Z",
     "start_time": "2018-12-26T05:13:15.114370Z"
    }
   },
   "outputs": [],
   "source": [
    "lines.to_csv('{}lines_alternative.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:45:37.683423Z",
     "start_time": "2019-01-05T04:45:37.570875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_table(soup):\n",
    "    '''in 2018, site puts spaces in the HTML.\n",
    "    this strips out the spaces and extract the table.'''\n",
    "    started = False\n",
    "    table = ''\n",
    "    for s in unicode(soup).split('\\n'):\n",
    "        if '<table' in s.replace(' ', ''):\n",
    "            started = True\n",
    "        if '</table>' in s.replace(' ', ''):\n",
    "            table += s\n",
    "            ## sometimes </table> then <table> are in the same line\n",
    "            ## leave it open in this case\n",
    "            if '<table' not in s.replace(' ', ''):\n",
    "                started = False\n",
    "            \n",
    "        if started is True:\n",
    "            table += s\n",
    "    table = table.replace('  ','__').replace(' ','').replace('__',' ')\n",
    "    while (table[:6] != '<table') & (table != ''):\n",
    "        table = table[1:]    \n",
    "    return BeautifulSoup(table)\n",
    "\n",
    "def soup_to_pandas(tbl):\n",
    "    '''takes an HTML table in BeautifulSoup form\n",
    "    and returns a pandas dataframe'''\n",
    "    cols = []\n",
    "    team_rank = {}\n",
    "    for i, tr in enumerate(tbl.findAll('tr')):\n",
    "        team_rank[i] = {}\n",
    "        for j, td in enumerate(tr.findAll('td')):\n",
    "            if i == 0:\n",
    "                cols.append(td.text.strip())\n",
    "            else:\n",
    "                team_rank[i][cols[j]] = td.text.strip()\n",
    "        if (i == 0) & (not cols):\n",
    "            cols = map(lambda x: x.text.strip(), tr.findAll('th'))\n",
    "    df = DataFrame.from_dict(team_rank, orient='index')\n",
    "    ## quit here if it's empty\n",
    "    if df.shape[0] == 0:\n",
    "        return df\n",
    "    df_cols = df.columns\n",
    "    df = df[df[df_cols[0]] != df_cols[0]]\n",
    "    df['idx'] = np.arange(1, df.shape[0]+1)\n",
    "    return df.set_index('idx')\n",
    "\n",
    "def rank_week_17_url(yr):\n",
    "    '''week 17 for rank has inconsistent\n",
    "    URL convention, so the years are\n",
    "    hard-coded'''\n",
    "    week_17_map = {\n",
    "        '2007': 'URL',\n",
    "        '2008': 'URL',\n",
    "        '2009': 'URL',\n",
    "        '2010': 'URL',\n",
    "        '2011': 'URL',\n",
    "        '2012': 'URL',\n",
    "        '2013': 'URL',\n",
    "        '2014': 'URL',\n",
    "        '2015': 'URL',\n",
    "        '2016': 'URL',\n",
    "        '2017': 'URL',\n",
    "        '2018': 'URL',\n",
    "    }\n",
    "    for extra_yr in np.arange(2019,2100):\n",
    "        week_17_map[extra_yr] = '/{0}/URL'.format(extra_yr)\n",
    "    return week_17_map[str(yr)]\n",
    "    \n",
    "def scrape_rank_week(week, year):\n",
    "    '''for a given week/year, scrapes offense,\n",
    "    defense, special teams rankings by week.\n",
    "    potentially useful for features'''\n",
    "    N_TEAMS = 32\n",
    "    dfs = []\n",
    "    base_url = 'URL'\n",
    "    \n",
    "    if week == 17:\n",
    "        url = base_url + rank_week_17_url(year)\n",
    "    else:\n",
    "        url = base_url + '/{}/URL'.format(year, week)\n",
    "    soup = ScrapeTools().get_soup(url, driver)\n",
    "    \n",
    "    data_tables = soup.findAll('table', {'class': 'stats'})\n",
    "    if not data_tables:\n",
    "        #return soup\n",
    "        data_tables = extract_table(soup).findAll('table', {'class': 'stats'})\n",
    "\n",
    "    if len(data_tables) == 0:\n",
    "        print 'NO DATA TABLES FOUND'\n",
    "        return\n",
    "    for data_table in data_tables:\n",
    "        ## base HTML table\n",
    "        if ( ('TOTALrank' in data_table.text) \n",
    "                   | ('TOTALVOA' in data_table.text) ) \\\n",
    "               & (('OFFENSErank' in data_table.text) | ('OFFENSEVOA' in data_table.text)) \\\n",
    "               & ('W-L' in data_table.text):\n",
    "            \n",
    "            df = soup_to_pandas(data_table)\n",
    "            if df.shape[0] == N_TEAMS:\n",
    "                dfs.append(df)\n",
    "        \n",
    "        ## some weeks have a past schedule table as well\n",
    "        elif 'PASTSCHED' in data_table.text or 'PAST SCHED' in data_table.text:\n",
    "            df = soup_to_pandas(data_table)\n",
    "            if df.shape[0] == N_TEAMS:\n",
    "                dfs.append(df)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    ## combine the DataFrames\n",
    "    key = 'TEAM'\n",
    "    for i, t in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            df = t\n",
    "        else:\n",
    "            newcols = [key] + list(set(t.columns) - set(df.columns))\n",
    "            df = df.merge(t[newcols], left_on=key, right_on=key) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:45:39.007932Z",
     "start_time": "2019-01-05T04:45:38.989804Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_cols(df):\n",
    "    '''clean up/standardize column\n",
    "    names for merging weekly dataframes'''\n",
    "    col_map = dict(map(\n",
    "        lambda x: (x, x.replace('.','').strip()), \n",
    "        df.columns\n",
    "    ))\n",
    "    return df.rename(columns=col_map)\n",
    "\n",
    "def merge_dfs(a, b):\n",
    "    '''function passed into reduce call.\n",
    "    merges the weekly dataframes. standardize columns.\n",
    "    filling with None where necessary to line up columns.'''\n",
    "    a = clean_cols(a)\n",
    "    b = clean_cols(b)\n",
    "    \n",
    "    if sorted(a.columns) != sorted(b.columns):\n",
    "        for a_not_b in set(a) - set(b):\n",
    "            b[a_not_b] = None\n",
    "        for b_not_a in set(b) - set(a):\n",
    "            a[b_not_a] = None    \n",
    "    return a.append(b[a.columns]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:43:57.218690Z",
     "start_time": "2019-01-05T04:43:54.828259Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = ScrapeTools().get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:46:09.664368Z",
     "start_time": "2019-01-05T04:45:40.003217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 17\n",
      "2017 17\n",
      "2016 17\n",
      "2015 17\n",
      "2014 17\n",
      "2013 17\n",
      "2012 17\n",
      "2011 17\n",
      "2010 17\n",
      "2009 17\n",
      "2008 17\n",
      "2007 17\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p rank\n",
    "\n",
    "dfs = []\n",
    "started = True\n",
    "for year in np.arange(2018, 2006, -1):\n",
    "    for week in np.arange(17,18): # no week 17 ratings\n",
    "        if started is True:\n",
    "            print year, week\n",
    "            week_rank = scrape_rank_week(week, year)\n",
    "            if week_rank is None:\n",
    "                print 'FAIL', week, year\n",
    "            else:\n",
    "                week_rank['Year'] = year\n",
    "                week_rank['Week'] = week\n",
    "                week_rank.to_csv('{}/rank/rank_{:04d}_{:02d}.csv'.format(DATA_DIR, year, week))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean rank\n",
    "* In Week 8, they switch from TOTAL DAVE to WEIGHTED rank (except weeks 8-10 2007)\n",
    " * TOTAL DAVE: NULL weeks 8-16\n",
    " * WEIGHTED rank: NULL weeks 1-7 (for 2007, it's weeks 1-10)\n",
    "* LASTWEEK: NULL week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:59:20.842317Z",
     "start_time": "2019-01-05T04:59:20.832987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN --> Vikings\n",
      "MIA --> Dolphins\n",
      "CAR --> Panthers\n",
      "ATL --> Falcons\n",
      "DET --> Lions\n",
      "CIN --> Bengals\n"
     ]
    }
   ],
   "source": [
    "import json as JSON\n",
    "rank_team_map = JSON.load(open('team_map_for_rank.json','r'))\n",
    "\n",
    "for i, (k,v) in enumerate(rank_team_map.iteritems()):\n",
    "    print '{} --> {}'.format(k, v)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:59:23.189716Z",
     "start_time": "2019-01-05T04:59:21.064074Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_dfs = map(\n",
    "    lambda rank: read_csv('{}rank/{}'.format(DATA_DIR, rank)),\n",
    "    sorted(filter(\n",
    "        lambda x: x.startswith('rank_2'), \n",
    "        os.listdir('{}rank'.format(DATA_DIR))\n",
    "    ))\n",
    ")\n",
    "\n",
    "\n",
    "allcols = []\n",
    "for i in range(len(rank_dfs)):\n",
    "    d = rank_dfs[i]\n",
    "    d.columns = map(lambda x: str(x).strip(), d.columns)\n",
    "    \n",
    "    drop_cols = filter(lambda c: 'Unnamed' in c, d.columns)\n",
    "    drop_cols += [\n",
    "        'RANK', 'NON-ADJTOT VOA', '', '\\xc2\\xa0',\n",
    "        '#', 'rank', 'RK', \n",
    "    ]\n",
    "    \n",
    "    for c in drop_cols:\n",
    "        if c in d.columns:\n",
    "            d.drop(c, axis=1, inplace=True)\n",
    "\n",
    "    allcols.extend(d.columns)\n",
    "    allcols = list(set(allcols))\n",
    "\n",
    "    ## map team abbreviations to names\n",
    "    ## make sure no null team names\n",
    "    assert d[d['TEAM'].isnull()].shape[0] == 0\n",
    "    d['TEAM'] = d['TEAM'].map(rank_team_map)\n",
    "    assert d[d['TEAM'].isnull()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:59:25.787760Z",
     "start_time": "2019-01-05T04:59:23.192698Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_all = reduce(lambda x,y: x.append(y).reset_index(drop=True), rank_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T04:59:26.348727Z",
     "start_time": "2019-01-05T04:59:25.792107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6496, 18)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6496, 18)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_df = rank_all.loc[\n",
    "        :, ['Week',\n",
    "             'Year',\n",
    "             '...']\n",
    "\n",
    "    ].rename(\n",
    "        columns={'Year':'season', 'Week':'week','TEAM':'name'}\n",
    "    )\n",
    "rank_df.shape\n",
    "rank_df = rank_df[~rank_df['OFFENSErank'].str.contains('OFFENSE')]\n",
    "rank_df.shape\n",
    "\n",
    "## D or WEIGHTED rank\n",
    "rank_df['D_OR_WTDrank'] = rank_df[['TOTAL D','WEIGHTEDrank']].apply(\n",
    "    lambda x: filter(lambda y: str(y) != 'nan', x), axis=1\n",
    ")\n",
    "assert rank_df['DAVE_OR_WTDrank'].apply(\n",
    "                lambda x: len(x)\n",
    "            ).value_counts().index.max() == 1\n",
    "\n",
    "rank_df['D_OR_WTDrank'] = rank_df['D_OR_WTDrank'].apply(lambda x: x[0])\n",
    "\n",
    "## increment week_id by 1, since we want to join with the week after\n",
    "## e.g. week 1 rankings come after week 1, so they're used for \n",
    "## week 2\n",
    "rank_df['week'] = rank_df['week'].apply(lambda x: x+1)\n",
    "\n",
    "rank_df.to_csv('{}/rank/rank_alltime.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T15:42:19.033674Z",
     "start_time": "2018-12-29T15:42:18.896304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LASTWEEK'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "2008    1       32\n",
       "2009    1       32\n",
       "2010    1       32\n",
       "2011    1       32\n",
       "2012    1       32\n",
       "2013    1       32\n",
       "2014    1       32\n",
       "2015    1       32\n",
       "2016    1       32\n",
       "2017    1       32\n",
       "2018    1       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'TOTAL DAVE'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2008    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2009    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2010    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2011    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2012    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2013    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2014    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2015    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2016    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2017    8       32\n",
       "        9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "        16      32\n",
       "2018    9       32\n",
       "        10      32\n",
       "        11      32\n",
       "        12      32\n",
       "        13      32\n",
       "        14      32\n",
       "        15      32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'WEIGHTEDrank'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2008    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2009    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2010    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2011    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2012    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2013    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2014    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2015    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2016    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2017    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "2018    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        4       32\n",
       "        5       32\n",
       "        6       32\n",
       "        7       32\n",
       "        8       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'PAST SCHED'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2008    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2009    1       32\n",
       "        2       32\n",
       "2010    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2011    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2012    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2013    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2014    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2015    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2016    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2017    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2018    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'FUTURE SCHED'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2008    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        15      32\n",
       "        16      32\n",
       "2009    1       32\n",
       "        2       32\n",
       "        15      32\n",
       "        16      32\n",
       "2010    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2011    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2012    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2013    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2014    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2015    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2016    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2017    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2018    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ESTIM.WINS'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2008    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2009    1       32\n",
       "        2       32\n",
       "2010    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2011    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2012    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2013    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2014    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2015    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2016    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2017    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2018    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'VAR'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "season  week\n",
       "2007    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2008    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "        16      32\n",
       "2009    1       32\n",
       "        2       32\n",
       "2010    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2011    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2012    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2013    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2014    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2015    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2016    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2017    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "2018    1       32\n",
       "        2       32\n",
       "        3       32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in rank_df.columns:\n",
    "    nulls = rank_df[rank_df[c].isnull()].groupby(['season','week']).size()\n",
    "    if nulls.shape[0] > 0:\n",
    "        c\n",
    "        nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T03:03:20.016210Z",
     "start_time": "2019-01-10T03:03:20.009456Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = DataFrame.from_dict(\n",
    "        Counter(reduce(lambda x,y: x+y, map(lambda z: z.columns.tolist(), rank_dfs))),\n",
    "        orient='index'\n",
    "    ).sort_values(by=0, ascending=False)\n",
    "counts[0] = counts[0] / float(len(rank_dfs))\n",
    "\n",
    "counts['name'] = counts.index\n",
    "counts['name'] = counts['name'].apply(\n",
    "    lambda x: x.replace(' ', '')\n",
    ")\n",
    "\n",
    "counts.groupby('name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T15:42:20.652026Z",
     "start_time": "2018-12-29T15:42:20.002110Z"
    }
   },
   "outputs": [],
   "source": [
    "years = {}\n",
    "weeks = {}\n",
    "week_years = {}\n",
    "for d in rank_dfs:\n",
    "    d.columns = [\n",
    "            c.replace(' ','').replace('.','') \n",
    "            for c in d.columns\n",
    "        ]\n",
    "    yr, wk = d[['Year','Week']].values[0]\n",
    "    if yr not in years.keys():\n",
    "        years[yr] = d.columns.tolist()\n",
    "    else:\n",
    "        years[yr] += d.columns.tolist()\n",
    "    \n",
    "    if wk not in weeks.keys():\n",
    "        weeks[wk] = d.columns.tolist()\n",
    "    else:\n",
    "        weeks[wk] += d.columns.tolist()\n",
    "        \n",
    "    week_years[(wk,yr)] = d.columns.tolist()    \n",
    "    \n",
    "counters = {}\n",
    "n = 11.\n",
    "for i, (k,v) in enumerate(weeks.iteritems()):\n",
    "    curr = DataFrame.from_dict(Counter(v), orient='index').rename(columns={0:k})\n",
    "    curr[k] = curr[k].apply(lambda x: (n-x)/n)\n",
    "    if i == 0:\n",
    "        combined_wk = curr\n",
    "    else:\n",
    "        combined_wk = combined_wk.merge(curr, left_index=True, right_index=True, how='outer').fillna(1.)\n",
    "        \n",
    "    counters = {}\n",
    "n = 16.\n",
    "for i, (k,v) in enumerate(years.iteritems()):\n",
    "    curr = DataFrame.from_dict(Counter(v), orient='index').rename(columns={0:k})\n",
    "    curr[k] = curr[k].apply(lambda x: (n-x)/n)\n",
    "    if i == 0:\n",
    "        combined_yr = curr\n",
    "    else:\n",
    "        combined_yr = combined_yr.merge(curr, left_index=True, right_index=True, how='outer').fillna(1.)\n",
    "    \n",
    "for i, (k,v) in enumerate(week_years.iteritems()):\n",
    "    curr = DataFrame.from_dict(Counter(v), orient='index').rename(columns={0:k})\n",
    "    if i == 0:\n",
    "        combined_wkyr = curr\n",
    "    else:\n",
    "        combined_wkyr = combined_wkyr.merge(curr, left_index=True, right_index=True, how='outer').fillna(0)\n",
    "\n",
    "combined_wkyr = combined_wkyr.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop:\n",
    "* RK, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T15:42:21.160037Z",
     "start_time": "2018-12-29T15:42:21.129051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LASTWEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LASTWEEK\n",
       "1       1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LASTWEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LASTWEEK\n",
       "2007    0.0625\n",
       "2008    0.0625\n",
       "2009    0.0625\n",
       "2010    0.0625\n",
       "2011    0.0625\n",
       "2012    0.0625\n",
       "2013    0.0625\n",
       "2014    0.0625\n",
       "2015    0.0625\n",
       "2016    0.0625\n",
       "2017    0.0625\n",
       "2018    0.1250"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NULL weeks 8-16\n",
    "col = 'LASTWEEK'\n",
    "combined_wk.T[combined_wk.T[col] > 0].sort_index()[[col]]\n",
    "combined_yr.T[combined_yr.T[col] > 0].sort_index()[[col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Stats -- Primary Data Source\n",
    "* Step 1: write all HTML to disk\n",
    "* Step 2: extract individual tables and write them to separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T19:24:31.693428Z",
     "start_time": "2018-12-26T19:24:29.544779Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = ScrapeTools().get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T19:24:50.251583Z",
     "start_time": "2018-12-26T19:24:49.954881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "all_games = {}\n",
    "\n",
    "for year in range(2008,2009):\n",
    "    print len(all_games.keys())\n",
    "    for week in np.arange(16,17):\n",
    "        url = 'URL'\\\n",
    "                .format(year, week)\n",
    "\n",
    "        soup = ScrapeTools().get_soup(url, driver)\n",
    "        all_games[(week, year)] = map(\n",
    "            lambda x: x.findAll('a')[0].attrs['href'], \n",
    "            soup.findAll('td', {'class': 'gamelink'})\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T19:24:55.951007Z",
     "start_time": "2018-12-26T19:24:55.944124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(16, 2008): []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T00:20:47.024546Z",
     "start_time": "2018-12-26T00:20:46.998067Z"
    }
   },
   "outputs": [],
   "source": [
    "DataFrame.from_dict(all_games, orient='index').to_csv('{}/site_all_games_urls_1225.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 16, 2008 webpage is missing : \n",
    "* see here: URL\n",
    "* use the below URL to get all game URLs for this week only:\n",
    "  * URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:00:26.521141Z",
     "start_time": "2018-12-26T20:00:26.499153Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = read_csv('{}/site_all_games_urls_1225.csv'.format(DATA_DIR), index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:00:30.648603Z",
     "start_time": "2018-12-26T20:00:30.502201Z"
    }
   },
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "for _, row in tmp.iterrows():\n",
    "    all_urls.extend(row.dropna().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:00:31.113581Z",
     "start_time": "2018-12-26T20:00:31.104215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3193"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_urls))\n",
    "len((all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:00:35.764925Z",
     "start_time": "2018-12-26T20:00:35.760572Z"
    }
   },
   "outputs": [],
   "source": [
    "all_urls = list(set(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:00:49.815986Z",
     "start_time": "2018-12-26T20:00:49.796507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20181230</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20120101</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20081228</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171231</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160103</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "20181230  16\n",
       "20120101  16\n",
       "20081228  16\n",
       "20171231  16\n",
       "20160103  16"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.from_dict(\n",
    "    Counter(map(lambda x: x.split('/')[-1][:8], all_urls)), orient='index'\n",
    ").sort_values(by=0, ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### games_stats directory is raw html unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T05:18:31.313578Z",
     "start_time": "2018-12-26T05:18:31.171871Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_DIR}games_stats_1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:01:14.498772Z",
     "start_time": "2018-12-26T20:01:14.472936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200812080</th>\n",
       "      <td>1</td>\n",
       "      <td>200812080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812150</th>\n",
       "      <td>1</td>\n",
       "      <td>200812150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812040</th>\n",
       "      <td>1</td>\n",
       "      <td>200812040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812070</th>\n",
       "      <td>14</td>\n",
       "      <td>200812070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812140</th>\n",
       "      <td>14</td>\n",
       "      <td>200812140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812110</th>\n",
       "      <td>1</td>\n",
       "      <td>200812110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812280</th>\n",
       "      <td>16</td>\n",
       "      <td>200812280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200812010</th>\n",
       "      <td>1</td>\n",
       "      <td>200812010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        idx\n",
       "200812080   1  200812080\n",
       "200812150   1  200812150\n",
       "200812040   1  200812040\n",
       "200812070  14  200812070\n",
       "200812140  14  200812140\n",
       "200812110   1  200812110\n",
       "200812280  16  200812280\n",
       "200812010   1  200812010"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tmp = DataFrame.from_dict(Counter(map(lambda x: x.split('.')[0][:-3], files)), orient='index')\n",
    "\n",
    "tmp['idx'] = tmp.index\n",
    "tmp[tmp['idx'].str.startswith('200812')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:07.838381Z",
     "start_time": "2018-12-26T20:01:20.063961Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3193"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n"
     ]
    }
   ],
   "source": [
    "driver = ScrapeTools().get_driver()\n",
    "GAMES_STATS_DIR = 'games_stats_1225'\n",
    "\n",
    "files = os.listdir('{}{}/'.format(DATA_DIR, GAMES_STATS_DIR))\n",
    "\n",
    "# 3177\n",
    "len(all_urls)\n",
    "for i, url in enumerate(all_urls):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "    game_id = url.split('/')[-1].split('.')[0]\n",
    "    if '{}.txt'.format(game_id) in files:\n",
    "        pass\n",
    "    else:\n",
    "        url = 'URL/{}'.format(url)\n",
    "        soup = ScrapeTools().get_soup(url, driver)\n",
    "        with open('{}{}/{}.txt'.format(DATA_DIR, GAMES_STATS_DIR, game_id), 'w') as w:\n",
    "            w.write(unicode(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data From Stored HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:38.451652Z",
     "start_time": "2018-12-26T20:02:38.432470Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_scorebox_team_info(div, curr_game_metadata, i):\n",
    "    '''for the team divs in scorebox, extract the team name/link, \n",
    "    city, score, and coach name/link'''\n",
    "    team = div.findAll('a', {'itemprop':'name'})[0]\n",
    "\n",
    "    team_link = team.attrs['href']\n",
    "    team_full_name = team.text\n",
    "    team_name = team_full_name.split(' ')[-1]\n",
    "    team_city = ' '.join(team_full_name.split(' ')[:-1])\n",
    "    team_score = div.findAll('div', {'class': 'score'})[0].text\n",
    "    team_coach_raw = div.findAll('div', {'class': 'datapoint'})[0]\\\n",
    "                        .findAll('a')[0]\n",
    "    team_coach_name = team_coach_raw.text\n",
    "    team_coach_id = team_coach_raw.attrs['href']\n",
    "\n",
    "    curr_game_metadata['team{}_link'.format(i)] = team_link\n",
    "    curr_game_metadata['team{}_fullname'.format(i)] = team_full_name\n",
    "    curr_game_metadata['team{}_name'.format(i)] = team_name\n",
    "    curr_game_metadata['team{}_city'.format(i)] = team_city\n",
    "    curr_game_metadata['team{}_score'.format(i)] = team_score\n",
    "    curr_game_metadata['team{}_coach_raw'.format(i)] = team_coach_raw\n",
    "    curr_game_metadata['team{}_coach_name'.format(i)] = team_coach_name\n",
    "    curr_game_metadata['team{}_coach_id'.format(i)] = team_coach_id    \n",
    "    \n",
    "    return curr_game_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:38.668999Z",
     "start_time": "2018-12-26T20:02:38.660601Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_game_meta(div, field):\n",
    "    '''converts attendance to integer, and\n",
    "    strips unnecessary headers from strings\n",
    "    '''\n",
    "    div = div.replace('Stadium: ','')\\\n",
    "             .replace('Start Time: ','')\\\n",
    "             .replace('Attendance: ','')\\\n",
    "             .replace('Time of Game: ','')\n",
    "    \n",
    "    if field == 'attendance':\n",
    "        return int(div.replace(',',''))\n",
    "    else:\n",
    "        return div.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:39.863383Z",
     "start_time": "2018-12-26T20:02:39.846684Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_game_metadata(scorebox):\n",
    "    '''takes in the scorebox div. parses the 3\n",
    "    divs: team0, team1, game data, and returns a dict\n",
    "    entry for this game'''\n",
    "\n",
    "    div_children = filter(\n",
    "        lambda ch: str(type(ch)) != \"<class 'bs4.element.NavigableString'>\",\n",
    "        scorebox.children\n",
    "    )\n",
    "\n",
    "    div_teams = filter(\n",
    "            lambda x: x.attrs.get('class', []) == [], \n",
    "            div_children\n",
    "        )\n",
    "    div_game = filter(\n",
    "            lambda x: x.attrs.get('class', []) == ['scorebox_meta'], \n",
    "            div_children\n",
    "        )[0]\n",
    "\n",
    "    ## TEAM INFO\n",
    "    curr_game_metadata = OrderedDict()\n",
    "    for i, div in enumerate(div_teams):\n",
    "        curr_game_metadata = extract_scorebox_team_info(div, curr_game_metadata, i) \n",
    "\n",
    "    ## GAME INFO\n",
    "    ## it looks like there is no indicator of the field\n",
    "    ## e.g. date, start time. feels gross but hardcoding\n",
    "    ## is the only solution I can think of\n",
    "    meta_order = ['date','time','stadium','attendance','duration']\n",
    "\n",
    "    for field, div in zip(meta_order, div_game.findAll('div')):\n",
    "        curr_game_metadata[field] = clean_game_meta(div.text, field)\n",
    "        if field == 'stadium':\n",
    "            curr_game_metadata['stadium_link'] = div.findAll('a')[0].attrs['href']\n",
    "\n",
    "    return curr_game_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:41.839314Z",
     "start_time": "2018-12-26T20:02:41.782380Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_or_link(td):\n",
    "    '''if the element contains a hyperlink, return the\n",
    "    raw string. otherwise, return only the text'''\n",
    "    \n",
    "    ## some a tags do not have href. hrefs is a list\n",
    "    ## of href values. if empty, there is no href and\n",
    "    ## return the text\n",
    "    hrefs = map(lambda x: x.attrs.get('href', None), td.findAll('a'))\n",
    "    hrefs = filter(lambda x: (x is not None) & ('#' not in str(x)), hrefs)\n",
    "        \n",
    "    if hrefs:\n",
    "        return td\n",
    "    else:\n",
    "        return td.text\n",
    "\n",
    "def get_game_tables(soup, game_id):\n",
    "    '''for the given file (BeautifulSoup object),\n",
    "    extract all data tables, and return a dictionary\n",
    "    of data tables for the game.'''\n",
    "    file_tables = {}\n",
    "    for tbl_container in soup.findAll('div', {'class':'table_outer_container'}):\n",
    "        tbl = tbl_container.findAll('tbody')[-1]\n",
    "\n",
    "        table_dict = {}\n",
    "        for i, tr in enumerate(tbl.findAll('tr')):\n",
    "            if 'thead' in tr.attrs.get('class', []):\n",
    "                pass\n",
    "            else:\n",
    "                table_dict[i] = OrderedDict()\n",
    "                table_dict[i]['game_id'] = game_id\n",
    "\n",
    "                ## first column is a th, but can be treated same as td\n",
    "                for td in tr.findAll('th') + tr.findAll('td'):\n",
    "                    if td.attrs['data-stat'] in ['player','name']:\n",
    "                        name_type = td.attrs['data-stat']\n",
    "                        table_dict[i][name_type+'_raw'] = text_or_link(td)\n",
    "                        table_dict[i][name_type+'_name'] = td.text\n",
    "                        name = map(\n",
    "                            lambda x: x.attrs['href'], td.findAll('a')\n",
    "                        )\n",
    "                        if name:\n",
    "                            table_dict[i][name_type+'_id'] = name[0]\n",
    "                    else:\n",
    "                        table_dict[i][td.attrs['data-stat']] = text_or_link(td)\n",
    "                        \n",
    "        table_name = tbl_container.findAll('caption')[0].text\n",
    "        file_tables[table_name] = DataFrame.from_dict(table_dict, orient='index')\n",
    "    return file_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:42.105742Z",
     "start_time": "2018-12-26T20:02:42.077734Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_split_tables(game_metadata, game_tables):\n",
    "    '''certain data is broken out by team (drives, starters, snap counts). \n",
    "    this is indicated by the team name\\' presence in the table key.\n",
    "    make a Team column with the team name, and merge the DataFrames.\n",
    "    '''\n",
    "\n",
    "    team_names = map(lambda i: game_metadata['team{}_name'.format(i)], [0,1])\n",
    "    tables_w_team_name = filter(\n",
    "        lambda x: x.split(' ')[0] in team_names, \n",
    "        game_tables.keys()\n",
    "    )\n",
    "\n",
    "    for tbl in tables_w_team_name:\n",
    "        assert 'Team' not in game_tables[tbl].columns\n",
    "        game_tables[tbl]['Team'] = tbl.split(' ')[0]\n",
    "\n",
    "        base_name = ' '.join(tbl.split(' ')[1:])\n",
    "\n",
    "        if base_name not in game_tables.keys():\n",
    "            game_tables[base_name] = game_tables[tbl]\n",
    "        else:\n",
    "            game_tables[base_name] = game_tables[base_name].append(\n",
    "                game_tables[tbl]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "    for tbl in tables_w_team_name:\n",
    "        del game_tables[tbl]\n",
    "        \n",
    "    return game_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:02:43.907348Z",
     "start_time": "2018-12-26T20:02:43.902786Z"
    }
   },
   "outputs": [],
   "source": [
    "GAMES_STATS_DIR = '{}games_stats_1225'.format(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find net new files to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T20:06:41.497056Z",
     "start_time": "2018-12-26T20:06:41.453652Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3193"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !rm {DATA_DIR}game_metadata.csv\n",
    "\n",
    "files = sorted(os.listdir(GAMES_STATS_DIR))\n",
    "len(files)\n",
    "todo_ids = set(map(lambda x: x.split('.')[0], files))\n",
    "\n",
    "existing_ids = set(\n",
    "    read_csv('{}/game_metadata.csv'.format(DATA_DIR))\n",
    "        ['game_id'].values.tolist()\n",
    ")\n",
    "\n",
    "new_files = map(lambda x: '{}.txt'.format(x), todo_ids - existing_ids)\n",
    "len(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T03:04:57.791799Z",
     "start_time": "2019-01-10T03:04:57.779315Z"
    }
   },
   "outputs": [],
   "source": [
    "start_pt = 0\n",
    "\n",
    "for i, f in enumerate(new_files):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "\n",
    "    if i >= start_pt:\n",
    "        try:\n",
    "            soup = BeautifulSoup(\n",
    "                open('{}/{}'.format(GAMES_STATS_DIR, f)).read(), \n",
    "                'lxml'\n",
    "            )\n",
    "\n",
    "            scorebox = soup.findAll('div', {'class': 'scorebox'})[0]\n",
    "            game_metadata = extract_game_metadata(scorebox)\n",
    "                            \n",
    "            game_tables = get_game_tables(soup, f.split('.')[0])\n",
    "            game_tables = merge_split_tables(game_metadata, game_tables)\n",
    "\n",
    "            for k,v in game_tables.iteritems():\n",
    "                directory = k.replace(' ', '_')\\\n",
    "                             .replace('-','_')\\\n",
    "                             .replace('/','_')\\\n",
    "                             .replace('&','and')\\\n",
    "                             .replace(',','_')\\\n",
    "                             .lower()\\\n",
    "                             .replace('_table','')\n",
    "                filename = f.replace('txt','csv')\n",
    "\n",
    "                !mkdir -p {DATA_DIR}{directory}\n",
    "                filepath = '{}/{}/{}'.format(DATA_DIR, directory, filename)\n",
    "                if not os.path.exists(filepath):\n",
    "                    v.to_csv(filepath)\n",
    "    \n",
    "            game_metadata_df = DataFrame.from_dict(\n",
    "                    game_metadata, orient='index'\n",
    "                ).T\n",
    "            game_metadata_df['game_id'] = game_tables['Game Info Table'].loc[0, 'game_id']\n",
    "            if os.path.exists('{}/game_metadata.csv'.format(DATA_DIR)):\n",
    "                header = False\n",
    "            else:\n",
    "                header = True\n",
    "            game_metadata_df.to_csv(\n",
    "                '{}/game_metadata.csv'.format(DATA_DIR), \n",
    "                 mode='a', \n",
    "                 header=header,\n",
    "                 index=False\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print 'FAILED: {}'.format(f)\n",
    "            print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stadiums\n",
    "* #### Use wikipedia python package, scrape Wikipedia pages and extract lat/long for each stadium -- goal is to eventually compute travel distances between games\n",
    "* #### Make API calls to geonames.org to get timezone (GMT offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T19:55:52.775214Z",
     "start_time": "2019-01-04T19:55:52.762740Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lat_long(stadium_name):\n",
    "    '''given a WikipediaPage object, use \n",
    "    BeautifulSoup to extract/return the\n",
    "    lat/long for the stadium'''\n",
    "    soup = BeautifulSoup(\n",
    "        wikipedia.WikipediaPage(title = stadium_name).html()\n",
    "    )\n",
    "    geo = soup.findAll('span', {'class':'geo'})\n",
    "    if geo:\n",
    "        lat_long = map(float, geo[0].text.split('; '))\n",
    "        return {'lat': lat_long[0], 'long': lat_long[1]}\n",
    "    \n",
    "def get_tz(user, lat, lon):\n",
    "    '''pandas UDF: given geonames username,\n",
    "    lat, and lon, make a REST API call\n",
    "    and return the timezone and GMT offset'''\n",
    "    base_url = 'http://api.geonames.org/timezone'\n",
    "    base_url += '?lat={LAT}&lng={LONG}&username={USER}'\n",
    "    xml = requests.get(\n",
    "            base_url.format(LAT=lat, LONG=lon, USER=user)\n",
    "        ).text\n",
    "    \n",
    "    return BeautifulSoup(xml)\\\n",
    "                .findAll('gmtoffset')\\\n",
    "                [0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T19:55:49.085369Z",
     "start_time": "2019-01-04T19:53:06.126646Z"
    }
   },
   "outputs": [],
   "source": [
    "stadium_name_fixes = {\n",
    "    'TIAA Bank Stadium':'TIAA Bank Field'\n",
    "}\n",
    "\n",
    "stadiums = read_csv('{}/game_metadata.csv'.format(DATA_DIR))['stadium'].unique()\n",
    "stadium_geo = {}\n",
    "for s in stadiums:\n",
    "    stadium_geo[s] = extract_lat_long(stadium_name_fixes.get(s, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T20:02:45.505750Z",
     "start_time": "2019-01-04T20:02:30.942405Z"
    }
   },
   "outputs": [],
   "source": [
    "stadium_df = DataFrame.from_dict(stadium_geo, orient='index')\n",
    "stadium_df.index.name = 'stadium'\n",
    "user = open('{}/.username'.format(DATA_DIR)).read().strip()\n",
    "stadium_df['timezone'] = stadium_df[['lat','long']].apply(\n",
    "    lambda x: get_tz(user, *x), axis=1\n",
    ")\n",
    "stadium_df.to_csv('{}/stadiums_w_tz.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Annual Divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T16:12:49.213149Z",
     "start_time": "2018-11-08T16:12:49.189774Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_season_divisions(soup, divisions, season):\n",
    "    '''given a BeautifulSoup object for a\n",
    "    season standings page, get the team:division\n",
    "    mapping for that season'''\n",
    "    for conf in ['AFC','NFC']:\n",
    "        table = soup.findAll('table', {'id':conf})[0].findAll('tbody')[0]\n",
    "        for tr in table.findAll('tr'):\n",
    "            ## division name change\n",
    "            if len(tr.findAll('th')) == 0:\n",
    "                division = str(tr.text.strip())\n",
    "            else:\n",
    "                teamname = str(\n",
    "                    tr.findAll('th')[0].text.strip()\n",
    "                      .replace('*','')\n",
    "                      .replace('+','')\n",
    "                    )\n",
    "                divisions[(teamname, season)] = {\n",
    "                    'division': division,\n",
    "                }\n",
    "    return divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T16:12:53.176466Z",
     "start_time": "2018-11-08T16:12:49.813461Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = ScrapeTools().get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T16:14:21.820175Z",
     "start_time": "2018-11-08T16:12:58.169870Z"
    }
   },
   "outputs": [],
   "source": [
    "divisions = {}\n",
    "for season in range(2007,2019):\n",
    "    url = 'URL'\\\n",
    "            .format(season)\n",
    "\n",
    "    soup = ScrapeTools().get_soup(url, driver)\n",
    "    divisions = extract_season_divisions(soup, divisions, season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T16:16:11.194811Z",
     "start_time": "2018-11-08T16:16:11.170869Z"
    }
   },
   "outputs": [],
   "source": [
    "divisions_df = DataFrame.from_dict(divisions, orient='index')\\\n",
    "                    .reset_index(drop=False)\n",
    "divisions_df.columns = ['team','season','division']\n",
    "divisions_df.index.name = 'idx'\n",
    "divisions_df.to_csv('{}/team_divisions.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T16:16:11.789216Z",
     "start_time": "2018-11-08T16:16:11.767279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    32\n",
       "2017    32\n",
       "2016    32\n",
       "2015    32\n",
       "2014    32\n",
       "2013    32\n",
       "2012    32\n",
       "2011    32\n",
       "2010    32\n",
       "2009    32\n",
       "2008    32\n",
       "2007    32\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>season</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>2007</td>\n",
       "      <td>NFC West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>2008</td>\n",
       "      <td>NFC West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>2009</td>\n",
       "      <td>NFC West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>2010</td>\n",
       "      <td>NFC West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>2011</td>\n",
       "      <td>NFC West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  team  season  division\n",
       "idx                                     \n",
       "0    Arizona Cardinals    2007  NFC West\n",
       "1    Arizona Cardinals    2008  NFC West\n",
       "2    Arizona Cardinals    2009  NFC West\n",
       "3    Arizona Cardinals    2010  NFC West\n",
       "4    Arizona Cardinals    2011  NFC West"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisions_df['season'].value_counts()\n",
    "divisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape the Season and Week # for each game ID\n",
    "* get a list of teams\n",
    "* for each year/team, scrape list of (game_id, season, week)\n",
    "* NOTE: this has 2 entries for each game_id, one from each team's perspective. game_loc_team_ref indicates the perspective (and offensive team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T20:55:41.010759Z",
     "start_time": "2019-01-04T20:55:40.986725Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gamelog_from_table(tbody):\n",
    "    '''given an HTML table, extract the gamelog elements.\n",
    "    headers are taken from HTML tags rather than from table\n",
    "    headers. called by extract_gamelog_from_soup'''\n",
    "    data = {}\n",
    "    for i, tr in enumerate(tbody.findAll('tr')):\n",
    "        data[i] = {'week_id': tr.findAll('th', {'data-stat':'week_num'})[0].text}\n",
    "        for j, td in enumerate(tr.findAll('td')):\n",
    "            ## get game URL/ID\n",
    "            if td.attrs.get('data-stat','') == 'boxscore_word':\n",
    "                data[i]['game_id'] = td.findAll('a')[0]\\\n",
    "                                        .attrs['href']\\\n",
    "                                        .split('/')[-1].split('.')[0]\n",
    "            else:\n",
    "                colname = td.attrs['data-stat']\n",
    "                data[i][colname] = td.text\n",
    "\n",
    "    return DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "def extract_gamelog_from_soup(soup):\n",
    "    tables = soup.findAll(\n",
    "        'table', \n",
    "        {'class': 'row_summable sortable stats_table now_sortable sliding_cols'}\n",
    "    )\n",
    "\n",
    "    ## tables are duped\n",
    "    if len(tables) == 4:\n",
    "        tables = tables[:2]\n",
    "    elif len(tables) == 2:\n",
    "        tables = tables[:1]\n",
    "    else:\n",
    "        print '{}: too many tables'.format(team_season)\n",
    "\n",
    "    dfs = [\n",
    "        extract_gamelog_from_table(\n",
    "                curr_tbl.findAll('tbody')[0]\n",
    "            )\n",
    "        for curr_tbl in tables\n",
    "    ]\n",
    "\n",
    "    return reduce(\n",
    "        lambda df1,df2: df1.append(df2).reset_index(drop=True), \n",
    "        dfs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:20:46.456307Z",
     "start_time": "2019-01-04T21:05:01.958191Z"
    }
   },
   "outputs": [],
   "source": [
    "games_df = read_csv('{}/game_metadata.csv'.format(DATA_DIR))\n",
    "all_team_seasons = set(map(lambda x: x[0], games_df[['team0_link']].values)) \\\n",
    "                    | set(map(lambda x: x[0], games_df[['team1_link']].values))\n",
    "\n",
    "\n",
    "driver = ScrapeTools().get_driver()\n",
    "for i, team_season in enumerate(all_team_seasons):\n",
    "    url = team_season.replace('.htm','/gamelog/')\n",
    "    url = 'URL' + url\n",
    "    soup = ScrapeTools().get_soup(url, driver)\n",
    "    gamelog = extract_gamelog_from_soup(soup)\n",
    "    ## add a column with the season\n",
    "    gamelog['season'] = team_season.split('/')[-1].split('.')[0]\n",
    "    gamelog['game_loc_team_ref'] =  team_season.split('/')[-2]\n",
    "    \n",
    "    if i == 0:\n",
    "        all_gamelogs = gamelog\n",
    "    else:\n",
    "        all_gamelogs = all_gamelogs.append(gamelog).reset_index(drop=True)\n",
    "        \n",
    "    assert all_gamelogs.drop_duplicates().shape[0] == all_gamelogs.shape[0]\n",
    "\n",
    "all_gamelogs.to_csv('{}/gamelogs.csv'.format(DATA_DIR), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
