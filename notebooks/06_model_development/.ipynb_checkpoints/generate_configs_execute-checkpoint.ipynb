{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:07:47.339952Z",
     "start_time": "2019-05-22T04:07:42.161894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../config/initialize.ipynb\n",
    "\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:07:47.379739Z",
     "start_time": "2019-05-22T04:07:47.345640Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* how to generalize to regression problems?\n",
    "  * easier to create a separate process?\n",
    "  \n",
    "### Modeling ideas\n",
    "* different labels (regression?)\n",
    "* hyperparam tuning\n",
    "* model selection\n",
    "* feature/team combinations, e.g. H-A DVOA\n",
    "* narrow down to certain weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:04:08.970653Z",
     "start_time": "2019-05-22T04:04:08.938881Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 00_plan_and_ideas.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 01_time_date.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 02_teams.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 03_matchup.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 04_travel.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 05_homeaway.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 06_weather.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 07_line.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run 08_rankings.ipynb\n",
    "# os.chdir('../modeling-football-outcomes/05_feature_engineering/')\n",
    "# %run combine_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:04:09.175129Z",
     "start_time": "2019-05-22T04:04:09.139044Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## SPREAD\n",
    "# ranks = spark.table('dvoa').select(\n",
    "#         'team_id','season','week_id','dave_or_wtddvoa',\n",
    "#         'offensedvoa','defensedvoa','s_t_dvoa'\n",
    "#     ).cache()\n",
    "\n",
    "# game_feats = spark.table('labels.team_game_line_labels').select(\n",
    "#         'game_id','season','week_id','team_id','is_home'\n",
    "#     ).cache()\n",
    "\n",
    "# features = game_feats.join(\n",
    "#         ranks, on=['team_id','season','week_id']\n",
    "#     ).fillna(-99999)\n",
    "# labels = spark.table('labels.team_game_line_labels').select(\n",
    "#     ## index\n",
    "#     'game_id','team_id',\n",
    "#     ## strata\n",
    "#     'is_home','is_fav_sbr',\n",
    "#     ## labels\n",
    "#     'did_win','final_margin','did_cover_pfr',\n",
    "#     'did_cover_sbr','did_cover_sbr_open'\n",
    "# )\n",
    "\n",
    "# assert features.count() == labels.count()\n",
    "\n",
    "# features.write.mode('overwrite').saveAsTable('features.190320_test')\n",
    "# labels.write.mode('overwrite').saveAsTable('labels.190320_test')\n",
    "\n",
    "# ## OVER/UNDER\n",
    "# features = spark.table('labels.over_under_labels').select(\n",
    "#         'game_id','season','week_id', 'sbr_ou'\n",
    "#     ).fillna(-99999).cache()\n",
    "\n",
    "# labels = spark.table('labels.over_under_labels').select(\n",
    "#     ## index\n",
    "#     'game_id',\n",
    "#     ## strata\n",
    "#     ## label\n",
    "#     'is_sbr_ou_over'\n",
    "# )\n",
    "\n",
    "# assert features.count() == labels.count()\n",
    "\n",
    "# features.write.mode('overwrite').saveAsTable('features.190320_ou_test')\n",
    "# labels.write.mode('overwrite').saveAsTable('labels.190320_ou_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set root dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:04:09.748074Z",
     "start_time": "2019-05-22T04:04:09.715076Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "ROOT='/Users/joshplotkin/Dropbox/data_science/'\\\n",
    "    'modeling-football-outcomes/models'\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "* set model ID\n",
    "* remove this model ID's directory if it exists\n",
    "* create directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:04:10.224357Z",
     "start_time": "2019-05-22T04:04:10.190299Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-a07b125f70e4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a07b125f70e4>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print 'Directory models/{} **EXISTS**'.format(MODEL_ID)\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = '0320_with_rankings_winner_20feats_noml'\n",
    "\n",
    "if os.path.exists(MODEL_ID):\n",
    "    print 'Directory models/{} **EXISTS**'.format(MODEL_ID)    \n",
    "else:\n",
    "    print 'Directory models/{} **DOES NOT EXIST**'.format(MODEL_ID)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:14.686344Z",
     "start_time": "2019-05-22T03:26:14.679867Z"
    }
   },
   "outputs": [],
   "source": [
    "## wipe out existing directory\n",
    "if os.path.exists(MODEL_ID):\n",
    "    shutil.rmtree(MODEL_ID)\n",
    "os.mkdir(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dictionary version of model.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:14.695644Z",
     "start_time": "2019-05-22T03:26:14.690736Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {'model_id': MODEL_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hive Tables\n",
    "* features\n",
    "* tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:14.932124Z",
     "start_time": "2019-05-22T03:26:14.701443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>features</td>\n",
       "      <td>190320_ou_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>features</td>\n",
       "      <td>190320_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>features</td>\n",
       "      <td>combined_0127</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>features</td>\n",
       "      <td>combined_0320</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>features</td>\n",
       "      <td>home_field</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>features</td>\n",
       "      <td>line</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>features</td>\n",
       "      <td>matchup</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>features</td>\n",
       "      <td>rankings</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>features</td>\n",
       "      <td>team_history</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>features</td>\n",
       "      <td>time_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>features</td>\n",
       "      <td>travel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>features</td>\n",
       "      <td>weather</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    database       tableName  isTemporary\n",
       "0   features  190320_ou_test        False\n",
       "1   features     190320_test        False\n",
       "2   features   combined_0127        False\n",
       "3   features   combined_0320        False\n",
       "4   features      home_field        False\n",
       "5   features            line        False\n",
       "6   features         matchup        False\n",
       "7   features        rankings        False\n",
       "8   features    team_history        False\n",
       "9   features       time_date        False\n",
       "10  features          travel        False\n",
       "11  features         weather        False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('show tables in features').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:14.943118Z",
     "start_time": "2019-05-22T03:26:14.937373Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['features_tbl'] = 'features.combined_0320'\n",
    "model_dict['labels_tbl'] = 'labels.combined_0320'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns from Hive tables\n",
    "* index: unique identifier in features/labels table (must be in both)\n",
    "* label column, and indicator of what is a positive label\n",
    "  * currently not supported: multi-class\n",
    "  * code will binarize\n",
    "* list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:14.960797Z",
     "start_time": "2019-05-22T03:26:14.948094Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['index'] = ['game_id']\n",
    "model_dict['label_col'] = 'did_win'\n",
    "model_dict['pos_labels'] = [1]\n",
    "model_dict['neg_labels'] = [-1]\n",
    "model_dict['features_list'] = ['rankings___h__estim_winrate',\n",
    " 'rankings___v__offensedvoa',\n",
    " 'rankings___h__dave_or_wtddvoa',\n",
    " 'travel___v_travel_from_last_game_decay',\n",
    " 'rankings___h__offensedvoa',\n",
    " 'rankings___v__dave_or_wtddvoa',\n",
    " 'rankings___h__s_t_dvoa',\n",
    " 'rankings___v__s_t_dvoa',\n",
    " 'travel___h_travel_from_last_game_decay',\n",
    " 'rankings___v__defensedvoa',\n",
    " 'rankings___v__estim_winrate',\n",
    " 'team_history___v_ovr_wr_past_3_seasons',\n",
    " 'rankings___h__defensedvoa',\n",
    " 'team_history___h_ovr_wr_past_3_seasons',\n",
    " 'weather___wind_chill',\n",
    " 'home_field___v_visitor__ovr_wr_ytd',\n",
    " 'home_field___v_visitor__ovr_wr_last_8_games',\n",
    " 'weather___wind_mph',\n",
    " 'weather___humidity_pct',\n",
    " 'team_history___h_ovr_wr_ytd']\n",
    "\n",
    "model_dict['features_list'].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.342407Z",
     "start_time": "2019-05-22T03:26:14.965662Z"
    }
   },
   "outputs": [],
   "source": [
    "## assert these fields are of the correct type\n",
    "assert type(model_dict['index']) is list\n",
    "assert type(model_dict['label_col']) is str\n",
    "assert type(model_dict['features_tbl']) is str\n",
    "assert type(model_dict['features_tbl']) is str\n",
    "assert type(model_dict['features_list']) is list\n",
    "assert type(model_dict['pos_labels']) is list\n",
    "\n",
    "## assert format is schema.table and that\n",
    "## table exists in hive\n",
    "for tbl_str in ['features_tbl','features_tbl']:\n",
    "    schema_and_tbl = model_dict[tbl_str].split('.')\n",
    "    assert len(schema_and_tbl) == 2\n",
    "    schema, tbl = schema_and_tbl\n",
    "    assert spark.sql(\n",
    "            'show tables in {}'.format(schema)\n",
    "        ).filter(\n",
    "            col('tableName') == tbl\n",
    "        ).count() == 1\n",
    "\n",
    "feat_cols_set = set(spark.table(model_dict['features_tbl']).columns)\n",
    "label_cols_set = set(spark.table(model_dict['labels_tbl']).columns)\n",
    "idx_set = set(model_dict['index'])\n",
    "feat_set = set(model_dict['features_list'])\n",
    "label_set = set([model_dict['label_col']])\n",
    "\n",
    "## assert the chosen columns exist in the\n",
    "## chosen tables\n",
    "assert not idx_set - feat_cols_set\n",
    "assert not idx_set - label_cols_set\n",
    "assert not feat_set - feat_cols_set\n",
    "assert not label_set - label_cols_set\n",
    "\n",
    "## check that positive and negative label values \n",
    "## are valid\n",
    "for label_val in ['pos_labels','neg_labels']:\n",
    "    assert spark.table(\n",
    "            model_dict['labels_tbl']\n",
    "        ).filter(\n",
    "            col(model_dict['label_col']).isin(model_dict[label_val])\n",
    "        ).count() > 0\n",
    "    \n",
    "## assert that labels and features share identical index\n",
    "a = spark.table(model_dict['features_tbl']).count()\n",
    "b = spark.table(model_dict['labels_tbl']).count()\n",
    "c = spark.table(model_dict['features_tbl']).join(\n",
    "    spark.table(model_dict['labels_tbl']),\n",
    "    on=model_dict['index']\n",
    ").count()\n",
    "assert a == b\n",
    "assert a == c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Sets\n",
    "* random seeds for reproducibility\n",
    "* number of folds for cross-validation (value of <= 1 doesn't do k-fold\n",
    "* global_dataset_proportions\n",
    " * proportion of the data for each of training, scoring only, holdout, and throwaway\n",
    " * generated using stratified sampling\n",
    "* dimensional_dataset_proportions\n",
    " * post-processing after global_dataset_proportions\n",
    " * idea is to move specific field values, e.g. move certain seasons to the holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: use cross-validation data from another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.351835Z",
     "start_time": "2019-05-22T03:26:19.346699Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model_cv_to_use'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV parameters, when not using another model CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample usage for Dimensional Dataset Proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T23:45:00.946875Z",
     "start_time": "2019-02-28T23:45:00.932060Z"
    }
   },
   "source": [
    "```python\n",
    "model_dict['dimensional_dataset_proportions'] = {\n",
    "        'throw_away': [\n",
    "            {\n",
    "                'vals': [\n",
    "                    0\n",
    "                ], \n",
    "                'dim': 'is_home',\n",
    "                'prop_to_move': 1.0, \n",
    "                'from_groups': [\n",
    "                    'in_training',\n",
    "                    'holdout',\n",
    "                    'scoring_only'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.383589Z",
     "start_time": "2019-05-22T03:26:19.356849Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['kfold_seed'] = 99\n",
    "model_dict['dataset_seed'] = 9\n",
    "model_dict['kfolds'] = 5\n",
    "model_dict['strata_cols'] = ['did_cover_sbr','week_id']\n",
    "model_dict['holdout_set'] = {\n",
    "    'store_to_disk': False,\n",
    "    'score_using_full_model': False \n",
    "}\n",
    "\n",
    "model_dict['global_dataset_proportions'] = {\n",
    "        'in_training': 1.,\n",
    "        'holdout': 0,\n",
    "        'throw_away': 0,\n",
    "        'scoring_only': 0\n",
    "    }\n",
    "\n",
    "# DEFAULT: model_dict['dimensional_dataset_proportions'] = {}\n",
    "model_dict['dimensional_dataset_proportions'] = {\n",
    "        'throw_away': [\n",
    "            {\n",
    "                'vals': [\n",
    "                    1, 2, 3, 4, 17, 18, 19, 20, 21, 22\n",
    "                ], \n",
    "                'dim': 'week_id',\n",
    "                'prop_to_move': 1.0, \n",
    "                'from_groups': [\n",
    "                    'in_training',\n",
    "                    'holdout',\n",
    "                    'scoring_only'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.436168Z",
     "start_time": "2019-05-22T03:26:19.389545Z"
    }
   },
   "outputs": [],
   "source": [
    "if model_dict['model_cv_to_use']:\n",
    "    assert type(model_dict['model_cv_to_use']) in [str, unicode]\n",
    "    assert os.path.exists(model_dict['model_cv_to_use'])\n",
    "else:\n",
    "    ## assert the data structures/types are correct\n",
    "    assert type(model_dict['kfold_seed']) is int\n",
    "    assert type(model_dict['dataset_seed']) is int\n",
    "    assert type(model_dict['kfolds']) is int\n",
    "    assert type(model_dict['strata_cols']) is list\n",
    "    assert type(model_dict['global_dataset_proportions']) is dict\n",
    "    assert type(model_dict['dimensional_dataset_proportions']) is dict\n",
    "    assert type(model_dict['holdout_set']) is dict\n",
    "\n",
    "    ## assert strata cols are present in the labels table\n",
    "    assert not set(model_dict['strata_cols']) - label_cols_set\n",
    "\n",
    "    dataset_types = set(['in_training','holdout','throw_away','scoring_only'])\n",
    "    global_datasets = model_dict['global_dataset_proportions']\n",
    "    dim_datasets = model_dict['dimensional_dataset_proportions']\n",
    "\n",
    "    ## assert global_dataset_proportions has all possible dataset types\n",
    "    assert set(global_datasets.keys()) == dataset_types\n",
    "    ## values are proportions that must sum to 1\n",
    "    assert sum(global_datasets.values()) == 1\n",
    "    ## assert that the keys are valid dataset types\n",
    "    assert not set(dim_datasets.keys()) - dataset_types\n",
    "    ## assert the following (in order of assertion block):\n",
    "    ## (1) each value is a list\n",
    "    ## (2) each element of the list is a dict\n",
    "    ## (3) each dict has the 5 required keys\n",
    "    ## (4) the \"dim\" field is in the strata columns \n",
    "    ## (5) \"prop_to_move\" field is [0, 1]\n",
    "    ## (6) \"from_groups\" are in the possible dataset types\n",
    "    for k, dim_list in dim_datasets.iteritems():\n",
    "        assert (type(dim_list)) is list\n",
    "        for entry in dim_list:\n",
    "            assert type(entry) is dict\n",
    "            assert set(entry.keys()) \\\n",
    "                    == set(['vals','dim','prop_to_move','from_groups'])\n",
    "            assert entry['dim'] in model_dict['strata_cols']\n",
    "            assert 0 <= entry['prop_to_move'] <= 1\n",
    "            assert not set(entry['from_groups']) - dataset_types\n",
    "\n",
    "    ## assert holdout set has 2 keys (store_to_disk, score_using_full_model)\n",
    "    ## and the corresponding values are boolean\n",
    "    assert set(model_dict['holdout_set'].keys()) \\\n",
    "            == set(['store_to_disk','score_using_full_model'])\n",
    "    assert len(filter(\n",
    "        lambda x: type(x) is not bool, \n",
    "        model_dict['holdout_set'].values()\n",
    "    )) == 0\n",
    "    ## if holdout data isn't stored, it can't be scored\n",
    "    assert not (model_dict['holdout_set']['store_to_disk'] is False) \\\n",
    "                & (model_dict['holdout_set']['score_using_full_model'] is True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice\n",
    "* package/class name as a string\n",
    "* parameters as a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.451410Z",
     "start_time": "2019-05-22T03:26:19.443040Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model'] = 'sklearn.ensemble.GradientBoostingClassifier'\n",
    "model_dict['model_params'] = {\n",
    "    'learning_rate': 0.1, \n",
    "    'n_estimators': 200, \n",
    "    'max_features': 'auto', \n",
    "    'subsample': 0.9, \n",
    "    'random_state': 9, \n",
    "    'max_depth': 12, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:19.468695Z",
     "start_time": "2019-05-22T03:26:19.457513Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict['model'] = 'xgboost.XGBClassifier'\n",
    "model_dict['model_params'] = {\n",
    "        'n_jobs': 1, \n",
    "        'learning_rate': 0.1, \n",
    "        'n_estimators': 100, \n",
    "        'max_features': 'auto', \n",
    "        'booster': 'gbtree', \n",
    "        'silent': True, \n",
    "        'nthread': None, \n",
    "        'subsample': 0.5, \n",
    "        'random_state': 9, \n",
    "        'objective': 'binary:logistic', \n",
    "        'max_depth': 6, \n",
    "        'gamma': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.463883Z",
     "start_time": "2019-05-22T03:26:19.474399Z"
    }
   },
   "outputs": [],
   "source": [
    "## test that model object can be created\n",
    "## from model inputs\n",
    "try:\n",
    "    import importlib\n",
    "\n",
    "    model_class_str = model_dict['model']\n",
    "    model_obj_path = '.'.join(model_class_str.split('.')[:-1])\n",
    "    model_name = model_class_str.split('.')[-1]\n",
    "    model_package = importlib.import_module(model_obj_path)\n",
    "    model_class = getattr(model_package, model_name)\n",
    "    _ = model_class(**model_dict['model_params'])\n",
    "except Exception as e:\n",
    "    e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out model.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.477068Z",
     "start_time": "2019-05-22T03:26:20.469070Z"
    }
   },
   "outputs": [],
   "source": [
    "model_json_path = '{}/model.json'.format(model_dict['model_id'])\n",
    "assert os.path.exists(model_dict['model_id'])\n",
    "assert not os.path.exists(model_json_path)\n",
    "\n",
    "with open(model_json_path,'w') as w:\n",
    "    json.dump(model_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dictionary version of plots.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.486977Z",
     "start_time": "2019-05-22T03:26:20.482057Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict = {'model_id': MODEL_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Labels\n",
    "* labels --> names (note: keys should be strings)\n",
    "* name for success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.497521Z",
     "start_time": "2019-05-22T03:26:20.492110Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['label_map'] = {\n",
    "    '1': 'Won',\n",
    "    '0': 'Lost'\n",
    "}\n",
    "plots_dict['success_name'] = 'Win Rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.510591Z",
     "start_time": "2019-05-22T03:26:20.503194Z"
    }
   },
   "outputs": [],
   "source": [
    "assert type(plots_dict['label_map']) is dict\n",
    "assert type(plots_dict['success_name']) is str\n",
    "assert set(plots_dict['label_map'].keys()) == set(['0','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins to plot\n",
    "* plot_bins: \n",
    "   * Number of bins to plot (i.e. number of bars on the bar chart)\n",
    "* bin_types:\n",
    "   * \"Bin\" puts scores into uniform bins, e.g. [0, 0.10], (0.10, 0.20], ..., (0.9, 1.0]\n",
    "   * \"Percentile\" bins scores into ntiles determined by plot_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.520801Z",
     "start_time": "2019-05-22T03:26:20.515382Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['bin_types'] = ['Bin', 'Percentile']\n",
    "plots_dict['plot_bins'] = [10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.533029Z",
     "start_time": "2019-05-22T03:26:20.525713Z"
    }
   },
   "outputs": [],
   "source": [
    "## currently only supports \"Bin\" and \"Percentile\"\n",
    "assert not set(plots_dict['bin_types']) - set(['Bin','Percentile'])\n",
    "## all plot bins values should be ints\n",
    "assert plots_dict['plot_bins'] == map(int, plots_dict['plot_bins'])\n",
    "## ensure all bins values are in [2, 1000]\n",
    "assert filter(\n",
    "        lambda x: 2 <= x <= 1000, plots_dict['plot_bins']\n",
    "    )   == plots_dict['plot_bins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:11:29.935287Z",
     "start_time": "2019-01-14T05:11:29.928839Z"
    }
   },
   "source": [
    "### Threshold Metrics to Plot\n",
    "* metrics evaluated at each of 100 score threshold points\n",
    "* currently only supports Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.543647Z",
     "start_time": "2019-05-22T03:26:20.538221Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_dict['threshold_metrics'] = ['Accuracy','F1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.554809Z",
     "start_time": "2019-05-22T03:26:20.548874Z"
    }
   },
   "outputs": [],
   "source": [
    "assert type(plots_dict['threshold_metrics']) is list\n",
    "## currently only supports Accuracy and F1\n",
    "assert not set(plots_dict['threshold_metrics']) - set(['Accuracy','F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T17:15:57.428617Z",
     "start_time": "2019-01-14T17:15:57.423675Z"
    }
   },
   "source": [
    "### Write out plots.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:26:20.565560Z",
     "start_time": "2019-05-22T03:26:20.558642Z"
    }
   },
   "outputs": [],
   "source": [
    "plots_json_path = '{}/plots.json'.format(model_dict['model_id'])\n",
    "assert not os.path.exists(plots_json_path)\n",
    "\n",
    "with open(plots_json_path,'w') as w:\n",
    "    json.dump(plots_dict, w, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:27:07.848147Z",
     "start_time": "2019-05-22T03:26:20.570477Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0320_with_rankings_winner_20feats_noml'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check JSON files\n",
      "\n",
      "real\t0m15.191s\n",
      "user\t0m35.433s\n",
      "sys\t0m2.689s\n",
      "\n",
      "Cross-validation data\n",
      "\n",
      "real\t0m25.279s\n",
      "user\t1m33.174s\n",
      "sys\t0m9.026s\n",
      "\n",
      "Train and score\n",
      "\n",
      "real\t0m2.283s\n",
      "user\t0m1.351s\n",
      "sys\t0m0.918s\n",
      "\n",
      "Evaluate and plot\n",
      "\n",
      "real\t0m1.358s\n",
      "user\t0m0.942s\n",
      "sys\t0m0.394s\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID\n",
    "!source ~/.bashrc && \\\n",
    "    unset PYSPARK_PYTHON && \\\n",
    "    unset PYSPARK_DRIVER_PYTHON && \\\n",
    "    unset PYSPARK_DRIVER_PYTHON_OPTS && \\\n",
    "    cd /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/ && \\\n",
    "    model_pipeline/model_pipeline.sh {MODEL_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:27:07.986062Z",
     "start_time": "2019-05-22T03:27:07.851964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON configuration files passed checks.\r\n",
      "cv sets wrote successfully.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"model_pipeline/eval_plot.py\", line 524, in <module>\r\n",
      "    importance = get_feat_importance_df()\r\n",
      "  File \"model_pipeline/eval_plot.py\", line 408, in get_feat_importance_df\r\n",
      "    assert imp_files\r\n",
      "AssertionError\r\n"
     ]
    }
   ],
   "source": [
    "!cat /Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/models/{MODEL_ID}/logs/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T03:27:08.490355Z",
     "start_time": "2019-05-22T03:27:07.990793Z"
    }
   },
   "outputs": [],
   "source": [
    "!open {MODEL_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:33.967277Z",
     "start_time": "2019-05-22T04:08:33.928723Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT='/Users/joshplotkin/Dropbox/data_science/'\\\n",
    "    'modeling-football-outcomes/models'\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:34.339090Z",
     "start_time": "2019-05-22T04:08:34.300985Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID='0320_with_rankings_winner_20feats_noml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:34.574810Z",
     "start_time": "2019-05-22T04:08:34.536190Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:34.747805Z",
     "start_time": "2019-05-22T04:08:34.709718Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:34.922121Z",
     "start_time": "2019-05-22T04:08:34.883648Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/joshplotkin/Dropbox/data_science/modeling-football-outcomes/model_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:35.138955Z",
     "start_time": "2019-05-22T04:08:35.101007Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_score_functions import get_model_obj, store_feature_importance, check_bad_values, cv_train, cv_score, score_holdout_set, store_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:35.538485Z",
     "start_time": "2019-05-22T04:08:35.499483Z"
    }
   },
   "outputs": [],
   "source": [
    "## \n",
    "model_dict = json.load(open('{}/model.json'.format(MODEL_ID)))\n",
    "os.chdir(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:36.333724Z",
     "start_time": "2019-05-22T04:08:36.277291Z"
    }
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv('cv_data/training.csv')\n",
    "scoring_only = pd.read_csv('cv_data/scoring_only.csv')\n",
    "\n",
    "# if not os.path.exists('serialized_models'): \n",
    "#     os.mkdir('serialized_models')\n",
    "# if not os.path.exists('scores'): \n",
    "#     os.mkdir('scores')\n",
    "# if not os.path.exists('stats'): \n",
    "#     os.mkdir('stats')\n",
    "#     os.mkdir('stats/reported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T04:08:36.744068Z",
     "start_time": "2019-05-22T04:08:36.700673Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "def xgb_crossval(n_estimators, max_depth):\n",
    "    model_dict['model_params'] = {\n",
    "                                 u'booster': u'gbtree',\n",
    "                                 u'gamma': 0,\n",
    "                                 u'learning_rate': 0.1,\n",
    "                                 u'max_depth': int(max_depth),\n",
    "                                 u'max_features': u'auto',\n",
    "                                 u'n_estimators': int(n_estimators),\n",
    "                                 u'n_jobs': 1,\n",
    "                                 u'nthread': None,\n",
    "                                 u'objective': u'binary:logistic',\n",
    "                                 u'random_state': 9,\n",
    "                                 u'silent': True,\n",
    "                                 u'subsample': 0.5}\n",
    "\n",
    "    model_class_str = model_dict['model']\n",
    "    model_obj_path = '.'.join(model_class_str.split('.')[:-1])\n",
    "    model_name = model_class_str.split('.')[-1]\n",
    "    model_package = importlib.import_module(model_obj_path)\n",
    "    model_obj = getattr(model_package, model_name)\n",
    "    library = model_class_str.split('.')[0]\n",
    "\n",
    "\n",
    "    training = pd.read_csv('cv_data/training.csv')\n",
    "    scoring_only = pd.read_csv('cv_data/scoring_only.csv')\n",
    "#     model_obj, library = get_model_obj(model_dict)\n",
    "    library = 'xgboost'\n",
    "    ## in memory is faster when possible\n",
    "    ## spark can train/score on larger data when needed\n",
    "    if library in ['sklearn','xgboost']:\n",
    "        training_scoring_dict = cv_train(model_dict, training, \n",
    "                                         scoring_only, model_obj)\n",
    "\n",
    "        store_models(library, training_scoring_dict)\n",
    "\n",
    "        scores_df = cv_score(model_dict, training_scoring_dict)\n",
    "        scores_df[['label','score']].to_csv('scores/reported_scores.csv')\n",
    "\n",
    "        if model_dict['holdout_set']['score_using_full_model']:\n",
    "            holdout = pd.read_csv('cv_data/holdout.csv')\n",
    "            holdout = score_holdout_set(model_dict, training_scoring_dict, \n",
    "                                        holdout)\n",
    "            holdout[['label','score']].to_csv('scores/holdout_scores.csv')\n",
    "\n",
    "    score_pred = scores_df[['label','score']]\\\n",
    "                    .reset_index(drop=False)\\\n",
    "                    .set_index('game_id')\n",
    "    \n",
    "    print(roc_auc_score(score_pred['label'], score_pred['score'])    )\n",
    "    return roc_auc_score(score_pred['label'], score_pred['score'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-22T04:08:37.640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "0.6698689915537562\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6699  \u001b[0m | \u001b[0m 3.35    \u001b[0m | \u001b[0m 227.3   \u001b[0m |\n",
      "0.6684431843261629\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6684  \u001b[0m | \u001b[0m 4.289   \u001b[0m | \u001b[0m 229.9   \u001b[0m |\n",
      "0.6720433183824035\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.672   \u001b[0m | \u001b[95m 3.497   \u001b[0m | \u001b[95m 212.7   \u001b[0m |\n",
      "0.6798508098841956\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6799  \u001b[0m | \u001b[95m 4.684   \u001b[0m | \u001b[95m 110.6   \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_crossval,\n",
    "    pbounds={\n",
    "        \"n_estimators\": (100, 250),\n",
    "        \"max_depth\": (3, 6)\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=10)\n",
    "\n",
    "print(\"Final result:\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
